{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now loss =  0.1808724560433325  accuracy =  0.684\n",
      "gradient of first layer [[-0.07744788 -0.26657356  0.06777491  0.29275662 -0.08944882 -0.23512905\n",
      "  -0.27468576 -0.16527042  0.22169788  0.21092774]\n",
      " [-0.04034235 -0.62649035  0.49545245  0.10628071  0.72791738  0.00448151\n",
      "   0.14124937  0.2425211   0.07769399 -0.0558281 ]\n",
      " [ 0.00610366  0.19979755  0.39320667 -0.4095081  -0.07572955 -0.01547589\n",
      "  -0.15464427 -0.41503966  0.07899618 -0.11112828]\n",
      " [ 0.37844371 -0.53780093  0.2407952   0.38486458 -0.59882674 -0.13702275\n",
      "   0.49562978  0.24787186 -0.1526701   0.10931066]\n",
      " [-0.06614395 -0.1517539  -0.11647016 -0.11534242 -0.43728971  0.1470622\n",
      "   0.20837228  0.03242494  0.37460612  0.6337539 ]\n",
      " [ 0.40117356  0.84329932  0.36237475  0.18352367  0.6084485  -0.27540447\n",
      "  -0.20654706 -0.19630229 -0.01070293  0.08548563]\n",
      " [ 0.02750023  0.32158354  0.20911348  0.12139281 -0.02645482 -0.37219882\n",
      "   0.02195599  0.03248126  0.29849626 -0.04217213]\n",
      " [-0.36451509 -0.19570538 -0.14640077 -0.3171622  -0.71365249  0.03629445\n",
      "  -0.07930203  0.23433135 -0.05482041 -0.11175787]\n",
      " [-0.35547636 -0.32219266 -0.05623754  0.12082866 -0.62083039  0.48453928\n",
      "   0.27401225  0.1858381   0.12242119  0.11549393]\n",
      " [-0.0970849  -0.07706825  0.62460993 -0.02428377 -0.15861274 -0.20837577\n",
      "  -0.53752151  0.11534496  0.07025353 -0.4987097 ]]\n",
      "now loss =  0.0829110310773234  accuracy =  0.801\n",
      "gradient of first layer [[-0.08992277 -0.26277361  0.065632    0.29580162 -0.11332047 -0.25154384\n",
      "  -0.28581063 -0.14606827  0.22020628  0.21043157]\n",
      " [-0.02186228 -0.63209846  0.49861607  0.10177891  0.76329013  0.02877271\n",
      "   0.15770554  0.21408783  0.07987514 -0.05512821]\n",
      " [ 0.02434465  0.19426563  0.3963176  -0.41394077 -0.04080975  0.00849701\n",
      "  -0.13840951 -0.44310303  0.08113524 -0.11045375]\n",
      " [ 0.38086522 -0.53853639  0.2411976   0.38428175 -0.5941921  -0.13383411\n",
      "   0.49778455  0.24414236 -0.15239038  0.10939729]\n",
      " [-0.0436261  -0.15853426 -0.11267984 -0.12077067 -0.39415197  0.17659739\n",
      "   0.22834619 -0.00218998  0.37715754  0.63447852]\n",
      " [ 0.40487346  0.84217508  0.363013    0.18261926  0.61553045 -0.27054174\n",
      "  -0.20325042 -0.20199405 -0.0102629   0.08562892]\n",
      " [ 0.03424871  0.31953451  0.2102568   0.11975555 -0.01353822 -0.36332215\n",
      "   0.02796429  0.02209413  0.29928714 -0.041921  ]\n",
      " [-0.36451389 -0.19570575 -0.14640057 -0.31716249 -0.71365019  0.03629603\n",
      "  -0.07930096  0.2343295  -0.05482027 -0.11175783]\n",
      " [-0.36209558 -0.320152   -0.05739304  0.12246215 -0.63348179  0.47579754\n",
      "   0.26807679  0.19604278  0.12158884  0.11517996]\n",
      " [-0.09659826 -0.07721599  0.62468663 -0.0243985  -0.15768135 -0.20773331\n",
      "  -0.53708924  0.11459429  0.07030744 -0.4986943 ]]\n",
      "now loss =  0.1157102654496828  accuracy =  0.856\n",
      "gradient of first layer [[-0.09942445 -0.25994406  0.06343978  0.29847471 -0.13155035 -0.26377435\n",
      "  -0.29433881 -0.13162574  0.21880234  0.20982256]\n",
      " [-0.01222267 -0.63495521  0.50084645  0.0990566   0.78177683  0.04116925\n",
      "   0.16635444  0.19944223  0.08130149 -0.05450725]\n",
      " [ 0.03579862  0.19087633  0.3989756  -0.41717356 -0.01883278  0.02321207\n",
      "  -0.12814109 -0.46049593  0.08282339 -0.1097272 ]\n",
      " [ 0.38153011 -0.53873335  0.24135174  0.38409382 -0.59291683 -0.13297924\n",
      "   0.49838108  0.24313234 -0.15229194  0.10944018]\n",
      " [-0.02668584 -0.16349476 -0.10868271 -0.12559687 -0.36164077  0.19828523\n",
      "   0.24351035 -0.02786653  0.37966233  0.63555469]\n",
      " [ 0.40858411  0.84107658  0.36387989  0.18156718  0.62265005 -0.26577496\n",
      "  -0.19992168 -0.2076282  -0.00971142  0.08586946]\n",
      " [ 0.03732727  0.31862437  0.21097343  0.1188825  -0.00763427 -0.359366\n",
      "   0.03072639  0.01741888  0.29974455 -0.04172057]\n",
      " [-0.3645139  -0.19570574 -0.14640057 -0.31716249 -0.71365022  0.03629601\n",
      "  -0.07930097  0.23432952 -0.05482027 -0.11175783]\n",
      " [-0.36605626 -0.31894294 -0.0582709   0.12355122 -0.64107776  0.47065779\n",
      "   0.26450958  0.20208913  0.12100833  0.11492764]\n",
      " [-0.09656729 -0.07722512  0.62469381 -0.02440728 -0.15762199 -0.20769352\n",
      "  -0.53706147  0.11454726  0.07031202 -0.49869229]]\n",
      "now loss =  0.07307471058546763  accuracy =  0.878\n",
      "gradient of first layer [[-0.10458384 -0.25846938  0.06209402  0.30003816 -0.14142135 -0.27035213\n",
      "  -0.29900152 -0.12383758  0.21792767  0.20936032]\n",
      " [-0.00681117 -0.63642162  0.50225131  0.09743825  0.79214679  0.04796498\n",
      "   0.17116972  0.19132782  0.08214986 -0.0541113 ]\n",
      " [ 0.03959073  0.1898613   0.39994526 -0.41829578 -0.01155882  0.02795562\n",
      "  -0.1247869  -0.46617408  0.08339034 -0.1094846 ]\n",
      " [ 0.3818694  -0.53882684  0.24144085  0.38399149 -0.59226699 -0.13255121\n",
      "   0.49868481  0.24262268 -0.15223659  0.10946768]\n",
      " [-0.02298452 -0.16434593 -0.10778184 -0.12663354 -0.35450036  0.20272896\n",
      "   0.24663232 -0.03331546  0.38005404  0.63558473]\n",
      " [ 0.41372546  0.83964022  0.36522624  0.18001431  0.63249364 -0.25926407\n",
      "  -0.19530367 -0.21536505 -0.00886146  0.08630129]\n",
      " [ 0.04018526  0.31783356  0.21171972  0.11802133 -0.00216149 -0.3557556\n",
      "   0.03328635  0.01312262  0.30020976 -0.04148946]\n",
      " [-0.3645139  -0.19570574 -0.14640057 -0.31716249 -0.71365021  0.03629601\n",
      "  -0.07930097  0.23432952 -0.05482027 -0.11175783]\n",
      " [-0.37029457 -0.31767714 -0.05940378  0.12486335 -0.64916973  0.46518163\n",
      "   0.26061559  0.20852191  0.12021723  0.11445595]\n",
      " [-0.0965644  -0.0772258   0.62469457 -0.02440813 -0.15761641 -0.20769001\n",
      "  -0.53705898  0.11454299  0.0703124  -0.49869218]]\n",
      "now loss =  0.09113501333087452  accuracy =  0.888\n",
      "gradient of first layer [[-0.10571229 -0.25815953  0.06177208  0.30038639 -0.14357792 -0.27177473\n",
      "  -0.30002245 -0.12214567  0.2177149   0.20923361]\n",
      " [-0.00392118 -0.63715083  0.50305002  0.09657322  0.79768061  0.05152955\n",
      "   0.17371793  0.18703288  0.08262477 -0.05387383]\n",
      " [ 0.0425415   0.18911484  0.40076865 -0.41917525 -0.00589589  0.03158645\n",
      "  -0.12218941 -0.47055338  0.08387166 -0.10925061]\n",
      " [ 0.381952   -0.53884847  0.24146343  0.38396655 -0.59210883 -0.1324484\n",
      "   0.49875819  0.24249944 -0.15222288  0.10947464]\n",
      " [-0.01378001 -0.16666102 -0.10521676 -0.12936844 -0.33682475  0.21403141\n",
      "   0.25471585 -0.0469638   0.38153417  0.63628573]\n",
      " [ 0.41890174  0.8382793   0.36667918  0.17843901  0.64239434 -0.25281039\n",
      "  -0.19068132 -0.22309099 -0.00794945  0.08680416]\n",
      " [ 0.04204121  0.31735088  0.21223668  0.11745765  0.00138644 -0.35344562\n",
      "   0.03493949  0.01035404  0.30053189 -0.04131418]\n",
      " [-0.36451432 -0.19570563 -0.14640069 -0.31716236 -0.71365103  0.03629548\n",
      "  -0.07930135  0.23433015 -0.05482035 -0.11175787]\n",
      " [-0.36954724 -0.31782872 -0.05920489  0.12466173 -0.6477154   0.46604361\n",
      "   0.26122719  0.20744218  0.1202902   0.1144501 ]\n",
      " [-0.09658103 -0.07722141  0.62468984 -0.02440305 -0.15764822 -0.20771078\n",
      "  -0.53707388  0.11456783  0.07030938 -0.4986939 ]]\n",
      "now loss =  0.06362677023271333  accuracy =  0.89\n",
      "gradient of first layer [[-0.10543907 -0.25821341  0.06184923  0.30031027 -0.14305154 -0.27145757\n",
      "  -0.29979534 -0.12254149  0.21775066  0.20924435]\n",
      " [-0.00310323 -0.63730599  0.50327192  0.09634441  0.79924609  0.05248094\n",
      "   0.17439699  0.18584537  0.08272961 -0.05384143]\n",
      " [ 0.04391912  0.18881411  0.40117047 -0.41957265 -0.00325541  0.03322873\n",
      "  -0.12100891 -0.47257147  0.08409562 -0.10913812]\n",
      " [ 0.38189356 -0.53883376  0.24144571  0.38398422 -0.59222038 -0.13252059\n",
      "   0.49870601  0.24258629 -0.15223455  0.10946717]\n",
      " [-0.00767115 -0.16803672 -0.1034299  -0.13114893 -0.32512303  0.22136723\n",
      "   0.25999187 -0.05593902  0.38256024  0.63682472]\n",
      " [ 0.42309012  0.83724493  0.36789691  0.17717453  0.65038599 -0.24765097\n",
      "  -0.18696827 -0.22931008 -0.00717878  0.08725841]\n",
      " [ 0.0424893   0.31724407  0.21236112  0.11732252  0.00223949 -0.35289605\n",
      "   0.03533335  0.00968949  0.30060772 -0.04127329]\n",
      " [-0.36451471 -0.19570554 -0.14640081 -0.31716224 -0.71365177  0.036295\n",
      "  -0.0793017   0.23433073 -0.05482042 -0.11175791]\n",
      " [-0.36877372 -0.31796641 -0.05897895  0.12445235 -0.64622317  0.46692219\n",
      "   0.2618574   0.20633074  0.12039134  0.11448239]\n",
      " [-0.09660865 -0.07721455  0.62468171 -0.02439471 -0.15770092 -0.20774482\n",
      "  -0.53709841  0.11460884  0.07030418 -0.49869705]]\n",
      "now loss =  0.08626864552603151  accuracy =  0.893\n",
      "gradient of first layer [[-0.10472078 -0.25836758  0.06205554  0.30010209 -0.14168057 -0.27059828\n",
      "  -0.29917878 -0.12359587  0.21786903  0.20930728]\n",
      " [-0.00325462 -0.63722634  0.50320698  0.09640792  0.79896394  0.05224155\n",
      "   0.1742167   0.18609492  0.08265105 -0.05392048]\n",
      " [ 0.04249991  0.18919372  0.40073893 -0.41912688 -0.0059469   0.03143391\n",
      "  -0.12230789 -0.47044116  0.08378188 -0.10936148]\n",
      " [ 0.38181924 -0.53881547  0.24142326  0.3840067  -0.59236186 -0.13261234\n",
      "   0.49863972  0.24269674 -0.15224953  0.10945733]\n",
      " [-0.0108251  -0.16713084 -0.10440267 -0.1301277  -0.33108714  0.21729494\n",
      "   0.25703752 -0.05116376  0.38180075  0.63625053]\n",
      " [ 0.42696439  0.83634114  0.36904021  0.1760213   0.65776304 -0.24292436\n",
      "  -0.1835641  -0.23504215 -0.00645341  0.08770669]\n",
      " [ 0.04259546  0.31722556  0.21238648  0.11729244  0.00244128 -0.3527732\n",
      "   0.03541979  0.00953533  0.30061802 -0.0412723 ]\n",
      " [-0.36451635 -0.19570515 -0.14640129 -0.31716175 -0.71365488  0.036293\n",
      "  -0.07930314  0.23433315 -0.05482073 -0.11175811]\n",
      " [-0.37089041 -0.31740613 -0.05962358  0.12511331 -0.65023741  0.4642523\n",
      "   0.25992535  0.20950489  0.11992452  0.11414963]\n",
      " [-0.09664345 -0.07720634  0.62467133 -0.02438433 -0.15776721 -0.20778737\n",
      "  -0.53712909  0.11466036  0.07029751 -0.49870125]]\n",
      "now loss =  0.0817450841632259  accuracy =  0.897\n",
      "gradient of first layer [[-0.10277686 -0.25878734  0.06264134  0.29954006 -0.13797462 -0.26827067\n",
      "  -0.29750119 -0.12645109  0.21822703  0.20952858]\n",
      " [-0.00261436 -0.6373545   0.50338791  0.0962207   0.80018485  0.05299774\n",
      "   0.17475988  0.1851603   0.08274857 -0.05387489]\n",
      " [ 0.04193977  0.18933175  0.40055979 -0.41895667 -0.00700567  0.03073766\n",
      "  -0.12281296 -0.46960498  0.08364924 -0.10946478]\n",
      " [ 0.38170514 -0.53878985  0.24138849  0.38404    -0.59257907 -0.1327503\n",
      "   0.49854016  0.24286501 -0.15227181  0.10944271]\n",
      " [-0.01038035 -0.16716947 -0.10429813 -0.13022391 -0.33020493  0.2177378\n",
      "   0.25734611 -0.05177008  0.38178647  0.63617187]\n",
      " [ 0.43034928  0.83558633  0.37005922  0.17502772  0.6642049  -0.23883475\n",
      "  -0.18061487 -0.24003203 -0.00580894  0.08811906]\n",
      " [ 0.04203812  0.31735098  0.21221319  0.11745232  0.00137951 -0.35344641\n",
      "   0.03493371  0.0103572   0.30050512 -0.0413494 ]\n",
      " [-0.36451953 -0.19570444 -0.14640225 -0.31716082 -0.71366094  0.03628916\n",
      "  -0.07930591  0.23433785 -0.05482134 -0.11175851]\n",
      " [-0.37095392 -0.31735895 -0.05965291  0.12515072 -0.65034008  0.4641242\n",
      "   0.25982839  0.2096249   0.11986643  0.11408064]\n",
      " [-0.09669197 -0.07719548  0.62465662 -0.02437015 -0.15785958 -0.20784601\n",
      "  -0.53717139  0.11473191  0.07028815 -0.49870733]]\n",
      "now loss =  0.061269152945376756  accuracy =  0.894\n",
      "gradient of first layer [[-1.01328777e-01 -2.59095862e-01  6.30753439e-02  2.99125948e-01\n",
      "  -1.35220343e-01 -2.66537311e-01 -2.96255296e-01 -1.28579778e-01\n",
      "   2.18496989e-01  2.09703823e-01]\n",
      " [-3.14400329e-04 -6.37829776e-01  5.04071485e-01  9.55693919e-02\n",
      "   8.04552132e-01  5.57384895e-02  1.76726576e-01  1.81783069e-01\n",
      "   8.31720129e-02 -5.35981181e-02]\n",
      " [ 4.22538587e-02  1.89277080e-01  4.00648049e-01 -4.19039158e-01\n",
      "  -6.40103414e-03  3.10944314e-02 -1.22559304e-01 -4.70056683e-01\n",
      "   8.36880730e-02 -1.09452074e-01]\n",
      " [ 3.81638895e-01 -5.38775214e-01  2.41367803e-01  3.84058836e-01\n",
      "  -5.92705535e-01 -1.32829742e-01  4.98482837e-01  2.42962399e-01\n",
      "  -1.52284891e-01  1.09433926e-01]\n",
      " [-8.93031427e-03 -1.67438594e-01 -1.03884923e-01 -1.30614731e-01\n",
      "  -3.27418043e-01  2.19407711e-01  2.58537195e-01 -5.38663871e-02\n",
      "   3.81984227e-01  6.36251840e-01]\n",
      " [ 4.33213699e-01  8.34965998e-01  3.70926761e-01  1.74206971e-01\n",
      "   6.69653870e-01 -2.35396063e-01 -1.78140461e-01 -2.44247394e-01\n",
      "  -5.26040121e-03  8.84829964e-02]\n",
      " [ 4.13945639e-02  3.17496536e-01  2.12011338e-01  1.17637363e-01\n",
      "   1.50870160e-04 -3.54222463e-01  3.43731331e-02  1.13050350e-02\n",
      "   3.00375553e-01 -4.14372774e-02]\n",
      " [-3.64526107e-01 -1.95703015e-01 -1.46404265e-01 -3.17158946e-01\n",
      "  -7.13673470e-01  3.62812765e-02 -7.93115871e-02  2.34347513e-01\n",
      "  -5.48226054e-02 -1.11759343e-01]\n",
      " [-3.71182342e-01 -3.17288478e-01 -5.97365528e-02  1.25225319e-01\n",
      "  -6.50767151e-01  4.63821249e-01  2.59605154e-01  2.09975576e-01\n",
      "   1.19788543e-01  1.14008371e-01]\n",
      " [-9.67418482e-02 -7.71845890e-02  6.24641329e-01 -2.43558979e-02\n",
      "  -1.57954589e-01 -2.07905865e-01 -5.37214508e-01  1.14805276e-01\n",
      "   7.02784754e-02 -4.98713774e-01]]\n",
      "now loss =  0.10251049841370379  accuracy =  0.891\n",
      "gradient of first layer [[-9.99958025e-02 -2.59369725e-01  6.34785610e-02  2.98754032e-01\n",
      "  -1.32676655e-01 -2.64960767e-01 -2.95124118e-01 -1.30530173e-01\n",
      "   2.18737058e-01  2.09856413e-01]\n",
      " [ 2.33283069e-05 -6.37860976e-01  5.04150590e-01  9.54910880e-02\n",
      "   8.05191718e-01  5.60978704e-02  1.76975176e-01  1.81305142e-01\n",
      "   8.31946448e-02 -5.36018436e-02]\n",
      " [ 4.20673332e-02  1.89343861e-01  4.00576054e-01 -4.18972641e-01\n",
      "  -6.75559876e-03  3.08395564e-02 -1.22748546e-01 -4.69768976e-01\n",
      "   8.36225914e-02 -1.09511604e-01]\n",
      " [ 3.81496469e-01 -5.38743965e-01  2.41323511e-01  3.84099215e-01\n",
      "  -5.92977295e-01 -1.33000411e-01  4.98360039e-01  2.43171813e-01\n",
      "  -1.52312900e-01  1.09414643e-01]\n",
      " [-8.29558838e-03 -1.67479885e-01 -1.03739525e-01 -1.30745023e-01\n",
      "  -3.26198152e-01  2.20047326e-01  2.58976892e-01 -5.47464133e-02\n",
      "   3.81996453e-01  6.36200348e-01]\n",
      " [ 4.35471914e-01  8.34484070e-01  3.71620352e-01  1.73570969e-01\n",
      "   6.73960654e-01 -2.32703482e-01 -1.76206718e-01 -2.47562216e-01\n",
      "  -4.83020647e-03  8.87740294e-02]\n",
      " [ 3.94713014e-02  3.17919561e-01  2.11412134e-01  1.18183150e-01\n",
      "  -3.51995589e-03 -3.56527633e-01  3.27142132e-02  1.41328289e-02\n",
      "   2.99996785e-01 -4.16980559e-02]\n",
      " [-3.64540517e-01 -1.95699891e-01 -1.46408731e-01 -3.17154873e-01\n",
      "  -7.13700973e-01  3.62640570e-02 -7.93239716e-02  2.34368677e-01\n",
      "  -5.48253955e-02 -1.11761239e-01]\n",
      " [-3.71666578e-01 -3.17139227e-01 -5.99094855e-02  1.25383305e-01\n",
      "  -6.51686343e-01  4.63187425e-01  2.59141285e-01  2.10711672e-01\n",
      "   1.19643089e-01  1.13879591e-01]\n",
      " [-9.68215258e-02 -7.71672171e-02  6.24616624e-01 -2.43333424e-02\n",
      "  -1.58106605e-01 -2.08001236e-01 -5.37283099e-01  1.14922384e-01\n",
      "   7.02629247e-02 -4.98724433e-01]]\n",
      "now loss =  0.1040624979481876  accuracy =  0.895\n",
      "gradient of first layer [[-9.81239592e-02 -2.59758794e-01  6.40556192e-02  2.98234206e-01\n",
      "  -1.29103052e-01 -2.62744774e-01 -2.93535790e-01 -1.33270437e-01\n",
      "   2.19087698e-01  2.10095777e-01]\n",
      " [ 1.92084724e-04 -6.37866523e-01  5.04188652e-01  9.54567887e-02\n",
      "   8.05518497e-01  5.62596282e-02  1.77087392e-01  1.81075820e-01\n",
      "   8.31915076e-02 -5.36262744e-02]\n",
      " [ 4.18031360e-02  1.89416375e-01  4.00490379e-01 -4.18887869e-01\n",
      "  -7.25094198e-03  3.04982074e-02 -1.22994931e-01 -4.69368192e-01\n",
      "   8.35505703e-02 -1.09576556e-01]\n",
      " [ 3.81363865e-01 -5.38715058e-01  2.41281725e-01  3.84136341e-01\n",
      "  -5.93230542e-01 -1.33158771e-01  4.98246375e-01  2.43366577e-01\n",
      "  -1.52339322e-01  1.09395575e-01]\n",
      " [-7.05895315e-03 -1.67687038e-01 -1.03367014e-01 -1.31053842e-01\n",
      "  -3.23807636e-01  2.21426489e-01  2.59960614e-01 -5.65148960e-02\n",
      "   3.82163850e-01  6.36269595e-01]\n",
      " [ 4.37655253e-01  8.34024435e-01  3.72296428e-01  1.72963309e-01\n",
      "   6.78128483e-01 -2.30112014e-01 -1.74349014e-01 -2.50761786e-01\n",
      "  -4.41447541e-03  8.90627475e-02]\n",
      " [ 3.72729952e-02  3.18396837e-01  2.10721275e-01  1.18798473e-01\n",
      "  -7.71749668e-03 -3.59151812e-01  3.08309181e-02  1.73611510e-02\n",
      "   2.99560831e-01 -4.20113436e-02]\n",
      " [-3.64558695e-01 -1.95695955e-01 -1.46414443e-01 -3.17149796e-01\n",
      "  -7.13735685e-01  3.62423760e-02 -7.93395260e-02  2.34395364e-01\n",
      "  -5.48289913e-02 -1.11763825e-01]\n",
      " [-3.71448115e-01 -3.17159612e-01 -5.98492948e-02  1.25336236e-01\n",
      "  -6.51259285e-01  4.63408236e-01  2.59296976e-01  2.10410227e-01\n",
      "   1.19653540e-01  1.13866286e-01]\n",
      " [-9.69027813e-02 -7.71495990e-02  6.24591088e-01 -2.43106207e-02\n",
      "  -1.58261765e-01 -2.08098186e-01 -5.37352663e-01  1.15041694e-01\n",
      "   7.02468326e-02 -4.98736005e-01]]\n",
      "now loss =  0.05514807412902902  accuracy =  0.897\n",
      "gradient of first layer [[-0.09652119 -0.26008385  0.0645512   0.29779714 -0.12603808 -0.26086029\n",
      "  -0.29219069 -0.13560992  0.21938095  0.21030135]\n",
      " [ 0.00185858 -0.63818403  0.5046901   0.09500849  0.80870292  0.05819813\n",
      "   0.17846598  0.17865185  0.08347372 -0.05343872]\n",
      " [ 0.04093042  0.18962486  0.40020501 -0.41863733 -0.00891479  0.0294326\n",
      "  -0.12376124 -0.46807594  0.08335322 -0.10973579]\n",
      " [ 0.38128576 -0.5386979   0.24125692  0.38415805 -0.5933797  -0.13325222\n",
      "   0.49817944  0.24348136 -0.1523552   0.1093836 ]\n",
      " [-0.00936927 -0.16711394 -0.10413211 -0.13038267 -0.32820819  0.21857805\n",
      "   0.2579099  -0.05308097  0.38161602  0.63581398]\n",
      " [ 0.44006562  0.83352311  0.37304495  0.17229956  0.68273282 -0.22725952\n",
      "  -0.17231094 -0.25428932 -0.00395855  0.0893904 ]\n",
      " [ 0.03545515  0.31878851  0.21014791  0.11930191 -0.01119024 -0.36131772\n",
      "   0.02928059  0.02002826  0.29920058 -0.04227808]\n",
      " [-0.36457515 -0.19569245 -0.14641963 -0.31714528 -0.71376715  0.03622285\n",
      "  -0.07935349  0.23441947 -0.0548322  -0.11176618]\n",
      " [-0.37268779 -0.31686143 -0.06025461  0.12569242 -0.65362221  0.46189119\n",
      "   0.2582066   0.2122475   0.11937138  0.1136368 ]\n",
      " [-0.0969646  -0.07713632  0.62457162 -0.02429352 -0.15837985 -0.20817179\n",
      "  -0.53740533  0.11513237  0.07023463 -0.49874504]]\n",
      "now loss =  0.05103080223903955  accuracy =  0.895\n",
      "gradient of first layer [[-0.09485264 -0.26042511  0.06507587  0.29734916 -0.12283516 -0.25890615\n",
      "  -0.29079943 -0.13804057  0.2196847   0.21051923]\n",
      " [ 0.00309012 -0.6384276   0.50507115  0.09467714  0.81106203  0.0596354\n",
      "   0.17948764  0.1768596   0.08369154 -0.05328396]\n",
      " [ 0.04063622  0.18969085  0.40011166 -0.41855085 -0.00947011  0.02907396\n",
      "  -0.12401505 -0.46763997  0.08328763 -0.10979679]\n",
      " [ 0.38118755 -0.53867698  0.24122585  0.38418468 -0.59356805 -0.13336834\n",
      "   0.49809655  0.243625   -0.15237397  0.1093699 ]\n",
      " [-0.00928283 -0.16711124 -0.10410329 -0.13037413 -0.32799935  0.21862125\n",
      "   0.25794778 -0.05317593  0.381585    0.63573497]\n",
      " [ 0.442139    0.83308724  0.37369493  0.17173284  0.68670003 -0.22480653\n",
      "  -0.17056455 -0.25732273 -0.00356414  0.08968703]\n",
      " [ 0.03286107  0.31934055  0.20932957  0.12000812 -0.01616031 -0.36438873\n",
      "   0.02709085  0.02382399  0.29870344 -0.04264711]\n",
      " [-0.36459411 -0.19568848 -0.14642562 -0.31714017 -0.71380353  0.03620053\n",
      "  -0.0793694   0.23444714 -0.05483574 -0.11176874]\n",
      " [-0.37305237 -0.3167685  -0.06036796  0.1258094  -0.6542972   0.46142241\n",
      "   0.25787495  0.2128006   0.11927342  0.11353533]\n",
      " [-0.09703719 -0.07712098  0.62454869 -0.02427388 -0.15851908 -0.20825747\n",
      "  -0.53746645  0.11523846  0.07022089 -0.49875504]]\n",
      "now loss =  0.050600486542565826  accuracy =  0.896\n",
      "gradient of first layer [[-0.09304666 -0.26077954  0.06563716  0.29687959 -0.11935453 -0.2568184\n",
      "  -0.28932203 -0.14065662  0.21998504  0.2107266 ]\n",
      " [ 0.00413593 -0.63859045  0.50537305  0.09442354  0.81308163  0.06079364\n",
      "   0.18029771  0.17536823  0.08381557 -0.05322366]\n",
      " [ 0.04082326  0.18971231  0.40014625 -0.41856748 -0.00908944  0.02920748\n",
      "  -0.12393146 -0.46787118  0.08324433 -0.10987145]\n",
      " [ 0.38106093 -0.53864919  0.24118531  0.38421893 -0.59381102 -0.1335188\n",
      "   0.49798959  0.24381047 -0.15239887  0.10935038]\n",
      " [-0.00689363 -0.16738559 -0.10343788 -0.13088947 -0.32332601  0.22110521\n",
      "   0.25967132 -0.05650338  0.38173386  0.63568466]\n",
      " [ 0.443744    0.83275331  0.37419869  0.17130595  0.68978351 -0.22292188\n",
      "  -0.16922902 -0.25966251 -0.00327231  0.08990592]\n",
      " [ 0.02946506  0.32007228  0.20824761  0.12092084 -0.02268157 -0.36840536\n",
      "   0.02423753  0.02878934  0.2980527  -0.04314838]\n",
      " [-0.36462265 -0.19568239 -0.14643468 -0.31713254 -0.71385839  0.03616686\n",
      "  -0.07939329  0.23448883 -0.05484111 -0.11177285]\n",
      " [-0.37215421 -0.31685376 -0.06012509  0.12562188 -0.65253541  0.46233246\n",
      "   0.25850409  0.21156165  0.11930722  0.1134867 ]\n",
      " [-0.09712709 -0.07710151  0.62452002 -0.02424969 -0.15869168 -0.20836394\n",
      "  -0.53754208  0.11536996  0.07020356 -0.49876848]]\n",
      "now loss =  0.0575828670185334  accuracy =  0.897\n",
      "gradient of first layer [[-0.09163375 -0.26107187  0.06608191  0.29650877 -0.1166323  -0.25516814\n",
      "  -0.28815836 -0.14271113  0.2202333   0.21091645]\n",
      " [ 0.00589259 -0.63892124  0.5059057   0.09397238  0.81646251  0.06281055\n",
      "   0.18171172  0.1728287   0.08408898 -0.05302687]\n",
      " [ 0.04075595  0.18977473  0.40009898 -0.41852956 -0.00921018  0.02906662\n",
      "  -0.12404158 -0.46774404  0.08317078 -0.10995647]\n",
      " [ 0.38095887 -0.53862661  0.24115233  0.38424622 -0.59400749 -0.1336398\n",
      "   0.49790399  0.2439597  -0.15241862  0.10933439]\n",
      " [-0.00648623 -0.16730827 -0.10339438 -0.13092681 -0.32250487  0.22136847\n",
      "   0.25982139 -0.05699486  0.38159559  0.63547895]\n",
      " [ 0.4455933   0.83236522  0.37478189  0.17081709  0.69334211 -0.22075213\n",
      "  -0.16769746 -0.26235684 -0.0029387   0.0901647 ]\n",
      " [ 0.02616099  0.32078759  0.20719187  0.12180156 -0.02903845 -0.37230787\n",
      "   0.02147808  0.03361509  0.29742935 -0.0436441 ]\n",
      " [-0.36464751 -0.19567701 -0.14644264 -0.31712596 -0.71390627  0.03613756\n",
      "  -0.07941398  0.23452509 -0.05484575 -0.11177655]\n",
      " [-0.37222231 -0.31677308 -0.06018009  0.12566897 -0.65264943  0.46216309\n",
      "   0.25836955  0.21170378  0.11920742  0.11336933]\n",
      " [-0.09720878 -0.0770837   0.6244938  -0.02422794 -0.15884896 -0.20846048\n",
      "  -0.53761031  0.11548927  0.07018808 -0.49878088]]\n",
      "now loss =  0.04744760664365459  accuracy =  0.897\n",
      "gradient of first layer [[-0.09056781 -0.26129399  0.06641921  0.29623216 -0.1145718  -0.25392604\n",
      "  -0.28728649 -0.14425812  0.2204157   0.21105934]\n",
      " [ 0.00713558 -0.63916438  0.50628911  0.09365518  0.81886607  0.06424075\n",
      "   0.18271017  0.171034    0.08428133 -0.05288089]\n",
      " [ 0.0406349   0.18982083  0.4000573  -0.41848491 -0.00942915  0.02888922\n",
      "  -0.12416868 -0.4675496   0.0831207  -0.11001323]\n",
      " [ 0.38086902 -0.53860679  0.24112336  0.38426993 -0.594181   -0.13374588\n",
      "   0.49782927  0.24409078 -0.15243534  0.10932077]\n",
      " [-0.0063769  -0.16726324 -0.10336733 -0.13090984 -0.32223952  0.22137297\n",
      "   0.25981777 -0.05709009  0.38151767  0.63535662]\n",
      " [ 0.44746414  0.83196287  0.37537497  0.17032419  0.69694987 -0.2185505\n",
      "  -0.16615144 -0.26508308 -0.0026024   0.09043903]\n",
      " [ 0.02236457  0.32161395  0.20597746  0.12280243 -0.03636433 -0.37678179\n",
      "   0.01833054  0.03915047  0.29673403 -0.04420909]\n",
      " [-0.36467054 -0.195672   -0.14645004 -0.31711991 -0.71395076  0.03611046\n",
      "  -0.07943305  0.23455864 -0.05484995 -0.11177994]\n",
      " [-0.37277245 -0.3166215  -0.06036053  0.1258342  -0.65368771  0.46145873\n",
      "   0.25787074  0.21253475  0.11906223  0.11322517]\n",
      " [-0.09727832 -0.07706849  0.62447145 -0.02420963 -0.15898325 -0.20854243\n",
      "  -0.53766799  0.11559065  0.0701753  -0.49879126]]\n",
      "now loss =  0.09604390367255565  accuracy =  0.896\n",
      "gradient of first layer [[-0.0891353  -0.26159706  0.06687431  0.29586259 -0.11179796 -0.25225505\n",
      "  -0.28611849 -0.1463363   0.22065905  0.21125766]\n",
      " [ 0.00774051 -0.63926705  0.50646455  0.09350869  0.82003611  0.06491868\n",
      "   0.18317733  0.17016858  0.08435468 -0.0528311 ]\n",
      " [ 0.04114564  0.18972693  0.40020908 -0.41860662 -0.00843153  0.02946263\n",
      "  -0.12377647 -0.46827861  0.08317932 -0.10997164]\n",
      " [ 0.38074314 -0.53857892  0.24108276  0.38430276 -0.59442468 -0.13389421\n",
      "   0.49772545  0.24427409 -0.15245809  0.10930154]\n",
      " [-0.0030048  -0.1679345  -0.10232721 -0.13174727 -0.3156774   0.22523478\n",
      "   0.26248909 -0.06194282  0.38199918  0.63572929]\n",
      " [ 0.44883374  0.83166984  0.37581058  0.16996816  0.69959813 -0.2169461\n",
      "  -0.16502886 -0.26707382 -0.00236351  0.09063616]\n",
      " [ 0.01748989  0.32268511  0.2044096   0.12407127 -0.04580212 -0.38251566\n",
      "   0.01431915  0.04624381  0.29586391 -0.04494149]\n",
      " [-0.36470889 -0.19566355 -0.14646237 -0.31710991 -0.71402498  0.03606529\n",
      "  -0.07946465  0.23461448 -0.05485684 -0.11178577]\n",
      " [-0.37176973 -0.31680839 -0.06005775  0.12559092 -0.65173264  0.46258935\n",
      "   0.25864948  0.21110045  0.11918786  0.11331467]\n",
      " [-0.09737343 -0.07704752  0.62444083 -0.02418485 -0.15916736 -0.20865442\n",
      "  -0.53774636  0.11572911  0.07015821 -0.49880568]]\n",
      "now loss =  0.087131516291215  accuracy =  0.893\n",
      "gradient of first layer [[-0.08848402 -0.26173213  0.06707677  0.29569584 -0.11053684 -0.25149787\n",
      "  -0.28559388 -0.14727928  0.22076239  0.21134513]\n",
      " [ 0.00867584 -0.63941956  0.5067277   0.09328002  0.82183782  0.06596469\n",
      "   0.18389272  0.16883125  0.08446154 -0.0527525 ]\n",
      " [ 0.0397549   0.19006855  0.39974988 -0.41822929 -0.01112083  0.02778054\n",
      "  -0.124953   -0.46623498  0.08289867 -0.11023127]\n",
      " [ 0.38069195 -0.53856666  0.2410656   0.38431621 -0.59452425 -0.13395521\n",
      "   0.49768278  0.2443488  -0.15246791  0.10929276]\n",
      " [-0.00782016 -0.16676196 -0.10390762 -0.1304415  -0.32498226  0.21941732\n",
      "   0.25842299 -0.05486884  0.38103634  0.6348385 ]\n",
      " [ 0.45090778  0.83121623  0.37646818  0.16942944  0.70361492 -0.2145083\n",
      "  -0.16333535 -0.27008873 -0.00200935  0.09094523]\n",
      " [ 0.01338024  0.32361798  0.20307793  0.12514294 -0.05377699 -0.38737258\n",
      "   0.01093702  0.05222704  0.29512996 -0.04558871]\n",
      " [-0.3647296  -0.19565881 -0.14646913 -0.31710452 -0.71406521  0.03604081\n",
      "  -0.07948171  0.23464463 -0.05486056 -0.11178905]\n",
      " [-0.37438136 -0.31618102 -0.06091335  0.12629227 -0.65678581  0.45944961\n",
      "   0.25645706  0.21492902  0.11867851  0.11284789]\n",
      " [-0.09741762 -0.07703725  0.62442626 -0.02417331 -0.15925323 -0.20870679\n",
      "  -0.53778291  0.11579348  0.07015006 -0.4988129 ]]\n",
      "now loss =  0.06291975987667128  accuracy =  0.901\n",
      "gradient of first layer [[-0.08730169 -0.26198798  0.06745088  0.29539381 -0.10823814 -0.25011771\n",
      "  -0.28464074 -0.14899087  0.22095115  0.21151157]\n",
      " [ 0.00985475 -0.63964397  0.50707946  0.09298818  0.82412225  0.06731026\n",
      "   0.18481626  0.1671361   0.08461984 -0.05262373]\n",
      " [ 0.03952164  0.19014385  0.39966714 -0.41815573 -0.01156559  0.02747103\n",
      "  -0.12516935 -0.46588005  0.0828312  -0.11030764]\n",
      " [ 0.38060968 -0.5385472   0.2410385   0.38433762 -0.59468455 -0.13405283\n",
      "   0.49761507  0.24446853 -0.15248263  0.10927922]\n",
      " [-0.00754106 -0.16674919 -0.10384011 -0.13046717 -0.32440341  0.21962448\n",
      "   0.25855983 -0.05521652  0.38098732  0.63473947]\n",
      " [ 0.45302847  0.83074209  0.37714481  0.16888229  0.70773679 -0.21201347\n",
      "  -0.16161287 -0.27316683 -0.00165676  0.09126668]\n",
      " [ 0.00879684  0.32466373  0.2015941   0.12632754 -0.06269673 -0.39277811\n",
      "   0.00719543  0.05888479  0.2943446  -0.04630205]\n",
      " [-0.36475433 -0.19565313 -0.14647719 -0.31709814 -0.71411337  0.03601165\n",
      "  -0.07950192  0.23468054 -0.05486484 -0.11179293]\n",
      " [-0.37463714 -0.31608552 -0.06101003  0.12637758 -0.65727208  0.45909344\n",
      "   0.25620762  0.21532539  0.11859067  0.11274339]\n",
      " [-0.09747938 -0.07702285  0.62440604 -0.0241573  -0.15937354 -0.20877986\n",
      "  -0.53783356  0.11588328  0.07013921 -0.4988228 ]]\n",
      "now loss =  0.042262401125322056  accuracy =  0.905\n",
      "gradient of first layer [[-0.0860465  -0.26226584  0.06785261  0.29507523 -0.10579043 -0.24865004\n",
      "  -0.2836323  -0.15080594  0.22114974  0.21169562]\n",
      " [ 0.01176004 -0.64006302  0.50768147  0.09250165  0.82782761  0.0695433\n",
      "   0.18634887  0.16437674  0.084922   -0.05234025]\n",
      " [ 0.04003034  0.19003573  0.39982705 -0.41828063 -0.01056871  0.02805719\n",
      "  -0.12476908 -0.46661133  0.08290166 -0.11024529]\n",
      " [ 0.38054664 -0.53853287  0.24101803  0.3843536  -0.59480773 -0.13412677\n",
      "   0.49756429  0.24455974 -0.15249283  0.10926961]\n",
      " [-0.0051247  -0.16726814 -0.10307333 -0.13106352 -0.31966828  0.22241425\n",
      "   0.26046917 -0.05869239  0.38133246  0.63504539]\n",
      " [ 0.45470855  0.83036526  0.37768259  0.16845224  0.71100836 -0.21003954\n",
      "  -0.16025611 -0.2756013  -0.00138375  0.09152359]\n",
      " [ 0.00381004  0.32579     0.19998865  0.12759983 -0.0724202  -0.39863638\n",
      "   0.00317035  0.06610802  0.29353407 -0.04706701]\n",
      " [-0.36478796 -0.19564555 -0.14648802 -0.31708957 -0.71417895  0.03597216\n",
      "  -0.07952905  0.23472924 -0.05487029 -0.11179806]\n",
      " [-0.3739617  -0.31622276 -0.06079815  0.12621455 -0.65594563  0.45986175\n",
      "   0.25673319  0.21435897  0.11867829  0.11281509]\n",
      " [-0.09753617 -0.07701001  0.62438768 -0.02414288 -0.15948441 -0.20884647\n",
      "  -0.53787931  0.11596547  0.07013005 -0.49883143]]\n",
      "now loss =  0.05789166746303327  accuracy =  0.898\n",
      "gradient of first layer [[-0.08501516 -0.26249469  0.06817938  0.29481526 -0.10377375 -0.24744725\n",
      "  -0.28281345 -0.15229352  0.2213013   0.21184303]\n",
      " [ 0.01329479 -0.64037165  0.50813966  0.09212255  0.83081716  0.07130627\n",
      "   0.18753912  0.16217303  0.08511315 -0.05215778]\n",
      " [ 0.03992642  0.19008815  0.39977795 -0.41824119 -0.01076498  0.02789604\n",
      "  -0.12488821 -0.46644235  0.08284633 -0.11030652]\n",
      " [ 0.38048453 -0.53851751  0.2409972   0.38436963 -0.59492939 -0.13420085\n",
      "   0.49751344  0.24465001 -0.15250377  0.10925862]\n",
      " [-0.00469154 -0.16727714 -0.10297895 -0.1311294  -0.31879151  0.22279231\n",
      "   0.26069836 -0.05925541  0.38127297  0.63496321]\n",
      " [ 0.4565152   0.82995496  0.37825892  0.16799303  0.71453697 -0.20791788\n",
      "  -0.15880881 -0.27821441 -0.00110457  0.09179787]\n",
      " [-0.0013317   0.32699692  0.19831299  0.12891343 -0.08247834 -0.40470739\n",
      "  -0.00098226  0.07355634  0.29269774 -0.04789367]\n",
      " [-0.36482007 -0.19563798 -0.14649848 -0.31708135 -0.71424174  0.03593419\n",
      "  -0.07955502  0.23477579 -0.05487555 -0.11180329]\n",
      " [-0.37404711 -0.31616038 -0.06084961  0.12625516 -0.65610359  0.45970301\n",
      "   0.25661112  0.21450972  0.11860712  0.11273526]\n",
      " [-0.09759706 -0.07699539  0.62436763 -0.02412724 -0.15960356 -0.20891871\n",
      "  -0.53792876  0.11605381  0.07011981 -0.49884167]]\n",
      "now loss =  0.05877182447860825  accuracy =  0.909\n",
      "gradient of first layer [[-8.44679510e-02 -2.62616260e-01  6.83511703e-02  2.94678183e-01\n",
      "  -1.02701157e-01 -2.46809692e-01 -2.82383267e-01 -1.53080854e-01\n",
      "   2.21375967e-01  2.11919757e-01]\n",
      " [ 1.43848954e-02 -6.40592219e-01  5.08473603e-01  9.18601495e-02\n",
      "   8.32953696e-01  7.25487266e-02  1.88375183e-01  1.60615349e-01\n",
      "   8.52436721e-02 -5.20308754e-02]\n",
      " [ 3.83543391e-02  1.90479174e-01  3.99267999e-01 -4.17827413e-01\n",
      "  -1.38369961e-02  2.60066989e-02 -1.26169814e-01 -4.64154387e-01\n",
      "   8.25848252e-02 -1.10590507e-01]\n",
      " [ 3.80422026e-01 -5.38502014e-01  2.40976473e-01  3.84385754e-01\n",
      "  -5.95051958e-01 -1.34275449e-01  4.97462505e-01  2.44740791e-01\n",
      "  -1.52514344e-01  1.09247749e-01]\n",
      " [-1.02022861e-02 -1.65911028e-01 -1.04762897e-01 -1.29680057e-01\n",
      "  -3.29555946e-01  2.16172127e-01  2.56208778e-01 -5.12355081e-02\n",
      "   3.80358136e-01  6.33968632e-01]\n",
      " [ 4.58910795e-01  8.29398496e-01  3.79022933e-01  1.67384362e-01\n",
      "   7.19229181e-01 -2.05095665e-01 -1.56898895e-01 -2.81675841e-01\n",
      "  -7.48464655e-04  9.21695249e-02]\n",
      " [-6.47661412e-03  3.28221585e-01  1.96648571e-01  1.30228003e-01\n",
      "  -9.25603440e-02 -4.10798059e-01 -5.11772531e-03  8.10046085e-02\n",
      "   2.91894516e-01 -4.87262715e-02]\n",
      " [-3.64844849e-01 -1.95632051e-01 -1.46506561e-01 -3.17075014e-01\n",
      "  -7.14290287e-01  3.59048334e-02 -7.95750086e-02  2.34811687e-01\n",
      "  -5.48795283e-02 -1.11807359e-01]\n",
      " [-3.76906784e-01 -3.15455214e-01 -6.17744572e-02  1.27003759e-01\n",
      "  -6.61692325e-01  4.56274015e-01  2.54286241e-01  2.18668454e-01\n",
      "   1.18137345e-01  1.12226645e-01]\n",
      " [-9.76493263e-02 -7.69827535e-02  6.24350511e-01 -2.41138668e-02\n",
      "  -1.59706040e-01 -2.08980727e-01 -5.37970994e-01  1.16129556e-01\n",
      "   7.01113619e-02 -4.98850350e-01]]\n",
      "now loss =  0.04071521523127619  accuracy =  0.907\n",
      "gradient of first layer [[-8.32826171e-02 -2.62885591e-01  6.87293826e-02  2.94384162e-01\n",
      "  -1.00368258e-01 -2.45428724e-01 -2.81454232e-01 -1.54783359e-01\n",
      "   2.21535877e-01  2.12088541e-01]\n",
      " [ 1.66881233e-02 -6.41113576e-01  5.09206418e-01  9.12903808e-02\n",
      "   8.37487325e-01  7.52298173e-02  1.90176121e-01  1.57308959e-01\n",
      "   8.55496171e-02 -5.17053177e-02]\n",
      " [ 3.97527640e-02  1.90168932e-01  3.99718093e-01 -4.18163943e-01\n",
      "  -1.10714068e-02  2.76156839e-02 -1.25087410e-01 -4.66152216e-01\n",
      "   8.27609401e-02 -1.10412631e-01]\n",
      " [ 3.80384368e-01 -5.38492296e-01  2.40964005e-01  3.84395595e-01\n",
      "  -5.95125679e-01 -1.34320971e-01  4.97431644e-01  2.44795681e-01\n",
      "  -1.52520892e-01  1.09240476e-01]\n",
      " [-4.32257212e-03 -1.67220335e-01 -1.02870728e-01 -1.31101076e-01\n",
      "  -3.17935099e-01  2.22948351e-01  2.60769374e-01 -5.96419240e-02\n",
      "   3.81108444e-01  6.34728911e-01]\n",
      " [ 4.60690216e-01  8.28979843e-01  3.79592517e-01  1.66936445e-01\n",
      "   7.22725952e-01 -2.03000401e-01 -1.55489920e-01 -2.84241481e-01\n",
      "  -4.94815579e-04  9.24459842e-02]\n",
      " [-1.20371962e-02  3.29548204e-01  1.94852346e-01  1.31630439e-01\n",
      "  -1.03488476e-01 -4.17364479e-01 -9.54324908e-03  8.90321078e-02\n",
      "   2.91073398e-01 -4.96158673e-02]\n",
      " [-3.64875676e-01 -1.95624621e-01 -1.46516553e-01 -3.17067178e-01\n",
      "  -7.14350794e-01  3.58682930e-02 -7.95996785e-02  2.34856271e-01\n",
      "  -5.48842384e-02 -1.11812466e-01]\n",
      " [-3.74533401e-01 -3.15973229e-01 -6.10144097e-02  1.26433037e-01\n",
      "  -6.57000668e-01  4.58995651e-01  2.56117332e-01  2.15280952e-01\n",
      "   1.18430307e-01  1.12519155e-01]\n",
      " [-9.76932718e-02 -7.69719143e-02  6.24336178e-01 -2.41025885e-02\n",
      "  -1.59792218e-01 -2.09033165e-01 -5.38006433e-01  1.16193276e-01\n",
      "   7.01043564e-02 -4.98858031e-01]]\n",
      "now loss =  0.09843845626996794  accuracy =  0.909\n",
      "gradient of first layer [[-8.23919892e-02 -2.63092099e-01  6.90130043e-02  2.94163876e-01\n",
      "  -9.86094305e-02 -2.44388801e-01 -2.80761950e-01 -1.56059851e-01\n",
      "   2.21647968e-01  2.12217401e-01]\n",
      " [ 1.83179954e-02 -6.41494180e-01  5.09724794e-01  9.08859078e-02\n",
      "   8.40704467e-01  7.71368471e-02  1.91445483e-01  1.54970651e-01\n",
      "   8.57567720e-02 -5.14662810e-02]\n",
      " [ 3.97107721e-02  1.90189433e-01  3.99704091e-01 -4.18141671e-01\n",
      "  -1.11391011e-02  2.75407905e-02 -1.25141954e-01 -4.66077702e-01\n",
      "   8.27329640e-02 -1.10447391e-01]\n",
      " [ 3.80320562e-01 -5.38476516e-01  2.40943180e-01  3.84411893e-01\n",
      "  -5.95251074e-01 -1.34397021e-01  4.97380506e-01  2.44888022e-01\n",
      "  -1.52530704e-01  1.09229372e-01]\n",
      " [-3.95262939e-03 -1.67267392e-01 -1.02754752e-01 -1.31149889e-01\n",
      "  -3.17149118e-01  2.23286683e-01  2.60977942e-01 -6.01202032e-02\n",
      "   3.81073178e-01  6.34678693e-01]\n",
      " [ 4.62762047e-01  8.28485285e-01  3.80256599e-01  1.66414416e-01\n",
      "   7.26806920e-01 -2.00556955e-01 -1.53856922e-01 -2.87224692e-01\n",
      "  -2.10165473e-04  9.27720544e-02]\n",
      " [-1.77345888e-02  3.30925577e-01  1.93009651e-01  1.33065898e-01\n",
      "  -1.14711845e-01 -4.24102101e-01 -1.40536757e-02  9.72453468e-02\n",
      "   2.90262844e-01 -5.05409685e-02]\n",
      " [-3.64911329e-01 -1.95616026e-01 -1.46528082e-01 -3.17058208e-01\n",
      "  -7.14421024e-01  3.58261594e-02 -7.96278798e-02  2.34907654e-01\n",
      "  -5.48892981e-02 -1.11818246e-01]\n",
      " [-3.74667062e-01 -3.15920186e-01 -6.10619762e-02  1.26485686e-01\n",
      "  -6.57240180e-01  4.58792854e-01  2.55971891e-01  2.15498513e-01\n",
      "   1.18369394e-01  1.12447587e-01]\n",
      " [-9.77521458e-02 -7.69575575e-02  6.24317061e-01 -2.40876720e-02\n",
      "  -1.59908065e-01 -2.09103008e-01 -5.38053285e-01  1.16278287e-01\n",
      "   7.00956835e-02 -4.98867884e-01]]\n",
      "now loss =  0.04786944643201072  accuracy =  0.913\n",
      "gradient of first layer [[-8.19446680e-02 -2.63188797e-01  6.91499811e-02  2.94057237e-01\n",
      "  -9.77225355e-02 -2.43875885e-01 -2.80425684e-01 -1.56695042e-01\n",
      "   2.21690102e-01  2.12269146e-01]\n",
      " [ 1.79957909e-02 -6.41353796e-01  5.09587966e-01  9.09923618e-02\n",
      "   8.40075838e-01  7.66783693e-02  1.91126527e-01  1.55469886e-01\n",
      "   8.56401755e-02 -5.16111370e-02]\n",
      " [ 3.72796279e-02  1.90831048e-01  3.98904878e-01 -4.17507266e-01\n",
      "  -1.59243635e-02  2.45938003e-02 -1.27105365e-01 -4.62547619e-01\n",
      "   8.23517106e-02 -1.10920950e-01]\n",
      " [ 3.80265831e-01 -5.38461730e-01  2.40924613e-01  3.84426187e-01\n",
      "  -5.95359009e-01 -1.34463431e-01  4.97335866e-01  2.44967608e-01\n",
      "  -1.52539966e-01  1.09218414e-01]\n",
      " [-1.13451009e-02 -1.65304197e-01 -1.05186627e-01 -1.29212523e-01\n",
      "  -3.31688743e-01  2.14302977e-01  2.54989978e-01 -4.93743871e-02\n",
      "   3.79894787e-01  6.33212102e-01]\n",
      " [ 4.64905860e-01  8.27966504e-01  3.80942597e-01  1.65878623e-01\n",
      "   7.31042502e-01 -1.98025862e-01 -1.52179186e-01 -2.90305763e-01\n",
      "   6.76800805e-05  9.31097508e-02]\n",
      " [-2.41374012e-02  3.32521815e-01  1.90928578e-01  1.34683813e-01\n",
      "  -1.27358091e-01 -4.31715168e-01 -1.91174247e-02  1.06474246e-01\n",
      "   2.89369471e-01 -5.16195330e-02]\n",
      " [-3.64937342e-01 -1.95609452e-01 -1.46536618e-01 -3.17051615e-01\n",
      "  -7.14472410e-01  3.57951483e-02 -7.96485494e-02  2.34945191e-01\n",
      "  -5.48930566e-02 -1.11822754e-01]\n",
      " [-3.77957113e-01 -3.15045596e-01 -6.21469563e-02  1.27346737e-01\n",
      "  -6.63713337e-01  4.54796060e-01  2.53306163e-01  2.20280540e-01\n",
      "   1.17843266e-01  1.11795900e-01]\n",
      " [-9.77985837e-02 -7.69454710e-02  6.24301587e-01 -2.40757413e-02\n",
      "  -1.59999721e-01 -2.09158795e-01 -5.38090631e-01  1.16345531e-01\n",
      "   7.00884422e-02 -4.98876478e-01]]\n",
      "now loss =  0.06155882604241464  accuracy =  0.917\n",
      "gradient of first layer [[-8.11417061e-02 -2.63374213e-01  6.93997050e-02  2.93861378e-01\n",
      "  -9.61322068e-02 -2.42939860e-01 -2.79812934e-01 -1.57840943e-01\n",
      "   2.21775502e-01  2.12380905e-01]\n",
      " [ 1.96948797e-02 -6.41703757e-01  5.10095174e-01  9.05938043e-02\n",
      "   8.43441570e-01  7.86069461e-02  1.92382075e-01  1.53066474e-01\n",
      "   8.57778781e-02 -5.14336266e-02]\n",
      " [ 3.79629887e-02  1.90709006e-01  3.99103046e-01 -4.17658745e-01\n",
      "  -1.45667479e-02  2.53430499e-02 -1.26619334e-01 -4.63503043e-01\n",
      "   8.23886322e-02 -1.10877713e-01]\n",
      " [ 3.80216572e-01 -5.38448850e-01  2.40907957e-01  3.84438576e-01\n",
      "  -5.95456652e-01 -1.34522451e-01  4.97296426e-01  2.45038729e-01\n",
      "  -1.52547507e-01  1.09209394e-01]\n",
      " [-7.32631452e-03 -1.66140037e-01 -1.03965416e-01 -1.30149595e-01\n",
      "  -3.23712809e-01  2.18859420e-01  2.57967292e-01 -5.50569127e-02\n",
      "   3.80236051e-01  6.33634404e-01]\n",
      " [ 4.67140834e-01  8.27421527e-01  3.81653779e-01  1.65324380e-01\n",
      "   7.35468927e-01 -1.95385531e-01 -1.50443142e-01 -2.93510746e-01\n",
      "   3.38185447e-04  9.34599319e-02]\n",
      " [-3.02824371e-02  3.34074771e-01  1.88932631e-01  1.36222454e-01\n",
      "  -1.39533583e-01 -4.39035556e-01 -2.39498823e-02  1.15313419e-01\n",
      "   2.88556517e-01 -5.26581626e-02]\n",
      " [-3.64967042e-01 -1.95602018e-01 -1.46546270e-01 -3.17044208e-01\n",
      "  -7.14531248e-01  3.57598657e-02 -7.96718754e-02  2.34987889e-01\n",
      "  -5.48969845e-02 -1.11827695e-01]\n",
      " [-3.76325875e-01 -3.15368146e-01 -6.16588808e-02  1.26971988e-01\n",
      "  -6.60475790e-01  4.56624497e-01  2.54498383e-01  2.17982537e-01\n",
      "   1.17965416e-01  1.11944875e-01]\n",
      " [-9.78509850e-02 -7.69321959e-02  6.24284353e-01 -2.40626479e-02\n",
      "  -1.60103546e-01 -2.09221182e-01 -5.38132013e-01  1.16420955e-01\n",
      "   7.00811911e-02 -4.98885444e-01]]\n",
      "now loss =  0.05760858652318884  accuracy =  0.909\n",
      "gradient of first layer [[-8.08524925e-02 -2.63437839e-01  6.94869307e-02  2.93791857e-01\n",
      "  -9.55570563e-02 -2.42606691e-01 -2.79599013e-01 -1.58250005e-01\n",
      "   2.21797790e-01  2.12416290e-01]\n",
      " [ 2.12696467e-02 -6.42058733e-01  5.10574296e-01  9.02135579e-02\n",
      "   8.46560991e-01  8.04338289e-02  1.93567753e-01  1.50826018e-01\n",
      "   8.59256631e-02 -5.12256031e-02]\n",
      " [ 3.70692956e-02  1.90964776e-01  3.98798647e-01 -4.17423684e-01\n",
      "  -1.63410398e-02  2.42421519e-02 -1.27343171e-01 -4.62207033e-01\n",
      "   8.22498091e-02 -1.11071601e-01]\n",
      " [ 3.80170155e-01 -5.38436173e-01  2.40892300e-01  3.84450508e-01\n",
      "  -5.95548780e-01 -1.34578746e-01  4.97259186e-01  2.45105812e-01\n",
      "  -1.52554449e-01  1.09200294e-01]\n",
      " [-1.04987408e-02 -1.65235790e-01 -1.05040682e-01 -1.29315380e-01\n",
      "  -3.30004035e-01  2.14952155e-01  2.55396945e-01 -5.04538417e-02\n",
      "   3.79742108e-01  6.32948025e-01]\n",
      " [ 4.69717409e-01  8.26783586e-01  3.82476235e-01  1.64683343e-01\n",
      "   7.40580914e-01 -1.92332312e-01 -1.48448196e-01 -2.97201190e-01\n",
      "   6.40392203e-04  9.38768551e-02]\n",
      " [-3.60891961e-02  3.35542074e-01  1.87053660e-01  1.37673020e-01\n",
      "  -1.51059334e-01 -4.45946057e-01 -2.84775224e-02  1.23643831e-01\n",
      "   2.87836590e-01 -5.36370294e-02]\n",
      " [-3.64995743e-01 -1.95594701e-01 -1.46555604e-01 -3.17037022e-01\n",
      "  -7.14588215e-01  3.57256455e-02 -7.96943262e-02  2.35029098e-01\n",
      "  -5.49006298e-02 -1.11832616e-01]\n",
      " [-3.78079069e-01 -3.14877566e-01 -6.22493866e-02  1.27428819e-01\n",
      "  -6.63951297e-01  4.54476666e-01  2.53083492e-01  2.20523465e-01\n",
      "   1.17697304e-01  1.11578845e-01]\n",
      " [-9.79006455e-02 -7.69192381e-02  6.24268021e-01 -2.40500909e-02\n",
      "  -1.60202090e-01 -2.09280750e-01 -5.38171197e-01  1.16492423e-01\n",
      "   7.00745106e-02 -4.98894379e-01]]\n",
      "now loss =  0.0448369723040158  accuracy =  0.917\n",
      "gradient of first layer [[-8.03941936e-02 -2.63545779e-01  6.96312003e-02  2.93681043e-01\n",
      "  -9.46455686e-02 -2.42072104e-01 -2.79251443e-01 -1.58902121e-01\n",
      "   2.21844075e-01  2.12481449e-01]\n",
      " [ 2.25931244e-02 -6.42368288e-01  5.10984298e-01  8.98944651e-02\n",
      "   8.49190406e-01  8.19788778e-02  1.94566850e-01  1.48943207e-01\n",
      "   8.60524584e-02 -5.10379700e-02]\n",
      " [ 3.81543197e-02  1.90710047e-01  3.99139964e-01 -4.17682647e-01\n",
      "  -1.41789191e-02  2.55059887e-02 -1.26525766e-01 -4.63747733e-01\n",
      "   8.23531350e-02 -1.10919146e-01]\n",
      " [ 3.80067516e-01 -5.38409952e-01  2.40859063e-01  3.84475848e-01\n",
      "  -5.95753277e-01 -1.34700855e-01  4.97179868e-01  2.45252634e-01\n",
      "  -1.52566257e-01  1.09183085e-01]\n",
      " [-5.99652230e-03 -1.66303990e-01 -1.03615517e-01 -1.30394524e-01\n",
      "  -3.21031251e-01  2.20207151e-01  2.58801425e-01 -5.68523684e-02\n",
      "   3.80185617e-01  6.33594049e-01]\n",
      " [ 4.71399626e-01  8.26363008e-01  3.83012592e-01  1.64269109e-01\n",
      "   7.43928209e-01 -1.90338849e-01 -1.47154888e-01 -2.99605430e-01\n",
      "   8.25148998e-04  9.41471163e-02]\n",
      " [-4.35030308e-02  3.37419924e-01  1.84665222e-01  1.39500491e-01\n",
      "  -1.65825773e-01 -4.54751892e-01 -3.41945562e-02  1.34244159e-01\n",
      "   2.86999588e-01 -5.48589792e-02]\n",
      " [-3.65044933e-01 -1.95582263e-01 -1.46571437e-01 -3.17024918e-01\n",
      "  -7.14686206e-01  3.56672558e-02 -7.97322149e-02  2.35099404e-01\n",
      "  -5.49061346e-02 -1.11840682e-01]\n",
      " [-3.76536497e-01 -3.15235150e-01 -6.17631244e-02  1.27062423e-01\n",
      "  -6.60876226e-01  4.56265445e-01  2.54242615e-01  2.18335718e-01\n",
      "   1.17842923e-01  1.11788856e-01]\n",
      " [-9.79830436e-02 -7.68983469e-02  6.24241438e-01 -2.40297970e-02\n",
      "  -1.60366241e-01 -2.09378606e-01 -5.38234734e-01  1.16610224e-01\n",
      "   7.00651909e-02 -4.98907975e-01]]\n",
      "now loss =  0.09879578861050645  accuracy =  0.915\n",
      "gradient of first layer [[-0.08014796 -0.26360073  0.06970404  0.29362173 -0.09415602 -0.24178778\n",
      "  -0.27907002 -0.15925048  0.22186129  0.21251089]\n",
      " [ 0.02404519 -0.64269574  0.51142126  0.08955158  0.8520881   0.08365581\n",
      "   0.19563375  0.1468933   0.08615216 -0.05086124]\n",
      " [ 0.03755081  0.19089299  0.39893244 -0.41751859 -0.01537066  0.02474446\n",
      "  -0.1270252  -0.46286454  0.08224717 -0.11107319]\n",
      " [ 0.38002848 -0.53839968  0.240846    0.38448537 -0.59583152 -0.13474741\n",
      "   0.49714978  0.2453083  -0.15257072  0.10917621]\n",
      " [-0.00756825 -0.16580685 -0.10416356 -0.12995647 -0.32412365  0.21818983\n",
      "   0.25747358 -0.05453529  0.3798793   0.63315208]\n",
      " [ 0.47423723  0.82565041  0.38391234  0.16356949  0.74958286 -0.18697098\n",
      "  -0.14498765 -0.30365396  0.00111499  0.09460586]\n",
      " [-0.04973676  0.33901977  0.18265406  0.14104137 -0.17826478 -0.46218108\n",
      "  -0.03898296  0.14314593  0.28632696 -0.05591335]\n",
      " [-0.36506967 -0.19557595 -0.14657946 -0.31701887 -0.71473564  0.03563788\n",
      "  -0.07975115  0.23513467 -0.05490876 -0.11184479]\n",
      " [-0.37761425 -0.31491431 -0.06213019  0.12735123 -0.66300835  0.45491396\n",
      "   0.25335906  0.21990795  0.11766424  0.111526  ]\n",
      " [-0.09802539 -0.07688745  0.6242276  -0.02401944 -0.16045092 -0.20942897\n",
      "  -0.53826721  0.1166706   0.0700606  -0.49891513]]\n",
      "now loss =  0.04067432135328878  accuracy =  0.915\n",
      "gradient of first layer [[-0.07967927 -0.26370844  0.06984811  0.29351101 -0.0932175  -0.24124594\n",
      "  -0.27872571 -0.15991103  0.22189376  0.21256995]\n",
      " [ 0.0245177  -0.64275304  0.51154     0.08945952  0.85303792  0.08413751\n",
      "   0.19592775  0.1462551   0.08612828 -0.05087544]\n",
      " [ 0.03663156  0.19117904  0.39861537 -0.4172692  -0.0171953   0.02357817\n",
      "  -0.12778285 -0.46152261  0.08209453 -0.11130957]\n",
      " [ 0.37998444 -0.5383876   0.24083141  0.38449643 -0.59591958 -0.13480079\n",
      "   0.49711545  0.2453714  -0.15257596  0.10916773]\n",
      " [-0.00875108 -0.16533073 -0.10461882 -0.12958414 -0.3264387   0.21653281\n",
      "   0.25637282 -0.05273563  0.37954621  0.6326655 ]\n",
      " [ 0.476921    0.82497369  0.38476355  0.16291226  0.75494637 -0.18378603\n",
      "  -0.14295391 -0.30747159  0.0013672   0.09503882]\n",
      " [-0.05665422  0.34080935  0.18042799  0.14274495 -0.19209901 -0.47043834\n",
      "  -0.04426352  0.15300219  0.28563223 -0.05709112]\n",
      " [-0.36510055 -0.19556798 -0.1465894  -0.31701129 -0.71479742  0.03560105\n",
      "  -0.07977469  0.23517864 -0.05491183 -0.11185001]\n",
      " [-0.37795361 -0.31474402 -0.06227544  0.12747312 -0.66366372  0.45439083\n",
      "   0.25300508  0.22044621  0.11752768  0.11133149]\n",
      " [-0.09806803 -0.0768762   0.62421372 -0.02400889 -0.16053622 -0.20948011\n",
      "  -0.53829998  0.11673146  0.07005607 -0.49892269]]\n",
      "now loss =  0.06437159758782053  accuracy =  0.917\n",
      "gradient of first layer [[-0.07917275 -0.26383553  0.07000641  0.29338781 -0.09220302 -0.24064556\n",
      "  -0.27834669 -0.1606288   0.22193411  0.21264928]\n",
      " [ 0.02691418 -0.64335228  0.51229285  0.08888076  0.8578374   0.08697506\n",
      "   0.197722    0.14285898  0.08632483 -0.05049824]\n",
      " [ 0.03688825  0.19112583  0.39869284 -0.41732392 -0.01667756  0.02386657\n",
      "  -0.12760335 -0.46187882  0.08210333 -0.11128513]\n",
      " [ 0.37998205 -0.53838683  0.24083049  0.38449703 -0.59592446 -0.13480376\n",
      "   0.49711356  0.2453748  -0.15257628  0.10916712]\n",
      " [-0.007369   -0.16564309 -0.10419344 -0.12989507 -0.3236551   0.21811928\n",
      "   0.25736504 -0.05466804  0.37961708  0.632831  ]\n",
      " [ 0.47978462  0.82424235  0.38566978  0.16221305  0.76068311 -0.18037835\n",
      "  -0.14079679 -0.31153624  0.00161367  0.095506  ]\n",
      " [-0.06255708  0.34232814  0.17854672  0.14418754 -0.20393039 -0.47747009\n",
      "  -0.04872078  0.16138395  0.28510948 -0.05806648]\n",
      " [-0.36513184 -0.19555995 -0.14659938 -0.31700365 -0.71486013  0.03556381\n",
      "  -0.07979831  0.23522307 -0.05491461 -0.11185515]\n",
      " [-0.37745338 -0.31485288 -0.06212387  0.12736225 -0.66265591  0.45496008\n",
      "   0.25335914  0.21974952  0.11754783  0.11138615]\n",
      " [-0.09810351 -0.07686706  0.62420238 -0.02400023 -0.16060735 -0.20952237\n",
      "  -0.53832678  0.11678184  0.0700529  -0.49892857]]\n",
      "now loss =  0.03627819375121685  accuracy =  0.927\n",
      "gradient of first layer [[-0.0792496  -0.26381177  0.06998017  0.29340791 -0.09235632 -0.24074265\n",
      "  -0.278409   -0.16051723  0.22192278  0.21263041]\n",
      " [ 0.02720677 -0.64340451  0.51236699  0.08881578  0.85841958  0.0873005\n",
      "   0.19791841  0.14245383  0.08632174 -0.05047935]\n",
      " [ 0.03598288  0.1913782   0.39839315 -0.41709519 -0.01849197  0.02276183\n",
      "  -0.12830467 -0.46058478  0.08200399 -0.11146775]\n",
      " [ 0.37995484 -0.53837952  0.24082157  0.38450374 -0.59597909 -0.13483651\n",
      "   0.49709276  0.24541354 -0.15257899  0.10916217]\n",
      " [-0.00961996 -0.16499661 -0.10494551 -0.12931664 -0.32815717  0.21534258\n",
      "   0.25559717 -0.05143603  0.3793439   0.63234238]\n",
      " [ 0.48217084  0.82363478  0.3864256   0.16163363  0.7654724  -0.17754239\n",
      "  -0.13901238 -0.31491395  0.00180383  0.09589492]\n",
      " [-0.07002769  0.34424969  0.17616424  0.14600448 -0.21893318 -0.48636898\n",
      "  -0.05432039  0.17196175  0.28449966 -0.05931095]\n",
      " [-0.36516869 -0.19555046 -0.14661112 -0.31699468 -0.71493414  0.03551987\n",
      "  -0.07982594  0.23527526 -0.05491761 -0.11186132]\n",
      " [-0.37865744 -0.31451175 -0.06252418  0.12766914 -0.66506529  0.45348216\n",
      "   0.25241783  0.22147603  0.11740621  0.11113411]\n",
      " [-0.09814816 -0.07685548  0.6241881  -0.02398933 -0.16069699 -0.20957569\n",
      "  -0.53836037  0.11684513  0.07004912 -0.49893614]]\n",
      "now loss =  0.028366291381367377  accuracy =  0.927\n",
      "gradient of first layer [[-0.07873181 -0.26394211  0.07014422  0.29328385 -0.09131228 -0.24013136\n",
      "  -0.27802887 -0.16124505  0.22195541  0.21271086]\n",
      " [ 0.02896417 -0.64384688  0.51291852  0.08838719  0.86194608  0.08938772\n",
      "   0.19922079  0.139973    0.08644678 -0.05019123]\n",
      " [ 0.03612941  0.19134413  0.39843921 -0.41712659 -0.01819396  0.02292923\n",
      "  -0.12819952 -0.46078914  0.0820109  -0.11145283]\n",
      " [ 0.37994898 -0.53837783  0.24081954  0.384505   -0.59599127 -0.13484345\n",
      "   0.4970885   0.24542168 -0.15257926  0.10916134]\n",
      " [-0.00801369 -0.16539574 -0.10443385 -0.12968494 -0.32490204  0.21722026\n",
      "   0.25676775 -0.05368709  0.37943628  0.63256356]\n",
      " [ 0.48517017  0.82286372  0.38737215  0.16090234  0.7714974  -0.17396396\n",
      "  -0.13677832 -0.31915404  0.0020253   0.09639627]\n",
      " [-0.07622636  0.34584422  0.17419368  0.147504   -0.23140806 -0.49374807\n",
      "  -0.05892637  0.18071234  0.28405024 -0.06033362]\n",
      " [-0.36520607 -0.19554085 -0.14662301 -0.31698567 -0.71500941  0.03547543\n",
      "  -0.07985365  0.23532798 -0.05492025 -0.11186744]\n",
      " [-0.37784003 -0.31471116 -0.06226213  0.12748447 -0.66340284  0.45442877\n",
      "   0.25300447  0.22033729  0.11744448  0.1112402 ]\n",
      " [-0.09818923 -0.07684488  0.62417499 -0.02397946 -0.16077979 -0.20962451\n",
      "  -0.5383908   0.11690302  0.07004625 -0.49894283]]\n",
      "now loss =  0.057720095872821967  accuracy =  0.929\n",
      "gradient of first layer [[-0.07887625 -0.26389974  0.07009672  0.29331995 -0.09160303 -0.2403104\n",
      "  -0.27814104 -0.16103841  0.22194075  0.21268067]\n",
      " [ 0.03019673 -0.6441447   0.51329027  0.08809262  0.8644242   0.09083996\n",
      "   0.20010908  0.13824578  0.08650002 -0.0500088 ]\n",
      " [ 0.03583904  0.19142963  0.39834217 -0.41705127 -0.01877315  0.02256631\n",
      "  -0.12843011 -0.46037033  0.08197323 -0.11152095]\n",
      " [ 0.3799476  -0.5383772   0.24081879  0.38450531 -0.59599424 -0.13484526\n",
      "   0.49708728  0.24542367 -0.15257964  0.10916081]\n",
      " [-0.00827221 -0.16529008 -0.10452964 -0.1296012  -0.32539721  0.21684639\n",
      "   0.25651663 -0.05328488  0.37935225  0.63244654]\n",
      " [ 0.48812266  0.82210787  0.3883004   0.16019016  0.77744215 -0.17044647\n",
      "  -0.13460096 -0.3233133   0.00221403  0.09688252]\n",
      " [-0.08295923  0.3475812   0.17205976  0.14912493 -0.24497748 -0.50177521\n",
      "  -0.06390054  0.19019748  0.28360707 -0.0614529 ]\n",
      " [-0.36523859 -0.19553249 -0.14663333 -0.31697786 -0.71507497  0.03543671\n",
      "  -0.07987765  0.23537376 -0.05492237 -0.11187282]\n",
      " [-0.37829945 -0.31457039 -0.06241563  0.12760605 -0.66431676  0.45384574\n",
      "   0.25263203  0.22100504  0.11737841  0.1111263 ]\n",
      " [-0.09822212 -0.07683638  0.62416449 -0.02397158 -0.16084615 -0.20966369\n",
      "  -0.53841508  0.11694933  0.07004408 -0.49894831]]\n",
      "now loss =  0.06322078370331931  accuracy =  0.936\n",
      "gradient of first layer [[-0.07928456 -0.26378973  0.06996448  0.2934193  -0.09242718 -0.24080373\n",
      "  -0.27844549 -0.16046241  0.22191141  0.21260472]\n",
      " [ 0.03004347 -0.6440866   0.51322424  0.08813458  0.86410929  0.09064076\n",
      "   0.19997762  0.13846835  0.08646741 -0.05005729]\n",
      " [ 0.03514382  0.19162412  0.39811314 -0.41687801 -0.02017526  0.02171959\n",
      "  -0.12895838 -0.45938393  0.08191225 -0.11165665]\n",
      " [ 0.37991845 -0.53836942  0.24080936  0.38451237 -0.59605315 -0.13488025\n",
      "   0.49706562  0.24546472 -0.15258165  0.10915571]\n",
      " [-0.00969232 -0.16486782 -0.10501281 -0.12923744 -0.32825651  0.21508558\n",
      "   0.25540676 -0.05125318  0.37919158  0.63213105]\n",
      " [ 0.49081645  0.82142112  0.38914025  0.15954498  0.7828779  -0.16723885\n",
      "  -0.13263346 -0.32709511  0.00235583  0.09731655]\n",
      " [-0.09081753  0.3496106   0.16958159  0.15101267 -0.26084232 -0.51115688\n",
      "  -0.06966649  0.20123883  0.28315768 -0.06275877]\n",
      " [-0.36528004 -0.19552174 -0.14664644 -0.31696789 -0.71515866  0.0353872\n",
      "  -0.07990811  0.23543203 -0.05492481 -0.11187973]\n",
      " [-0.37938696 -0.31426255 -0.06277626  0.12787737 -0.66650923  0.45251479\n",
      "   0.25180198  0.22254994  0.11727849  0.11090519]\n",
      " [-0.09827372 -0.07682297  0.62414814 -0.02395916 -0.16095034 -0.20972534\n",
      "  -0.53845305  0.11702187  0.07004099 -0.49895696]]\n",
      "now loss =  0.06328166372630367  accuracy =  0.937\n",
      "gradient of first layer [[-0.07925269 -0.26379312  0.06997189  0.2934131  -0.09236247 -0.24077202\n",
      "  -0.27842729 -0.16050442  0.22190756  0.21260273]\n",
      " [ 0.03235367 -0.64465676  0.51393325  0.087586    0.86877599  0.09337279\n",
      "   0.20163558  0.13524316  0.0865533  -0.04970415]\n",
      " [ 0.03548972  0.19155051  0.39821555 -0.41695316 -0.01946969  0.02211036\n",
      "  -0.12872651 -0.45985619  0.08190715 -0.11162415]\n",
      " [ 0.37991802 -0.53836908  0.24080905  0.3845125  -0.59605409 -0.13488101\n",
      "   0.49706512  0.24546539 -0.15258188  0.10915533]\n",
      " [-0.00772728 -0.16531716 -0.10441622 -0.1296793  -0.32425963  0.21734884\n",
      "   0.25676465 -0.05396047  0.37920867  0.63236597]\n",
      " [ 0.49427855  0.82053926  0.39022016  0.15871478  0.78987047 -0.16311183\n",
      "  -0.13011796 -0.33194436  0.00251985  0.09788454]\n",
      " [-0.09646692  0.3510665   0.16780209  0.15236777 -0.27225989 -0.51790668\n",
      "  -0.07378611  0.20915634  0.28287041 -0.06370881]\n",
      " [-0.36531039 -0.19551394 -0.146656   -0.31696062 -0.71522001  0.03535097\n",
      "  -0.07993021  0.23547453 -0.05492631 -0.11188481]\n",
      " [-0.37850084 -0.31446056 -0.06250927  0.12767957 -0.66470635  0.45352922\n",
      "   0.25240919  0.22133195  0.11728094  0.11100476]\n",
      " [-0.09830541 -0.07681474  0.6241381  -0.02395156 -0.16101442 -0.20976327\n",
      "  -0.53847619  0.11706628  0.07003935 -0.49896237]]\n",
      "now loss =  0.04992340531803986  accuracy =  0.94\n",
      "gradient of first layer [[-0.07975098 -0.26365938  0.06981371  0.29353439 -0.09337012 -0.2413758\n",
      "  -0.27879414 -0.15980456  0.22188057  0.21251111]\n",
      " [ 0.03191223 -0.64451059  0.51377873  0.0877027   0.86788385  0.0928046\n",
      "   0.20128152  0.13587823  0.08649885 -0.04982122]\n",
      " [ 0.03477177  0.19175044  0.39798548 -0.41677179 -0.02091281  0.02122754\n",
      "  -0.12926911 -0.45883809  0.08185146 -0.11177193]\n",
      " [ 0.37988344 -0.53835996  0.24079798  0.38452083 -0.5961241  -0.13492259\n",
      "   0.49703979  0.24551386 -0.15258374  0.10914917]\n",
      " [-0.00894665 -0.16494802 -0.10481305 -0.1293527  -0.32669109  0.21580169\n",
      "   0.25580116 -0.05220337  0.37907008  0.63206504]\n",
      " [ 0.49682196  0.81989702  0.39101215  0.15810766  0.79501379 -0.16008241\n",
      "  -0.12828313 -0.3354965   0.00262276  0.09830062]\n",
      " [-0.10455224  0.35313571  0.16526522  0.15429915 -0.28862337 -0.52756624\n",
      "  -0.07963698  0.2204532   0.28252615 -0.06506418]\n",
      " [-0.36535901 -0.19550155 -0.14667126 -0.31694902 -0.7153184   0.03529295\n",
      "  -0.07996536  0.23554245 -0.05492838 -0.11189293]\n",
      " [-0.37922299 -0.3142427  -0.06274355  0.12787029 -0.66615187  0.45261631\n",
      "   0.25184376  0.22236852  0.11720682  0.11083332]\n",
      " [-0.09836214 -0.0768002   0.62412023 -0.023938   -0.16112922 -0.20983105\n",
      "  -0.5385173   0.11714557  0.07003681 -0.49897196]]\n",
      "now loss =  0.0632601438914771  accuracy =  0.939\n",
      "gradient of first layer [[-0.08009193 -0.26356914  0.06970421  0.29361711 -0.09405936 -0.24178796\n",
      "  -0.27904444 -0.15932643  0.22186146  0.21244678]\n",
      " [ 0.03321558 -0.6448213   0.51416117  0.08739527  0.87052022  0.09433892\n",
      "   0.20219455  0.13407088  0.0865106  -0.04964299]\n",
      " [ 0.03438727  0.19186298  0.39785681 -0.4166727  -0.02168578  0.02074793\n",
      "  -0.12956516 -0.45829042  0.08181385 -0.11186254]\n",
      " [ 0.37987331 -0.53835727  0.24079461  0.3845232  -0.59614482 -0.13493471\n",
      "   0.49703249  0.24552789 -0.15258413  0.10914739]\n",
      " [-0.00935634 -0.16479388 -0.10496623 -0.1292274  -0.32749183  0.21523877\n",
      "   0.25543475 -0.0515861   0.37896714  0.63190153]\n",
      " [ 0.50052773  0.81895986  0.39215896  0.15722361  0.80252282 -0.155662\n",
      "  -0.12562974 -0.34065556  0.00273462  0.09890172]\n",
      " [-0.11023144  0.35458675  0.1634848   0.15565568 -0.30013526 -0.53435492\n",
      "  -0.08372217  0.22836575  0.28232505 -0.06601609]\n",
      " [-0.36538977 -0.19549371 -0.14668089 -0.31694169 -0.71538079  0.03525623\n",
      "  -0.07998744  0.23558527 -0.05492941 -0.11189802]\n",
      " [-0.37943906 -0.31416375 -0.06282458  0.12793432 -0.66657615  0.45232278\n",
      "   0.25165369  0.2226915   0.11715571  0.11075006]\n",
      " [-0.09839452 -0.0767919   0.62411001 -0.0239303  -0.16119498 -0.20986972\n",
      "  -0.53854055  0.11719062  0.07003573 -0.49897736]]\n",
      "now loss =  0.06658185468485849  accuracy =  0.945\n",
      "gradient of first layer [[-0.08058548 -0.26343626  0.06954671  0.2937375  -0.0950579  -0.24238886\n",
      "  -0.27940705 -0.15863458  0.22183617  0.21235154]\n",
      " [ 0.03256245 -0.64462825  0.51393175  0.08755455  0.86918595  0.09352967\n",
      "   0.20170158  0.13498708  0.0864602  -0.04979158]\n",
      " [ 0.03173236  0.19254381  0.39702249 -0.41603451 -0.02706318  0.0175533\n",
      "  -0.13147612 -0.45459383  0.08172156 -0.11233768]\n",
      " [ 0.37986606 -0.53835524  0.24079212  0.38452499 -0.59615961 -0.13494355\n",
      "   0.49702711  0.24553803 -0.1525846   0.10914589]\n",
      " [-0.01718851 -0.16276633 -0.10742814 -0.127335   -0.34334245  0.20578056\n",
      "   0.24977311 -0.04066341  0.37867113  0.63046573]\n",
      " [ 0.50373782  0.81816069  0.39315198  0.15645765  0.80902504 -0.15183418\n",
      "  -0.12334382 -0.34511398  0.00281888  0.09943264]\n",
      " [-0.11691919  0.35627115  0.16138721  0.15724571 -0.31370233 -0.54233938\n",
      "  -0.08849854  0.23765272  0.28212867 -0.06714249]\n",
      " [-0.3654258  -0.19548473 -0.14669215 -0.31693314 -0.71545388  0.03521332\n",
      "  -0.08001308  0.23563526 -0.05493037 -0.11190398]\n",
      " [-0.38268049 -0.31330906 -0.06384978  0.12872244 -0.67313362  0.44838868\n",
      "   0.24929337  0.2272221   0.11701497  0.11013521]\n",
      " [-0.09842297 -0.07678473  0.62410103 -0.02392353 -0.16125271 -0.20990368\n",
      "  -0.53856088  0.11723011  0.07003485 -0.49898219]]\n",
      "now loss =  0.021880690070338285  accuracy =  0.95\n",
      "gradient of first layer [[-0.08138618 -0.26322115  0.06928388  0.29393008 -0.09668631 -0.24336223\n",
      "  -0.27999034 -0.15751937  0.22179986  0.21219359]\n",
      " [ 0.03517836 -0.64520429  0.51466944  0.0869512   0.8744747   0.09657887\n",
      "   0.20347664  0.1313937   0.08641289 -0.04946407]\n",
      " [ 0.03272898  0.19235462  0.39728528 -0.41625397 -0.02504762  0.01867592\n",
      "  -0.13083135 -0.45594736  0.08166903 -0.11225952]\n",
      " [ 0.37984761 -0.53835035  0.24078599  0.38452937 -0.59619721 -0.1349658\n",
      "   0.49701374  0.24556365 -0.15258543  0.10914236]\n",
      " [-0.01403374 -0.16334258 -0.10660627 -0.12802116 -0.33696018  0.20930126\n",
      "   0.25179194 -0.04493652  0.37848339  0.63067704]\n",
      " [ 0.50685094  0.81741071  0.3940986   0.15572955  0.81534084 -0.14814702\n",
      "  -0.12116405 -0.34941424  0.00285215  0.09991303]\n",
      " [-0.12264721  0.35772183  0.15956954  0.15859879 -0.32534233 -0.54919482\n",
      "  -0.0925841   0.24559212  0.28196691 -0.06813895]\n",
      " [-0.36546601 -0.19547467 -0.14670477 -0.31692366 -0.71553556  0.03516532\n",
      "  -0.08004162  0.23569093 -0.05493131 -0.11191077]\n",
      " [-0.38136462 -0.31353305 -0.06352174  0.12843921 -0.67047604  0.44984016\n",
      "   0.25011914  0.22544597  0.11691674  0.11020028]\n",
      " [-0.09846334 -0.07677447  0.62408817 -0.02391399 -0.16133478 -0.209952\n",
      "  -0.53858969  0.11728604  0.07003369 -0.49898925]]\n",
      "now loss =  0.09016171126812342  accuracy =  0.948\n",
      "gradient of first layer [[-0.08219228 -0.26301677  0.06902985  0.29412126 -0.09832598 -0.24433002\n",
      "  -0.28056304 -0.15640438  0.22178335  0.21205327]\n",
      " [ 0.03700879 -0.64561632  0.51520471  0.08653037  0.87819219  0.09872148\n",
      "   0.2047206   0.12888787  0.08638105 -0.04921838]\n",
      " [ 0.03298589  0.19231813  0.39734731 -0.41630389 -0.02452307  0.01895042\n",
      "  -0.13068132 -0.45628593  0.08163598 -0.11225768]\n",
      " [ 0.37983758 -0.53834774  0.24078272  0.38453177 -0.59621765 -0.13497787\n",
      "   0.49700653  0.24557755 -0.15258576  0.10914053]\n",
      " [-0.01255931 -0.16360904 -0.10620903 -0.12832977 -0.33395171  0.21093917\n",
      "   0.25271874 -0.04691346  0.37837416  0.63076998]\n",
      " [ 0.51017711  0.81660303  0.39511857  0.15494751  0.82210217 -0.14419428\n",
      "  -0.11883867 -0.35399844  0.00287487  0.10043903]\n",
      " [-0.12816365  0.35909404  0.15784525  0.1598987  -0.33656511 -0.55577999\n",
      "  -0.0964764   0.25320965  0.28188155 -0.06905553]\n",
      " [-0.36550362 -0.1954654  -0.14671649 -0.31691484 -0.71561209  0.03512053\n",
      "  -0.08006807  0.23574283 -0.05493181 -0.11191691]\n",
      " [-0.38063391 -0.31366184 -0.06332623  0.12828662 -0.66898576  0.4506473\n",
      "   0.2505759   0.22446716  0.11686081  0.11024211]\n",
      " [-0.0984937  -0.0767669   0.62407862 -0.02390684 -0.16139658 -0.20998825\n",
      "  -0.53861113  0.11732798  0.07003318 -0.49899433]]\n",
      "now loss =  0.06998391297181794  accuracy =  0.946\n",
      "gradient of first layer [[-0.08291332 -0.26283598  0.06880278  0.29429212 -0.09979306 -0.24519573\n",
      "  -0.28107309 -0.15540953  0.22177182  0.21192687]\n",
      " [ 0.03708701 -0.64558696  0.51519389  0.08652474  0.87834687  0.09876405\n",
      "   0.20472362  0.1288052   0.08631969 -0.04926945]\n",
      " [ 0.03230572  0.19251673  0.39711532 -0.41613303 -0.02590562  0.01810169\n",
      "  -0.13119349 -0.45533103  0.08158851 -0.11241667]\n",
      " [ 0.3798078  -0.53834043  0.24077335  0.38453874 -0.59627837 -0.13501336\n",
      "   0.49698568  0.24561848 -0.15258601  0.10913559]\n",
      " [-0.01309459 -0.16336214 -0.10644815 -0.12816377 -0.3350316   0.21016092\n",
      "   0.25221374 -0.04610823  0.37821745  0.6305076 ]\n",
      " [ 0.51318331  0.81588301  0.39603627  0.15424299  0.82821593 -0.14062274\n",
      "  -0.11675001 -0.35812966  0.00287524  0.10091289]\n",
      " [-0.13460555  0.36067684  0.15583748  0.16141409 -0.34968022 -0.56346797\n",
      "  -0.10099374  0.26207707  0.2818261  -0.07012352]\n",
      " [-0.36555122 -0.19545386 -0.14673128 -0.3169037  -0.71570904  0.0350639\n",
      "  -0.08010127  0.23580822 -0.05493202 -0.11192461]\n",
      " [-0.3803606  -0.31366776 -0.06327762  0.12824339 -0.66842332  0.45089803\n",
      "   0.25069895  0.22412747  0.1167839   0.11019537]\n",
      " [-0.09854456 -0.07675451  0.62406273 -0.02389494 -0.16150022 -0.21004879\n",
      "  -0.53864666  0.11739784  0.07003287 -0.49900265]]\n",
      "now loss =  0.04138005099148804  accuracy =  0.959\n",
      "gradient of first layer [[-0.08411048 -0.26254136  0.06842439  0.29457271 -0.10223616 -0.24662534\n",
      "  -0.2819108  -0.15376618  0.22176405  0.21172507]\n",
      " [ 0.03809996 -0.64577357  0.51544968  0.08629356  0.88038754  0.0999231\n",
      "   0.20537071  0.1274357   0.08624537 -0.04917446]\n",
      " [ 0.03164694  0.19269922  0.39689235 -0.41597354 -0.02725162  0.01729259\n",
      "  -0.13167639 -0.45441633  0.08155824 -0.11255589]\n",
      " [ 0.37978581 -0.53833496  0.2407663   0.38454389 -0.59632332 -0.13503964\n",
      "   0.49697026  0.24564863 -0.15258619  0.10913183]\n",
      " [-0.01526441 -0.16276289 -0.10717319 -0.12763625 -0.33945561  0.20749131\n",
      "   0.25062241 -0.04309115  0.37811917  0.63004792]\n",
      " [ 0.51598428  0.81522161  0.39688969  0.15358771  0.83391785 -0.1373\n",
      "  -0.11481722 -0.36196694  0.00285661  0.1013492 ]\n",
      " [-0.14062786  0.36215544  0.15393239  0.1628246  -0.36197479 -0.57065215\n",
      "  -0.10520264  0.2703392   0.28179241 -0.07113124]\n",
      " [-0.36558975 -0.19544448 -0.14674334 -0.31689467 -0.71578762  0.03501797\n",
      "  -0.08012812  0.23586106 -0.0549321  -0.11193097]\n",
      " [-0.38128999 -0.31340028 -0.06359595  0.12847109 -0.67031935  0.44974418\n",
      "   0.25000567  0.22542572  0.11672729  0.10998528]\n",
      " [-0.09858221 -0.07674528  0.62405086 -0.02388611 -0.16157707 -0.21009373\n",
      "  -0.53867296  0.11744949  0.07003271 -0.49900894]]\n",
      "now loss =  0.049513955301332846  accuracy =  0.962\n",
      "gradient of first layer [[-0.08533673 -0.26224254  0.06803834  0.29485853 -0.10474503 -0.2480867\n",
      "  -0.2827619  -0.1520899   0.22176874  0.21152646]\n",
      " [ 0.03755815 -0.64560586  0.51523371  0.08642005  0.87924923  0.09925914\n",
      "   0.20496506  0.12818013  0.08620724 -0.04929423]\n",
      " [ 0.03168324  0.19270894  0.39688925 -0.41597892 -0.02718344  0.01732127\n",
      "  -0.13166812 -0.45445828  0.08153729 -0.11256825]\n",
      " [ 0.37975973 -0.5383286   0.240758    0.38454992 -0.59637677 -0.13507064\n",
      "   0.49695218  0.24568424 -0.15258612  0.10912764]\n",
      " [-0.01364902 -0.16308582 -0.10671486 -0.12800087 -0.3361698   0.20935795\n",
      "   0.25167888 -0.045268    0.37803516  0.63024   ]\n",
      " [ 0.51957774  0.81437588  0.39797862  0.15274443  0.84124011 -0.13302439\n",
      "  -0.11234624 -0.3668771   0.00280619  0.10191028]\n",
      " [-0.14691446  0.36367789  0.15195341  0.16428864 -0.37483738 -0.57813463\n",
      "  -0.10955828  0.27892688  0.28182317 -0.07214369]\n",
      " [-0.3656295  -0.19543491 -0.14675583 -0.31688546 -0.71586897  0.03497077\n",
      "  -0.08015559  0.23591533 -0.05493185 -0.11193726]\n",
      " [-0.38076357 -0.31349074 -0.06345585  0.12835555 -0.66925227  0.45033795\n",
      "   0.25033639  0.22472252  0.11668553  0.11003257]\n",
      " [-0.09862907 -0.07673397  0.62403609 -0.02387524 -0.16167299 -0.21014941\n",
      "  -0.53870537  0.11751347  0.07003298 -0.4990164 ]]\n",
      "now loss =  0.014853486071501328  accuracy =  0.963\n",
      "gradient of first layer [[-0.08645532 -0.26198147  0.06769488  0.29512014 -0.10702584 -0.24941726\n",
      "  -0.28353021 -0.15056457  0.22178367  0.2113471 ]\n",
      " [ 0.03949271 -0.64602965  0.51580145  0.08597107  0.88318254  0.10153981\n",
      "   0.20626743  0.12555207  0.08614697 -0.04901399]\n",
      " [ 0.03055339  0.19298118  0.39653502 -0.41571067 -0.0294888   0.01596864\n",
      "  -0.13245325 -0.45291412  0.0815418  -0.11276106]\n",
      " [ 0.37975052 -0.53832645  0.24075504  0.38455204 -0.59639568 -0.13508153\n",
      "   0.49694588  0.24569671 -0.15258599  0.10912618]\n",
      " [-0.01787746 -0.16207466 -0.10802916 -0.12699764 -0.34479058  0.20429912\n",
      "   0.24874736 -0.03948941  0.37806182  0.62952607]\n",
      " [ 0.52303601  0.81357794  0.39903445  0.1519359   0.84829093 -0.12891851\n",
      "  -0.10998027 -0.37158707  0.0027472   0.10245429]\n",
      " [-0.15093875  0.3646223   0.15070221  0.16522743 -0.3830563  -0.58291784\n",
      "  -0.11232504  0.28440962  0.28187063 -0.07279086]\n",
      " [-0.36567034 -0.19542549 -0.14676843 -0.31687595 -0.71595235  0.03492235\n",
      "  -0.08018352  0.23597088 -0.05493118 -0.11194367]\n",
      " [-0.38246447 -0.31307966 -0.06398545  0.12876028 -0.67271894  0.44829761\n",
      "   0.24915265  0.22705013  0.11669148  0.10974007]\n",
      " [-0.09866366 -0.07672597  0.62402534 -0.02386721 -0.16174371 -0.21019038\n",
      "  -0.53872902  0.11756048  0.07003354 -0.49902182]]\n",
      "now loss =  0.021960525182920323  accuracy =  0.963\n",
      "gradient of first layer [[-0.08834771 -0.26154471  0.06710859  0.29556059 -0.11089318 -0.25166645\n",
      "  -0.28482347 -0.14799492  0.22181963  0.21104442]\n",
      " [ 0.04007413 -0.64611933  0.51592807  0.0858365   0.88433951  0.10220826\n",
      "   0.20662586  0.12477149  0.08608095 -0.04896262]\n",
      " [ 0.03037022  0.19304354  0.39645933 -0.41566422 -0.02987295  0.01573666\n",
      "  -0.13259613 -0.45265973  0.08152338 -0.11281012]\n",
      " [ 0.37973449 -0.53832267  0.24074993  0.38455574 -0.59642855 -0.13510058\n",
      "   0.49693487  0.24571847 -0.15258578  0.10912357]\n",
      " [-0.01836803 -0.16189647 -0.10823422 -0.12686568 -0.3458167   0.20365839\n",
      "   0.24835271 -0.03880238  0.37800423  0.62937679]\n",
      " [ 0.52569503  0.81298139  0.39983845  0.15131851  0.85371573 -0.12577287\n",
      "  -0.10817986 -0.37519399  0.00267521  0.10285687]\n",
      " [-0.15672226  0.36596661  0.1488918   0.16656944 -0.39488736 -0.58978609\n",
      "  -0.11628501  0.29226436  0.28196226 -0.07372021]\n",
      " [-0.36571802 -0.19541458 -0.14678315 -0.31686489 -0.7160498   0.03486582\n",
      "  -0.080216    0.23603558 -0.05493017 -0.11195115]\n",
      " [-0.38271111 -0.31298825 -0.0640896   0.12882729 -0.67323483  0.44797202\n",
      "   0.2489519   0.22739623  0.11666072  0.10966161]\n",
      " [-0.0987081  -0.07671576  0.62401157 -0.02385691 -0.16183455 -0.21024309\n",
      "  -0.53875933  0.11762078  0.07003443 -0.49902884]]\n",
      "now loss =  0.03402436403390729  accuracy =  0.969\n",
      "gradient of first layer [[-0.08964603 -0.26124989  0.06670563  0.29586146 -0.11354997 -0.2532064\n",
      "  -0.28570554 -0.1462379   0.22185198  0.21084039]\n",
      " [ 0.04051533 -0.64618709  0.51603092  0.08573373  0.8852258   0.10270754\n",
      "   0.20689544  0.12418445  0.08602904 -0.04893022]\n",
      " [ 0.02964826  0.19321335  0.39623319 -0.41549446 -0.03134947  0.01487145\n",
      "  -0.13309284 -0.45167951  0.08153592 -0.11293231]\n",
      " [ 0.37972942 -0.53832149  0.24074818  0.38455686 -0.59643904 -0.13510654\n",
      "   0.49693139  0.24572529 -0.15258577  0.10912271]\n",
      " [-0.02016737 -0.1614698  -0.10879495 -0.12643958 -0.34949345  0.20149259\n",
      "   0.24711092 -0.03635664  0.37803523  0.62906625]\n",
      " [ 0.52874026  0.81230448  0.40075602  0.15060606  0.85993024 -0.12216494\n",
      "  -0.10612378 -0.37931499  0.00257566  0.10331793]\n",
      " [-0.16139823  0.36703889  0.14742879  0.16765393 -0.40445947 -0.59533775\n",
      "  -0.11947256  0.29859723  0.28206174 -0.07446739]\n",
      " [-0.3657599  -0.19540518 -0.1467962  -0.31685522 -0.71613551  0.03481631\n",
      "  -0.08024437  0.23609219 -0.05492912 -0.11195768]\n",
      " [-0.38311776 -0.31288056 -0.06421891  0.12892702 -0.67406574  0.44746737\n",
      "   0.24866081  0.22795469  0.11665971  0.10957913]\n",
      " [-0.09875182 -0.07670595  0.62399792 -0.02384683 -0.16192405 -0.21029477\n",
      "  -0.53878894  0.11767987  0.07003552 -0.49903566]]\n",
      "now loss =  0.03689757778293432  accuracy =  0.958\n",
      "gradient of first layer [[-0.09183106 -0.26074482  0.06601644  0.29637331 -0.11802475 -0.25581705\n",
      "  -0.28720229 -0.1432821   0.22189795  0.21047507]\n",
      " [ 0.0405405  -0.64606989  0.51592465  0.08575088  0.88524713  0.1026142\n",
      "   0.2067844   0.12419939  0.08586633 -0.04910257]\n",
      " [ 0.02936492  0.19335076  0.39608839 -0.41540741 -0.03193308  0.01444792\n",
      "  -0.13336528 -0.45126079  0.08144728 -0.11309341]\n",
      " [ 0.37971001 -0.53831666  0.24074181  0.38456152 -0.59647879 -0.13513017\n",
      "   0.49691771  0.24575174 -0.15258581  0.10911891]\n",
      " [-0.02041333 -0.16115605 -0.10906189 -0.12630415 -0.35000169  0.20088629\n",
      "   0.24666105 -0.03589426  0.37770624  0.62861562]\n",
      " [ 0.53074818  0.8118784   0.40134619  0.15013938  0.86402562 -0.11979974\n",
      "  -0.10478766 -0.3820202   0.00248155  0.10360015]\n",
      " [-0.16730635  0.3684297   0.14553433  0.16903813 -0.41657337 -0.60241244\n",
      "  -0.12354383  0.30659612  0.28215084 -0.07548496]\n",
      " [-0.36580678 -0.19539444 -0.14681091 -0.31684426 -0.71623151  0.03476039\n",
      "  -0.08027639  0.23615557 -0.05492801 -0.11196538]\n",
      " [-0.38265202 -0.31284547 -0.06417682  0.12885998 -0.67311656  0.44785212\n",
      "   0.24882462  0.22739579  0.11646645  0.10943378]\n",
      " [-0.09881335 -0.07669182  0.62397861 -0.02383242 -0.16205    -0.21036823\n",
      "  -0.53883101  0.11776308  0.07003691 -0.49904585]]\n",
      "now loss =  0.03598774691376453  accuracy =  0.965\n",
      "gradient of first layer [[-0.09348426 -0.26037228  0.06550098  0.29676025 -0.12141343 -0.25778555\n",
      "  -0.28832485 -0.14105469  0.22195057  0.21021293]\n",
      " [ 0.04245159 -0.6463942   0.5164209   0.08532144  0.8891262   0.10479619\n",
      "   0.20797876  0.121661    0.08567572 -0.04893231]\n",
      " [ 0.02930564  0.1934093   0.39603692 -0.41538062 -0.03206161  0.01432828\n",
      "  -0.13345128 -0.45116131  0.08139696 -0.11316368]\n",
      " [ 0.37971153 -0.53831669  0.24074201  0.38456121 -0.59647578 -0.13512862\n",
      "   0.49691845  0.24574979 -0.15258622  0.10911879]\n",
      " [-0.02047568 -0.16099843 -0.10918073 -0.12624569 -0.35014589  0.20065099\n",
      "   0.24647205 -0.03574514  0.37754314  0.62840921]\n",
      " [ 0.53396188  0.81118939  0.40230948  0.14939028  0.87059613 -0.11599963\n",
      "  -0.10263941 -0.38633981  0.00233299  0.10406606]\n",
      " [-0.17021687  0.36912856  0.14458209  0.16972451 -0.42255898 -0.60591206\n",
      "  -0.12556123  0.31053017  0.28218977 -0.07599854]\n",
      " [-0.3658392  -0.19538723 -0.146821   -0.3168367  -0.71629797  0.03472189\n",
      "  -0.08029832  0.23619918 -0.05492689 -0.11197044]\n",
      " [-0.38220887 -0.31286643 -0.06409529  0.12877844 -0.67221966  0.44829462\n",
      "   0.24904579  0.22683372  0.11636091  0.10939836]\n",
      " [-0.0988471  -0.07668421  0.62396801 -0.02382453 -0.16211922 -0.2104084\n",
      "  -0.53885394  0.11780852  0.07003796 -0.49905124]]\n",
      "now loss =  0.013539999670355856  accuracy =  0.973\n",
      "gradient of first layer [[-0.09568364 -0.25989972  0.06482919  0.29727306 -0.12591667 -0.26039187\n",
      "  -0.28980012 -0.13810397  0.22204731  0.20988277]\n",
      " [ 0.04238797 -0.6463639   0.51637868  0.08533262  0.88898171  0.10471706\n",
      "   0.20792312  0.12174949  0.08565573 -0.04895584]\n",
      " [ 0.02893824  0.19349648  0.39591863 -0.41529263 -0.03281505  0.01388372\n",
      "  -0.13370623 -0.45066485  0.0814035  -0.1132302 ]\n",
      " [ 0.3796951  -0.5383132   0.24073694  0.38456501 -0.59650947 -0.135148\n",
      "   0.49690747  0.24577177 -0.15258548  0.10911638]\n",
      " [-0.02158666 -0.16072665 -0.10953548 -0.12597394 -0.35241733  0.19928918\n",
      "   0.24569122 -0.03423682  0.37755647  0.62819342]\n",
      " [ 0.53676227  0.81059556  0.40315583  0.14873606  0.87632601 -0.11268587\n",
      "  -0.10076818 -0.39009396  0.0021983   0.10447684]\n",
      " [-0.17525616  0.37021161  0.14303206  0.17089473 -0.43288671 -0.61187378\n",
      "  -0.1289398   0.31728695  0.28240728 -0.07675117]\n",
      " [-0.36588491 -0.19537748 -0.14683495 -0.31682605 -0.71639158  0.03466779\n",
      "  -0.08032892  0.23626046 -0.05492481 -0.11197722]\n",
      " [-0.38273697 -0.31273234 -0.06426518  0.12890844 -0.67329901  0.44764148\n",
      "   0.24867013  0.22755368  0.11636268  0.10929026]\n",
      " [-0.09890105 -0.0766727   0.62395153 -0.02381197 -0.16222973 -0.21047223\n",
      "  -0.53889005  0.11788084  0.07004042 -0.49905923]]\n",
      "now loss =  0.031318780882591  accuracy =  0.972\n",
      "gradient of first layer [[-0.09836132 -0.25933447  0.06400629  0.29789731 -0.13140702 -0.26356037\n",
      "  -0.29158984 -0.13452512  0.22217754  0.20948557]\n",
      " [ 0.04261248 -0.64637792  0.51640288  0.08527665  0.88941225  0.10497041\n",
      "   0.2080465   0.12145238  0.0856045  -0.04895136]\n",
      " [ 0.0287427   0.19354708  0.39584863 -0.41524656 -0.03322097  0.01364536\n",
      "  -0.13384553 -0.45040112  0.08140174 -0.11326987]\n",
      " [ 0.37967243 -0.53830836  0.24072986  0.38457027 -0.59655606 -0.13517481\n",
      "   0.49689228  0.24580207 -0.15258445  0.10911299]\n",
      " [-0.02300455 -0.16040507 -0.10999807 -0.12564146 -0.35533443  0.19759066\n",
      "   0.24471945 -0.03233554  0.3775933   0.62794918]\n",
      " [ 0.53898652  0.81013576  0.40381563  0.14821302  0.88087206 -0.11005214\n",
      "  -0.09929016 -0.39306876  0.00207132  0.10479518]\n",
      " [-0.18005004  0.37124074  0.14153383  0.17200906 -0.4427349  -0.61754796\n",
      "  -0.13215582  0.32369441  0.28262028 -0.07747279]\n",
      " [-0.36594651 -0.19536455 -0.14685387 -0.31681172 -0.7165179   0.03459503\n",
      "  -0.08037001  0.23634275 -0.05492176 -0.11198625]\n",
      " [-0.38361493 -0.31253389 -0.06455194  0.1291138  -0.67510579  0.44659072\n",
      "   0.24806899  0.22873029  0.11638593  0.10913974]\n",
      " [-0.09897498 -0.07665715  0.62392881 -0.02379477 -0.16238136 -0.21055958\n",
      "  -0.53893939  0.11797961  0.07004406 -0.49907008]]\n",
      "now loss =  0.02625978775018214  accuracy =  0.968\n",
      "gradient of first layer [[-0.1002448  -0.25892721  0.06341454  0.29834134 -0.1352736  -0.26580666\n",
      "  -0.29286239 -0.13200761  0.22225767  0.20918559]\n",
      " [ 0.04416525 -0.64658963  0.51676965  0.08492646  0.8925591   0.10670853\n",
      "   0.20897096  0.11942185  0.08537607 -0.04887376]\n",
      " [ 0.02916137  0.19351583  0.39593103 -0.41533375 -0.03237088  0.01408075\n",
      "  -0.13362476 -0.45093448  0.08130705 -0.11329082]\n",
      " [ 0.37965123 -0.53830376  0.24072311  0.38457524 -0.59659962 -0.13520006\n",
      "   0.49687794  0.24583037 -0.15258361  0.10910958]\n",
      " [-0.02096601 -0.16064102 -0.10951676 -0.12607951 -0.35117404  0.19979101\n",
      "   0.24588099 -0.03496612  0.3772463   0.62796898]\n",
      " [ 0.54140413  0.80966003  0.40452037  0.14764644  0.88581401 -0.10720915\n",
      "  -0.09770501 -0.39628639  0.00190241  0.10511303]\n",
      " [-0.18330249  0.3719914   0.1404529   0.17277802 -0.44943685 -0.62145927\n",
      "  -0.13439982  0.32805469  0.28269001 -0.07805387]\n",
      " [-0.3659841  -0.19535644 -0.14686588 -0.31680293 -0.7165952   0.03455038\n",
      "  -0.08039538  0.23639292 -0.05492026 -0.11199222]\n",
      " [-0.38173337 -0.31282479 -0.0640474   0.12869409 -0.67125778  0.44870412\n",
      "   0.24921981  0.22626837  0.11616338  0.10927194]\n",
      " [-0.09903513 -0.07664437  0.62390992 -0.02378068 -0.16250488 -0.21063096\n",
      "  -0.5389798   0.11805986  0.07004679 -0.49907938]]\n",
      "now loss =  0.038064982199283524  accuracy =  0.977\n",
      "gradient of first layer [[-0.10289381 -0.25839079  0.06260809  0.29896319 -0.14070886 -0.26893609\n",
      "  -0.29461847 -0.12848779  0.22241997  0.20881075]\n",
      " [ 0.04380113 -0.6465098   0.51664429  0.08500709  0.89179832  0.10628405\n",
      "   0.20872705  0.11990246  0.08539085 -0.0489245 ]\n",
      " [ 0.02831711  0.19369343  0.39567126 -0.41513276 -0.03410091  0.01307363\n",
      "  -0.13419199 -0.44980811  0.08135029 -0.11342204]\n",
      " [ 0.37964147 -0.5383018   0.24072009  0.3845775  -0.59661972 -0.13521151\n",
      "   0.49687151  0.2458433  -0.15258299  0.10910827]\n",
      " [-0.02417059 -0.1599663  -0.11049704 -0.12531429 -0.35773319  0.19596223\n",
      "   0.24372586 -0.03068617  0.37740894  0.62746571]\n",
      " [ 0.54332508  0.80927042  0.40509884  0.14719234  0.88974853 -0.10493361\n",
      "  -0.09642981 -0.39884269  0.00178433  0.10538848]\n",
      " [-0.1880407   0.37295031  0.13900233  0.17388542 -0.45917005 -0.62704615\n",
      "  -0.13753723  0.33434465  0.28298174 -0.07871519]\n",
      " [-0.36604232 -0.19534472 -0.14688364 -0.3167893  -0.71671474  0.03448173\n",
      "  -0.0804339   0.23647019 -0.05491661 -0.11200032]\n",
      " [-0.38308231 -0.31253745 -0.06446057  0.12901671 -0.67401744  0.44708863\n",
      "   0.24830943  0.2280729   0.1162276   0.10905552]\n",
      " [-0.09909778 -0.07663174  0.6238908  -0.02376602 -0.16263354 -0.21070484\n",
      "  -0.53902126  0.11814302  0.07005071 -0.49908809]]\n",
      "now loss =  0.024359896886440236  accuracy =  0.976\n",
      "gradient of first layer [[-0.10559511 -0.25784186  0.06177177  0.29959807 -0.1462633  -0.2721335\n",
      "  -0.29641543 -0.12490752  0.22258447  0.20842239]\n",
      " [ 0.04422445 -0.64653686  0.51668146  0.08489973  0.89261809  0.10675786\n",
      "   0.20895231  0.1193465   0.0852747  -0.04893838]\n",
      " [ 0.02856683  0.19366831  0.39571946 -0.41519081 -0.03359933  0.01334928\n",
      "  -0.13405097 -0.4501315   0.08129965 -0.11342066]\n",
      " [ 0.37961791 -0.53829698  0.24071267  0.38458305 -0.59666822 -0.13523944\n",
      "   0.49685576  0.24587451 -0.15258165  0.10910477]\n",
      " [-0.02377038 -0.1599613  -0.11045994 -0.12540221 -0.35694039  0.1963593\n",
      "   0.2439062  -0.0311861   0.377269    0.62740377]\n",
      " [ 0.54563883  0.80882158  0.40577859  0.14664464  0.89448675 -0.10220412\n",
      "  -0.09491167 -0.40190783  0.0016084   0.10569238]\n",
      " [-0.19173292  0.37372929  0.13780254  0.17474587 -0.46679547 -0.63142171\n",
      "  -0.14002032  0.3392363   0.28315802 -0.079281  ]\n",
      " [-0.36609638 -0.19533377 -0.14690041 -0.31677659 -0.71682591  0.03441776\n",
      "  -0.08046986  0.23654181 -0.05491333 -0.11200811]\n",
      " [-0.38297038 -0.31251255 -0.0644756   0.12899321 -0.67380587  0.44717941\n",
      "   0.24833644  0.22794006  0.11615715  0.10900616]\n",
      " [-0.09917978 -0.07661522  0.62386551 -0.02374672 -0.16280207 -0.21080184\n",
      "  -0.53907572  0.11825164  0.07005584 -0.4990998 ]]\n",
      "now loss =  0.04414069048148289  accuracy =  0.98\n",
      "gradient of first layer [[-0.10787495 -0.25739684  0.06107732  0.30013741 -0.15094552 -0.27482528\n",
      "  -0.29792055 -0.12189524  0.22274373  0.20810737]\n",
      " [ 0.04619213 -0.64690965  0.51726021  0.08443245  0.89664824  0.10907686\n",
      "   0.21024016  0.11674699  0.08511811 -0.04868209]\n",
      " [ 0.02896189  0.19360179  0.39583437 -0.41528044 -0.03278622  0.01380208\n",
      "  -0.13380171 -0.45064707  0.08125884 -0.11338276]\n",
      " [ 0.37960172 -0.53829377  0.24070769  0.38458686 -0.59670149 -0.13525856\n",
      "   0.49684504  0.24589591 -0.1525806   0.10910249]\n",
      " [-0.02340452 -0.15999306 -0.11036228 -0.12547212 -0.35617628  0.19673428\n",
      "   0.24410342 -0.03164243  0.37719575  0.62738936]\n",
      " [ 0.54812833  0.80833793  0.40652755  0.14605392  0.89959394 -0.09926371\n",
      "  -0.0932709  -0.40519888  0.001428    0.10603179]\n",
      " [-0.19404521  0.37418407  0.13708538  0.17528975 -0.47155373 -0.6341478\n",
      "  -0.14154947  0.34228849  0.28331183 -0.07960336]\n",
      " [-0.3661509  -0.19532313 -0.14691704 -0.3167637  -0.7169379   0.03435343\n",
      "  -0.08050583  0.23661383 -0.05490952 -0.11201562]\n",
      " [-0.38274353 -0.31253684 -0.0644134   0.12894755 -0.67333352  0.4474181\n",
      "   0.24846377  0.2276541   0.11611719  0.10900444]\n",
      " [-0.09924873 -0.07660175  0.62384448 -0.02373042 -0.16294369 -0.21088321\n",
      "  -0.53912123  0.11834273  0.07006063 -0.49910932]]\n",
      "now loss =  0.018144422225768703  accuracy =  0.981\n",
      "gradient of first layer [[-0.11034985 -0.25691198  0.06031553  0.30072676 -0.15603461 -0.27775415\n",
      "  -0.29955998 -0.11863044  0.22291645  0.20776031]\n",
      " [ 0.04598981 -0.64682474  0.51713576  0.08447789  0.89619705  0.10881418\n",
      "   0.21006519  0.11701985  0.08507096 -0.04876153]\n",
      " [ 0.02810749  0.19378331  0.39555318 -0.41507643 -0.03455221  0.01278156\n",
      "  -0.13438128 -0.44951733  0.08129927 -0.11352026]\n",
      " [ 0.37958803 -0.53829102  0.24070332  0.38459011 -0.59672973 -0.13527476\n",
      "   0.49683591  0.24591397 -0.15257978  0.10910048]\n",
      " [-0.02741536 -0.15916442 -0.11164444 -0.12451157 -0.3644438   0.19195135\n",
      "   0.24140326 -0.02634016  0.37741956  0.62676987]\n",
      " [ 0.54997597  0.80799247  0.40706912  0.14561087  0.90337966 -0.09708411\n",
      "  -0.09206304 -0.40763406  0.00127148  0.10626844]\n",
      " [-0.19790473  0.3749642   0.13585523  0.17620488 -0.47951506 -0.63872181\n",
      "  -0.14412775  0.34737987  0.28354411 -0.08017217]\n",
      " [-0.36621416 -0.19531085 -0.14693656 -0.31674867 -0.71706802  0.03427874\n",
      "  -0.08054765  0.2366972  -0.05490506 -0.1120244 ]\n",
      " [-0.38448875 -0.31216824 -0.06497459  0.12936746 -0.67693173  0.44532749\n",
      "   0.24728123  0.22996483  0.11620763  0.1087261 ]\n",
      " [-0.09933512 -0.07658499  0.62381784 -0.02370989 -0.16312139 -0.21098522\n",
      "  -0.53917833  0.11845658  0.07006674 -0.49912129]]\n",
      "now loss =  0.03133247114374947  accuracy =  0.98\n",
      "gradient of first layer [[-0.11314092 -0.25637958  0.05945407  0.30139203 -0.16177838 -0.28104822\n",
      "  -0.30140031 -0.11496117  0.22312483  0.20737848]\n",
      " [ 0.04657819 -0.64691184  0.51726494  0.0843301   0.89737521  0.10950519\n",
      "   0.21042982  0.11624295  0.08498487 -0.04871036]\n",
      " [ 0.02822811  0.19377125  0.39557573 -0.41510311 -0.03431027  0.01291344\n",
      "  -0.13431406 -0.44967397  0.08127492 -0.11352028]\n",
      " [ 0.37957157 -0.53828786  0.24069818  0.38459402 -0.59676362 -0.13529417\n",
      "   0.49682504  0.24593561 -0.1525786   0.1090982 ]\n",
      " [-0.02847725 -0.15892007 -0.11201534 -0.12424615 -0.36664164  0.19064817\n",
      "   0.2406549  -0.02493122  0.37744445  0.62655821]\n",
      " [ 0.55182499  0.80764979  0.40761693  0.14516635  0.90717131 -0.09490367\n",
      "  -0.09085397 -0.41006652  0.00111475  0.10650737]\n",
      " [-0.20088322  0.37554395  0.13490506  0.17690765 -0.48566554 -0.64223197\n",
      "  -0.14610115  0.35129141  0.28374419 -0.08059089]\n",
      " [-0.36627411 -0.19529946 -0.14695503 -0.31673438 -0.71719138  0.03420803\n",
      "  -0.08058714  0.23677599 -0.05490053 -0.11203254]\n",
      " [-0.38515888 -0.31201453 -0.0652083   0.12953267 -0.67831925  0.44450806\n",
      "   0.24681072  0.23085395  0.11622354  0.1085947 ]\n",
      " [-0.09941438 -0.07656994  0.62379343 -0.02369099 -0.16328449 -0.2110787\n",
      "  -0.53923053  0.11856075  0.07007275 -0.49913204]]\n",
      "now loss =  0.017706804984092318  accuracy =  0.979\n",
      "gradient of first layer [[-1.15541361e-01 -2.55904351e-01  5.86784468e-02  3.01964734e-01\n",
      "  -1.66738480e-01 -2.83893794e-01 -3.03004707e-01 -1.11808667e-01\n",
      "   2.23276619e-01  2.07024897e-01]\n",
      " [ 4.76258048e-02 -6.46979473e-01  5.17375008e-01  8.40554176e-02\n",
      "   8.99416325e-01  1.10691883e-01  2.10998476e-01  1.14879626e-01\n",
      "   8.46970206e-02 -4.87330019e-02]\n",
      " [ 2.85117716e-02  1.93762009e-01  3.95597274e-01 -4.15174938e-01\n",
      "  -3.37594542e-02  1.32229606e-02 -1.34171006e-01 -4.50039863e-01\n",
      "   8.11852226e-02 -1.13541074e-01]\n",
      " [ 3.79544839e-01 -5.38282653e-01  2.40689682e-01  3.84600411e-01\n",
      "  -5.96818793e-01 -1.35325817e-01  4.96807255e-01  2.45970705e-01\n",
      "  -1.52576770e-01  1.09094381e-01]\n",
      " [-2.79911794e-02 -1.58865373e-01 -1.12076665e-01 -1.24371367e-01\n",
      "  -3.65744123e-01  1.91129944e-01  2.40827715e-01 -2.55454510e-02\n",
      "   3.77185407e-01  6.26425010e-01]\n",
      " [ 5.53815691e-01  8.07307663e-01  4.08172103e-01  1.44680156e-01\n",
      "   9.11237101e-01 -9.25612979e-02 -8.95718729e-02 -4.12676754e-01\n",
      "   9.03792655e-04  1.06734229e-01]\n",
      " [-2.02758620e-01  3.76006549e-01  1.34145326e-01  1.77336207e-01\n",
      "  -4.89625298e-01 -6.44485931e-01 -1.47439203e-01  3.53761154e-01\n",
      "   2.83715745e-01 -8.09811915e-02]\n",
      " [-3.66300794e-01 -1.95293902e-01 -1.46964125e-01 -3.16728099e-01\n",
      "  -7.17246787e-01  3.41763572e-02 -8.06051977e-02  2.36811057e-01\n",
      "  -5.48992961e-02 -1.12036787e-01]\n",
      " [-3.84588770e-01 -3.12037308e-01 -6.51554229e-02  1.29390103e-01\n",
      "  -6.77206064e-01  4.45129185e-01  2.47101631e-01  2.30119262e-01\n",
      "   1.16050814e-01  1.08557149e-01]\n",
      " [-9.94812029e-02 -7.65568643e-02  6.23772026e-01 -2.36750589e-02\n",
      "  -1.63422487e-01 -2.11157761e-01 -5.39275015e-01  1.18648476e-01\n",
      "   7.00771857e-02 -4.99141651e-01]]\n",
      "now loss =  0.029003295080994446  accuracy =  0.982\n",
      "gradient of first layer [[-1.18217071e-01 -2.55412323e-01  5.78589259e-02  3.02611490e-01\n",
      "  -1.72243746e-01 -2.87053805e-01 -3.04764998e-01 -1.08305180e-01\n",
      "   2.23498833e-01  2.06670915e-01]\n",
      " [ 4.73799389e-02 -6.46902374e-01  5.17258531e-01  8.41153805e-02\n",
      "   8.98885441e-01  1.10384045e-01  2.10808608e-01  1.15205047e-01\n",
      "   8.46786985e-02 -4.87982305e-02]\n",
      " [ 2.89791903e-02  1.93691516e-01  3.95723073e-01 -4.15285599e-01\n",
      "  -3.28064106e-02  1.37624720e-02 -1.33878574e-01 -4.50648715e-01\n",
      "   8.11277662e-02 -1.13497880e-01]\n",
      " [ 3.79526874e-01 -5.38279365e-01  2.40684166e-01  3.84604767e-01\n",
      "  -5.96855760e-01 -1.35347033e-01  4.96795434e-01  2.45994213e-01\n",
      "  -1.52575273e-01  1.09092001e-01]\n",
      " [-2.70010578e-02 -1.58994314e-01 -1.11826500e-01 -1.24600472e-01\n",
      "  -3.63730249e-01  1.92250016e-01  2.41425867e-01 -2.68277951e-02\n",
      "   3.77040576e-01  6.26489101e-01]\n",
      " [ 5.55445509e-01  8.07013853e-01  4.08659390e-01  1.44283999e-01\n",
      "   9.14582584e-01 -9.06361601e-02 -8.85043162e-02 -4.14811539e-01\n",
      "   7.58930101e-04  1.06943922e-01]\n",
      " [-2.06060966e-01  3.76627043e-01  1.33110526e-01  1.78132559e-01\n",
      "  -4.96435356e-01 -6.48388259e-01 -1.49622795e-01  3.58084194e-01\n",
      "   2.83971372e-01 -8.14305702e-02]\n",
      " [-3.66367554e-01 -1.95281698e-01 -1.46984518e-01 -3.16711961e-01\n",
      "  -7.17384110e-01  3.40975566e-02 -8.06490644e-02  2.36898449e-01\n",
      "  -5.48936832e-02 -1.12045564e-01]\n",
      " [-3.84078551e-01 -3.12100698e-01 -6.50279409e-02  1.29273012e-01\n",
      "  -6.76168449e-01  4.45702216e-01  2.47406616e-01  2.29459859e-01\n",
      "   1.15973347e-01  1.08586298e-01]\n",
      " [-9.95670725e-02 -7.65411729e-02  6.23745804e-01 -2.36542980e-02\n",
      "  -1.63599113e-01 -2.11259116e-01 -5.39331434e-01  1.18760880e-01\n",
      "   7.00844148e-02 -4.99152933e-01]]\n",
      "now loss =  0.033117121320025866  accuracy =  0.984\n",
      "gradient of first layer [[-1.20605155e-01 -2.54978861e-01  5.71237671e-02  3.03191742e-01\n",
      "  -1.77158182e-01 -2.89876272e-01 -3.06337619e-01 -1.05183526e-01\n",
      "   2.23699255e-01  2.06352650e-01]\n",
      " [ 4.89600555e-02 -6.47149952e-01  5.17690733e-01  8.37297684e-02\n",
      "   9.02110291e-01  1.12227886e-01  2.11811252e-01  1.13146741e-01\n",
      "   8.44889359e-02 -4.86383450e-02]\n",
      " [ 2.94186929e-02  1.93629342e-01  3.95836631e-01 -4.15391955e-01\n",
      "  -3.19115393e-02  1.42687766e-02 -1.33606686e-01 -4.51218914e-01\n",
      "   8.10660415e-02 -1.13463152e-01]\n",
      " [ 3.79506496e-01 -5.38275666e-01  2.40677882e-01  3.84609707e-01\n",
      "  -5.96897698e-01 -1.35371102e-01  4.96782018e-01  2.46020854e-01\n",
      "  -1.52573579e-01  1.09089280e-01]\n",
      " [-2.66054788e-02 -1.59003519e-01 -1.11776200e-01 -1.24694182e-01\n",
      "  -3.62944462e-01  1.92666555e-01  2.41623498e-01 -2.73263954e-02\n",
      "   3.76919752e-01  6.26454649e-01]\n",
      " [ 5.57143397e-01  8.06719470e-01  4.09162693e-01  1.43870834e-01\n",
      "   9.18068151e-01 -8.86391299e-02 -8.74004483e-02 -4.17027934e-01\n",
      "   5.95026746e-04  1.07150174e-01]\n",
      " [-2.07827396e-01  3.76968154e-01  1.32535655e-01  1.78560064e-01\n",
      "  -5.00086889e-01 -6.50486437e-01 -1.50805717e-01  3.60395860e-01\n",
      "   2.84088819e-01 -8.16918906e-02]\n",
      " [-3.66430186e-01 -1.95270377e-01 -1.47003743e-01 -3.16696766e-01\n",
      "  -7.17512960e-01  3.40235955e-02 -8.06902524e-02  2.36980325e-01\n",
      "  -5.48883917e-02 -1.12053859e-01]\n",
      " [-3.83636392e-01 -3.12145030e-01 -6.49320559e-02  1.29166897e-01\n",
      "  -6.75274159e-01  4.46194841e-01  2.47661625e-01  2.28892848e-01\n",
      "   1.15886212e-01  1.08595024e-01]\n",
      " [-9.96494347e-02 -7.65263326e-02  6.23720578e-01 -2.36343117e-02\n",
      "  -1.63768533e-01 -2.11356343e-01 -5.39385551e-01  1.18868531e-01\n",
      "   7.00914459e-02 -4.99163772e-01]]\n",
      "now loss =  0.02254264583609914  accuracy =  0.985\n",
      "gradient of first layer [[-1.23501629e-01 -2.54462976e-01  5.62263733e-02  3.03894472e-01\n",
      "  -1.83129038e-01 -2.93288783e-01 -3.08239099e-01 -1.01410053e-01\n",
      "   2.23953109e-01  2.05979915e-01]\n",
      " [ 4.99055553e-02 -6.47288077e-01  5.17911665e-01  8.34840955e-02\n",
      "   9.04015234e-01  1.13350517e-01  2.12407129e-01  1.11910409e-01\n",
      "   8.43464320e-02 -4.85519663e-02]\n",
      " [ 3.03676677e-02  1.93474683e-01  3.96112431e-01 -4.15624478e-01\n",
      "  -2.99650787e-02  1.53805765e-02 -1.32995498e-01 -4.52452060e-01\n",
      "   8.09629965e-02 -1.13357129e-01]\n",
      " [ 3.79480021e-01 -5.38270949e-01  2.40669671e-01  3.84616126e-01\n",
      "  -5.96952264e-01 -1.35402290e-01  4.96764634e-01  2.46055353e-01\n",
      "  -1.52571283e-01  1.09085853e-01]\n",
      " [-2.50149195e-02 -1.59240267e-01 -1.11327717e-01 -1.25082833e-01\n",
      "  -3.59687688e-01  1.94510363e-01  2.42629440e-01 -2.93838060e-02\n",
      "   3.76724458e-01  6.26608359e-01]\n",
      " [ 5.58593988e-01  8.06470811e-01  4.09582408e-01  1.43509829e-01\n",
      "   9.21039988e-01 -8.69227009e-02 -8.64557631e-02 -4.18920904e-01\n",
      "   4.44466856e-04  1.07324637e-01]\n",
      " [-2.10161900e-01  3.77401392e-01  1.31758056e-01  1.79112074e-01\n",
      "  -5.04933170e-01 -6.53223116e-01 -1.52352408e-01  3.63430812e-01\n",
      "   2.84251414e-01 -8.20137607e-02]\n",
      " [-3.66497504e-01 -1.95258412e-01 -1.47024550e-01 -3.16680443e-01\n",
      "  -7.17651693e-01  3.39443052e-02 -8.07344142e-02  2.37068036e-01\n",
      "  -5.48824656e-02 -1.12062495e-01]\n",
      " [-3.83201003e-01 -3.12197591e-01 -6.48229360e-02  1.29059086e-01\n",
      "  -6.74390455e-01  4.46692563e-01  2.47926871e-01  2.28332479e-01\n",
      "   1.15817749e-01  1.08624184e-01]\n",
      " [-9.97452498e-02 -7.65093400e-02  6.23691020e-01 -2.36110534e-02\n",
      "  -1.63965957e-01 -2.11469202e-01 -5.39448386e-01  1.18993367e-01\n",
      "   7.00999383e-02 -4.99176031e-01]]\n",
      "now loss =  0.026982695195417027  accuracy =  0.982\n",
      "gradient of first layer [[-1.25761641e-01 -2.54058380e-01  5.55112720e-02  3.04446615e-01\n",
      "  -1.87794448e-01 -2.95955804e-01 -3.09731206e-01 -9.84696299e-02\n",
      "   2.24141592e-01  2.05678331e-01]\n",
      " [ 4.94468167e-02 -6.47143408e-01  5.17656418e-01  8.35819696e-02\n",
      "   9.03007421e-01  1.12789693e-01  2.12045752e-01  1.12509797e-01\n",
      "   8.42816023e-02 -4.86931643e-02]\n",
      " [ 2.94468531e-02  1.93657693e-01  3.95804076e-01 -4.15395645e-01\n",
      "  -3.18709412e-02  1.42748766e-02 -1.33622758e-01 -4.51247686e-01\n",
      "   8.10165470e-02 -1.13506442e-01]\n",
      " [ 3.79443510e-01 -5.38264438e-01  2.40658209e-01  3.84625056e-01\n",
      "  -5.97027610e-01 -1.35445358e-01  4.96740581e-01  2.46102858e-01\n",
      "  -1.52568140e-01  1.09081072e-01]\n",
      " [-2.93003540e-02 -1.58417411e-01 -1.12717941e-01 -1.24015796e-01\n",
      "  -3.68532910e-01  1.89379701e-01  2.39738389e-01 -2.37808886e-02\n",
      "   3.77016253e-01  6.25949789e-01]\n",
      " [ 5.59811181e-01  8.06272134e-01  4.09918473e-01  1.43201551e-01\n",
      "   9.23524066e-01 -8.54815386e-02 -8.56695223e-02 -4.20508124e-01\n",
      "   3.01982563e-04  1.07460938e-01]\n",
      " [-2.12203411e-01  3.77809816e-01  1.31025563e-01  1.79595902e-01\n",
      "  -5.09197479e-01 -6.55636907e-01 -1.53739438e-01  3.66085073e-01\n",
      "   2.84345103e-01 -8.23404171e-02]\n",
      " [-3.66552775e-01 -1.95248597e-01 -1.47042032e-01 -3.16666974e-01\n",
      "  -7.17765856e-01  3.38792210e-02 -8.07708054e-02  2.37139872e-01\n",
      "  -5.48777387e-02 -1.12069697e-01]\n",
      " [-3.84794056e-01 -3.11874981e-01 -6.53591602e-02  1.29455733e-01\n",
      "  -6.77686138e-01  4.44772805e-01  2.46835784e-01  2.30420208e-01\n",
      "   1.15902172e-01  1.08356013e-01]\n",
      " [-9.98468086e-02 -7.64914646e-02  6.23659329e-01 -2.35862122e-02\n",
      "  -1.64175430e-01 -2.11588857e-01 -5.39515121e-01  1.19125428e-01\n",
      "   7.01089220e-02 -4.99189112e-01]]\n",
      "now loss =  0.03377352092652604  accuracy =  0.986\n",
      "gradient of first layer [[-1.28054327e-01 -2.53655449e-01  5.47966288e-02  3.05009980e-01\n",
      "  -1.92527434e-01 -2.98661165e-01 -3.11239351e-01 -9.54923420e-02\n",
      "   2.24351547e-01  2.05388054e-01]\n",
      " [ 5.24802850e-02 -6.47617520e-01  5.18497461e-01  8.28133430e-02\n",
      "   9.09196811e-01  1.16369430e-01  2.13998130e-01  1.08566117e-01\n",
      "   8.39192380e-02 -4.83590555e-02]\n",
      " [ 3.07015936e-02  1.93453068e-01  3.96168900e-01 -4.15708692e-01\n",
      "  -2.92986510e-02  1.57535823e-02 -1.32809416e-01 -4.52877631e-01\n",
      "   8.08798141e-02 -1.13361588e-01]\n",
      " [ 3.79413936e-01 -5.38259197e-01  2.40648924e-01  3.84632308e-01\n",
      "  -5.97088701e-01 -1.35480258e-01  4.96721097e-01  2.46141267e-01\n",
      "  -1.52565496e-01  1.09077285e-01]\n",
      " [-2.71424699e-02 -1.58756914e-01 -1.12106980e-01 -1.24554552e-01\n",
      "  -3.64119015e-01  1.91916207e-01  2.41126215e-01 -2.65828279e-02\n",
      "   3.76765513e-01  6.26185711e-01]\n",
      " [ 5.61895665e-01  8.05926431e-01  4.10527090e-01  1.42677086e-01\n",
      "   9.27799236e-01 -8.30163416e-02 -8.43120775e-02 -4.23217128e-01\n",
      "   7.73512808e-05  1.07706984e-01]\n",
      " [-2.12352777e-01  3.77878579e-01  1.30900230e-01  1.79613323e-01\n",
      "  -5.09560620e-01 -6.55809298e-01 -1.53867954e-01  3.66275107e-01\n",
      "   2.84295299e-01 -8.23952408e-02]\n",
      " [-3.66599994e-01 -1.95240206e-01 -1.47057011e-01 -3.16655478e-01\n",
      "  -7.17863490e-01  3.38236127e-02 -8.08019089e-02  2.37201175e-01\n",
      "  -5.48736399e-02 -1.12075775e-01]\n",
      " [-3.84081125e-01 -3.11978599e-01 -6.51680531e-02  1.29277162e-01\n",
      "  -6.76234801e-01  4.45606207e-01  2.47286962e-01  2.29495317e-01\n",
      "   1.15809323e-01  1.08425675e-01]\n",
      " [-9.99254717e-02 -7.64776637e-02  6.23634804e-01 -2.35669109e-02\n",
      "  -1.64337810e-01 -2.11681617e-01 -5.39566831e-01  1.19227579e-01\n",
      "   7.01161130e-02 -4.99199059e-01]]\n",
      "now loss =  0.031217501735123464  accuracy =  0.986\n",
      "gradient of first layer [[-1.31088709e-01 -2.53140952e-01  5.38514517e-02  3.05758472e-01\n",
      "  -1.98791527e-01 -3.02229519e-01 -3.13226285e-01 -9.15634147e-02\n",
      "   2.24642319e-01  2.05015555e-01]\n",
      " [ 4.98444050e-02 -6.47160264e-01  5.17615166e-01  8.34445816e-02\n",
      "   9.03712185e-01  1.13299829e-01  2.12265507e-01  1.11964354e-01\n",
      "   8.41339338e-02 -4.86918559e-02]\n",
      " [ 3.04131161e-02  1.93510407e-01  3.96068117e-01 -4.15636504e-01\n",
      "  -2.98975094e-02  1.54074509e-02 -1.33007437e-01 -4.52501438e-01\n",
      "   8.08939039e-02 -1.13410611e-01]\n",
      " [ 3.79339465e-01 -5.38246616e-01  2.40625945e-01  3.84650768e-01\n",
      "  -5.97242313e-01 -1.35567923e-01  4.96672359e-01  2.46237713e-01\n",
      "  -1.52558183e-01  1.09068235e-01]\n",
      " [-2.82943234e-02 -1.58529699e-01 -1.12483916e-01 -1.24257497e-01\n",
      "  -3.66489927e-01  1.90516220e-01  2.40334770e-01 -2.50722399e-02\n",
      "   3.76833986e-01  6.25987883e-01]\n",
      " [ 5.62230662e-01  8.05872992e-01  4.10600799e-01  1.42582198e-01\n",
      "   9.28470602e-01 -8.26037637e-02 -8.40934211e-02 -4.23657926e-01\n",
      "   2.52833989e-05  1.07743268e-01]\n",
      " [-2.17324716e-01  3.78725257e-01  1.29300398e-01  1.80820508e-01\n",
      "  -5.19862059e-01 -6.61622218e-01 -1.57123458e-01  3.72697771e-01\n",
      "   2.84743229e-01 -8.30068019e-02]\n",
      " [-3.66691884e-01 -1.95224659e-01 -1.47085567e-01 -3.16632791e-01\n",
      "  -7.18053196e-01  3.37155893e-02 -8.08620308e-02  2.37320118e-01\n",
      "  -5.48647165e-02 -1.12086928e-01]\n",
      " [-3.84606616e-01 -3.11872345e-01 -6.53430204e-02  1.29412694e-01\n",
      "  -6.77317478e-01  4.44965137e-01  2.46923077e-01  2.30185272e-01\n",
      "   1.15836699e-01  1.08331438e-01]\n",
      " [-1.00074046e-01 -7.64525882e-02  6.23588900e-01 -2.35301305e-02\n",
      "  -1.64644335e-01 -2.11856428e-01 -5.39664029e-01  1.19419960e-01\n",
      "   7.01306946e-02 -4.99217077e-01]]\n",
      "now loss =  0.03467842209318907  accuracy =  0.988\n",
      "gradient of first layer [[-1.33822929e-01 -2.52682937e-01  5.30080644e-02  3.06442125e-01\n",
      "  -2.04430981e-01 -3.05455317e-01 -3.15019138e-01 -8.80253367e-02\n",
      "   2.24915073e-01  2.04681677e-01]\n",
      " [ 5.22050427e-02 -6.47536535e-01  5.18288965e-01  8.28420943e-02\n",
      "   9.08544982e-01  1.16094286e-01  2.13797287e-01  1.08902615e-01\n",
      "   8.38590496e-02 -4.84243040e-02]\n",
      " [ 3.11845558e-02  1.93391687e-01  3.96294215e-01 -4.15828469e-01\n",
      "  -2.83114139e-02  1.63083468e-02 -1.32512360e-01 -4.53496840e-01\n",
      "   8.08029766e-02 -1.13330746e-01]\n",
      " [ 3.79296129e-01 -5.38239392e-01  2.40612457e-01  3.84661553e-01\n",
      "  -5.97331758e-01 -1.35618916e-01  4.96643976e-01  2.46293745e-01\n",
      "  -1.52553930e-01  1.09062947e-01]\n",
      " [-2.87623187e-02 -1.58414352e-01 -1.12647780e-01 -1.24128140e-01\n",
      "  -3.67454470e-01  1.89913874e-01  2.39987676e-01 -2.44484030e-02\n",
      "   3.76840766e-01  6.25876416e-01]\n",
      " [ 5.63057743e-01  8.05736309e-01  4.10834254e-01  1.42368152e-01\n",
      "   9.30162487e-01 -8.16163944e-02 -8.35524800e-02 -4.24733451e-01\n",
      "  -7.08675423e-05  1.07840168e-01]\n",
      " [-2.18254975e-01  3.78889327e-01  1.28970525e-01  1.81039880e-01\n",
      "  -5.21811197e-01 -6.62700924e-01 -1.57738891e-01  3.73891842e-01\n",
      "   2.84809569e-01 -8.31279563e-02]\n",
      " [-3.66780462e-01 -1.95209928e-01 -1.47112918e-01 -3.16610683e-01\n",
      "  -7.18235896e-01  3.36112553e-02 -8.09200183e-02  2.37434688e-01\n",
      "  -5.48558642e-02 -1.12097673e-01]\n",
      " [-3.85059026e-01 -3.11774535e-01 -6.54918978e-02  1.29533525e-01\n",
      "  -6.78249794e-01  4.44400665e-01  2.46602924e-01  2.30781586e-01\n",
      "   1.15860490e-01  1.08245742e-01]\n",
      " [-1.00193324e-01 -7.64327637e-02  6.23552094e-01 -2.35003551e-02\n",
      "  -1.64890339e-01 -2.11996918e-01 -5.39742102e-01  1.19574242e-01\n",
      "   7.01426325e-02 -4.99231533e-01]]\n",
      "now loss =  0.02631604851225002  accuracy =  0.986\n",
      "gradient of first layer [[-1.36162368e-01 -2.52291422e-01  5.22784040e-02  3.07030928e-01\n",
      "  -2.09262726e-01 -3.08216681e-01 -3.16556938e-01 -8.50029743e-02\n",
      "   2.25147821e-01  2.04395422e-01]\n",
      " [ 5.20954454e-02 -6.47475306e-01  5.18174527e-01  8.28585534e-02\n",
      "   9.08268856e-01  1.15959193e-01  2.13687899e-01  1.09041642e-01\n",
      "   8.38017427e-02 -4.84846037e-02]\n",
      " [ 3.09469470e-02  1.93443765e-01  3.96202150e-01 -4.15767662e-01\n",
      "  -2.88097601e-02  1.60178604e-02 -1.32682395e-01 -4.53187698e-01\n",
      "   8.08074071e-02 -1.13378736e-01]\n",
      " [ 3.79251520e-01 -5.38231964e-01  2.40598582e-01  3.84672763e-01\n",
      "  -5.97423872e-01 -1.35671518e-01  4.96614699e-01  2.46351370e-01\n",
      "  -1.52549455e-01  1.09057539e-01]\n",
      " [-3.12841306e-02 -1.57960884e-01 -1.13471701e-01 -1.23484630e-01\n",
      "  -3.72671766e-01  1.86898453e-01  2.38289945e-01 -2.11803328e-02\n",
      "   3.77043683e-01  6.25511459e-01]\n",
      " [ 5.64345003e-01  8.05531246e-01  4.11204612e-01  1.42036037e-01\n",
      "   9.32799580e-01 -8.00888206e-02 -8.27139846e-02 -4.26401553e-01\n",
      "  -2.20629261e-04  1.07988017e-01]\n",
      " [-2.20061838e-01  3.79222307e-01  1.28343440e-01  1.81483306e-01\n",
      "  -5.25584558e-01 -6.64830963e-01 -1.58951587e-01  3.76221629e-01\n",
      "   2.84938574e-01 -8.33797147e-02]\n",
      " [-3.66844483e-01 -1.95199264e-01 -1.47132963e-01 -3.16594637e-01\n",
      "  -7.18368190e-01  3.35358315e-02 -8.09620401e-02  2.37517344e-01\n",
      "  -5.48495043e-02 -1.12105443e-01]\n",
      " [-3.86334954e-01 -3.11542903e-01 -6.59115336e-02  1.29858508e-01\n",
      "  -6.80891836e-01  4.42874468e-01  2.45742483e-01  2.32435069e-01\n",
      "   1.15960992e-01  1.08059845e-01]\n",
      " [-1.00291537e-01 -7.64164478e-02  6.23521554e-01 -2.34756705e-02\n",
      "  -1.65093133e-01 -2.12112715e-01 -5.39806541e-01  1.19701092e-01\n",
      "   7.01525152e-02 -4.99243419e-01]]\n",
      "now loss =  0.01911245284625025  accuracy =  0.99\n",
      "gradient of first layer [[-1.38875953e-01 -2.51820174e-01  5.13892575e-02  3.07711738e-01\n",
      "  -2.14892524e-01 -3.11426116e-01 -3.18362480e-01 -8.15027731e-02\n",
      "   2.25385305e-01  2.04038597e-01]\n",
      " [ 5.26905112e-02 -6.47433779e-01  5.18078658e-01  8.26644265e-02\n",
      "   9.09338851e-01  1.16641005e-01  2.13946789e-01  1.08267152e-01\n",
      "   8.34903786e-02 -4.85943332e-02]\n",
      " [ 3.20103609e-02  1.93311844e-01  3.96447328e-01 -4.16048768e-01\n",
      "  -2.66599246e-02  1.72637598e-02 -1.32026143e-01 -4.54560326e-01\n",
      "   8.06199248e-02 -1.13310043e-01]\n",
      " [ 3.79214869e-01 -5.38225527e-01  2.40586331e-01  3.84681931e-01\n",
      "  -5.97500059e-01 -1.35714831e-01  4.96590231e-01  2.46398607e-01\n",
      "  -1.52546419e-01  1.09052629e-01]\n",
      " [-2.86948384e-02 -1.58250176e-01 -1.12931517e-01 -1.24174090e-01\n",
      "  -3.67465627e-01  1.89917971e-01  2.39854720e-01 -2.45208082e-02\n",
      "   3.76532024e-01  6.25632868e-01]\n",
      " [ 5.64510411e-01  8.05536584e-01  4.11180839e-01  1.41981022e-01\n",
      "   9.33099076e-01 -7.98926583e-02 -8.26366871e-02 -4.26618807e-01\n",
      "  -3.02955820e-04  1.07962856e-01]\n",
      " [-2.21941544e-01  3.79652770e-01  1.27512441e-01  1.81919363e-01\n",
      "  -5.29607527e-01 -6.67063847e-01 -1.60299449e-01  3.78639145e-01\n",
      "   2.84913794e-01 -8.37606903e-02]\n",
      " [-3.66854931e-01 -1.95196816e-01 -1.47137860e-01 -3.16592245e-01\n",
      "  -7.18390750e-01  3.35235001e-02 -8.09696007e-02  2.37530710e-01\n",
      "  -5.48497935e-02 -1.12107594e-01]\n",
      " [-3.84784174e-01 -3.11720393e-01 -6.55759562e-02  1.29447275e-01\n",
      "  -6.77767663e-01  4.44682105e-01  2.46684384e-01  2.30435229e-01\n",
      "   1.15665082e-01  1.08139842e-01]\n",
      " [-1.00363922e-01 -7.64037548e-02  6.23497358e-01 -2.34575747e-02\n",
      "  -1.65243613e-01 -2.12198221e-01 -5.39854843e-01  1.19794370e-01\n",
      "   7.01585279e-02 -4.99253082e-01]]\n",
      "now loss =  0.01934428381183616  accuracy =  0.986\n",
      "gradient of first layer [[-1.40469738e-01 -2.51541122e-01  5.08515194e-02  3.08109105e-01\n",
      "  -2.18207784e-01 -3.13310828e-01 -3.19429123e-01 -7.94501976e-02\n",
      "   2.25513934e-01  2.03821697e-01]\n",
      " [ 5.32045372e-02 -6.47432052e-01  5.18036180e-01  8.24941923e-02\n",
      "   9.10289362e-01  1.17252626e-01  2.14202535e-01  1.07595018e-01\n",
      "   8.32591734e-02 -4.86568336e-02]\n",
      " [ 3.22118031e-02  1.93306031e-01  3.96456368e-01 -4.16107872e-01\n",
      "  -2.62696028e-02  1.74943426e-02 -1.31921064e-01 -4.54818492e-01\n",
      "   8.05463132e-02 -1.13328164e-01]\n",
      " [ 3.79164351e-01 -5.38216972e-01  2.40570093e-01  3.84694719e-01\n",
      "  -5.97604685e-01 -1.35774679e-01  4.96556696e-01  2.46463731e-01\n",
      "  -1.52541660e-01  1.09046179e-01]\n",
      " [-2.87621762e-02 -1.58147530e-01 -1.13126174e-01 -1.24181402e-01\n",
      "  -3.67684786e-01  1.89804276e-01  2.39715341e-01 -2.44254198e-02\n",
      "   3.76362979e-01  6.25478689e-01]\n",
      " [ 5.65627221e-01  8.05379981e-01  4.11459155e-01  1.41681255e-01\n",
      "   9.35368240e-01 -7.85665366e-02 -8.19272782e-02 -4.28062826e-01\n",
      "  -4.79083375e-04  1.08056275e-01]\n",
      " [-2.22380238e-01  3.79803986e-01  1.27182183e-01  1.81991103e-01\n",
      "  -5.30622919e-01 -6.67572046e-01 -1.60662932e-01  3.79192815e-01\n",
      "   2.84792823e-01 -8.39249828e-02]\n",
      " [-3.66890589e-01 -1.95190416e-01 -1.47150149e-01 -3.16583399e-01\n",
      "  -7.18465047e-01  3.34812914e-02 -8.09936023e-02  2.37576649e-01\n",
      "  -5.48471825e-02 -1.12112660e-01]\n",
      " [-3.84224944e-01 -3.11757259e-01 -6.55083480e-02  1.29288536e-01\n",
      "  -6.76663190e-01  4.45325227e-01  2.46996095e-01  2.29718312e-01\n",
      "   1.15501261e-01  1.08120928e-01]\n",
      " [-1.00461695e-01 -7.63872087e-02  6.23465836e-01 -2.34328718e-02\n",
      "  -1.65446155e-01 -2.12313969e-01 -5.39919741e-01  1.19920386e-01\n",
      "   7.01676664e-02 -4.99265582e-01]]\n",
      "now loss =  0.02511793971344305  accuracy =  0.987\n",
      "gradient of first layer [[-1.43198404e-01 -2.51099648e-01  5.00005347e-02  3.08806256e-01\n",
      "  -2.23849434e-01 -3.16535321e-01 -3.21224412e-01 -7.59394443e-02\n",
      "   2.25804411e-01  2.03502620e-01]\n",
      " [ 5.44053557e-02 -6.47589993e-01  5.18309119e-01  8.21608889e-02\n",
      "   9.12706362e-01  1.18692612e-01  2.14963012e-01  1.06037232e-01\n",
      "   8.30539091e-02 -4.85564825e-02]\n",
      " [ 3.32540217e-02  1.93149907e-01  3.96754447e-01 -4.16380762e-01\n",
      "  -2.41299654e-02  1.87268559e-02 -1.31246234e-01 -4.56160040e-01\n",
      "   8.04109589e-02 -1.13222584e-01]\n",
      " [ 3.79122642e-01 -5.38210220e-01  2.40556974e-01  3.84705336e-01\n",
      "  -5.97690984e-01 -1.35823905e-01  4.96529247e-01  2.46517368e-01\n",
      "  -1.52537296e-01  1.09041274e-01]\n",
      " [-2.82406970e-02 -1.58198823e-01 -1.13017570e-01 -1.24323801e-01\n",
      "  -3.66633357e-01  1.90407894e-01  2.40027422e-01 -2.50917702e-02\n",
      "   3.76251501e-01  6.25494384e-01]\n",
      " [ 5.66418842e-01  8.05262010e-01  4.11672304e-01  1.41468873e-01\n",
      "   9.36983118e-01 -7.76212244e-02 -8.14144425e-02 -4.29086528e-01\n",
      "  -5.88413649e-04  1.08137054e-01]\n",
      " [-2.24057362e-01  3.80102241e-01  1.26578583e-01  1.82397922e-01\n",
      "  -5.34142872e-01 -6.69534358e-01 -1.61788050e-01  3.81339392e-01\n",
      "   2.84911038e-01 -8.41507690e-02]\n",
      " [-3.66950494e-01 -1.95180703e-01 -1.47169003e-01 -3.16568164e-01\n",
      "  -7.18589014e-01  3.34105911e-02 -8.10330319e-02  2.37653680e-01\n",
      "  -5.48409211e-02 -1.12119702e-01]\n",
      " [-3.84662884e-01 -3.11669217e-01 -6.56707361e-02  1.29397100e-01\n",
      "  -6.77582012e-01  4.44799438e-01  2.46691811e-01  2.30284234e-01\n",
      "   1.15521368e-01  1.08047604e-01]\n",
      " [-1.00559828e-01 -7.63713504e-02  6.23435137e-01 -2.34078505e-02\n",
      "  -1.65649102e-01 -2.12429852e-01 -5.39984295e-01  1.20046612e-01\n",
      "   7.01780527e-02 -4.99277069e-01]]\n",
      "now loss =  0.022102250035285174  accuracy =  0.986\n",
      "gradient of first layer [[-1.45584019e-01 -2.50690815e-01  4.91939767e-02  3.09409706e-01\n",
      "  -2.28817037e-01 -3.19355695e-01 -3.22821018e-01 -7.28761152e-02\n",
      "   2.26008454e-01  2.03188773e-01]\n",
      " [ 5.42802121e-02 -6.47414891e-01  5.17935984e-01  8.21285667e-02\n",
      "   9.12242794e-01  1.18557417e-01  2.14749247e-01  1.06177461e-01\n",
      "   8.27895687e-02 -4.87509622e-02]\n",
      " [ 3.34232363e-02  1.93164803e-01  3.96726718e-01 -4.16436354e-01\n",
      "  -2.38260481e-02  1.89213544e-02 -1.31172153e-01 -4.56378704e-01\n",
      "   8.03207284e-02 -1.13255054e-01]\n",
      " [ 3.79087501e-01 -5.38204136e-01  2.40544782e-01  3.84714120e-01\n",
      "  -5.97764395e-01 -1.35865246e-01  4.96505739e-01  2.46562408e-01\n",
      "  -1.52534458e-01  1.09036638e-01]\n",
      " [-2.87598816e-02 -1.57990372e-01 -1.13413212e-01 -1.24219655e-01\n",
      "  -3.67832752e-01  1.89764111e-01  2.39567912e-01 -2.44225725e-02\n",
      "   3.76090584e-01  6.25268434e-01]\n",
      " [ 5.66290277e-01  8.05316337e-01  4.11545899e-01  1.41483372e-01\n",
      "   9.36662354e-01 -7.77617694e-02 -8.15277906e-02 -4.28930896e-01\n",
      "  -6.41132426e-04  1.08083888e-01]\n",
      " [-2.25148801e-01  3.80415257e-01  1.25925728e-01  1.82616616e-01\n",
      "  -5.36591925e-01 -6.70805015e-01 -1.62624483e-01  3.82719278e-01\n",
      "   2.84773263e-01 -8.44388043e-02]\n",
      " [-3.66963908e-01 -1.95178017e-01 -1.47174495e-01 -3.16564987e-01\n",
      "  -7.18617588e-01  3.33949359e-02 -8.10422648e-02  2.37670790e-01\n",
      "  -5.48404772e-02 -1.12121834e-01]\n",
      " [-3.84498641e-01 -3.11621572e-01 -6.57588889e-02  1.29335535e-01\n",
      "  -6.77318827e-01  4.44977369e-01  2.46730650e-01  2.30073229e-01\n",
      "   1.15375308e-01  1.07970083e-01]\n",
      " [-1.00640567e-01 -7.63576659e-02  6.23407874e-01 -2.33874903e-02\n",
      "  -1.65817324e-01 -2.12524972e-01 -5.40038082e-01  1.20150162e-01\n",
      "   7.01851845e-02 -4.99287352e-01]]\n",
      "now loss =  0.042261926481625536  accuracy =  0.979\n",
      "gradient of first layer [[-0.14761828 -0.25030477  0.04840119  0.30990473 -0.2331114  -0.32175449\n",
      "  -0.32422234 -0.07027311  0.22609584  0.20286361]\n",
      " [ 0.05329521 -0.64698566  0.51697705  0.08224838  0.90983674  0.11742114\n",
      "   0.21384688  0.1074034   0.08233974 -0.04923996]\n",
      " [ 0.03387213  0.19316618  0.39670222 -0.41658594 -0.02298987  0.01945518\n",
      "  -0.13094316 -0.45696289  0.08012784 -0.11330343]\n",
      " [ 0.37903377 -0.53819419  0.24052466  0.38472729 -0.59787749 -0.13592853\n",
      "   0.49646914  0.24663116 -0.15253135  0.10902866]\n",
      " [-0.02860789 -0.15778645 -0.11387557 -0.12436007 -0.36780112  0.18994587\n",
      "   0.23945719 -0.02463836  0.37562484  0.62496932]\n",
      " [ 0.56764844  0.80517998  0.41177062  0.1410865   0.93935536 -0.07613655\n",
      "  -0.08070378 -0.43069113 -0.00095567  0.10813258]\n",
      " [-0.22509012  0.38067109  0.12530453  0.18246466 -0.53683474 -0.67069784\n",
      "  -0.16282847  0.38260262  0.2842223  -0.08479512]\n",
      " [-0.36690809 -0.19518503 -0.14716155 -0.31658058 -0.71850487  0.03346158\n",
      "  -0.08100695  0.23759868 -0.05485023 -0.11211766]\n",
      " [-0.38455819 -0.31148009 -0.0660736   0.12929237 -0.67760589  0.44490768\n",
      "   0.24656835  0.23013788  0.11512069  0.10777953]\n",
      " [-0.10069825 -0.0763461   0.62338395 -0.02337388 -0.16594008 -0.21259265\n",
      "  -0.54007817  0.12022378  0.07018659 -0.49929715]]\n",
      "now loss =  0.02351146317553842  accuracy =  0.985\n",
      "gradient of first layer [[-0.1486585  -0.25010307  0.04796267  0.31014757 -0.23532856 -0.32296612\n",
      "  -0.32494371 -0.0689506   0.22611912  0.20269042]\n",
      " [ 0.05446204 -0.64700071  0.51685153  0.08181974  0.91195223  0.11889379\n",
      "   0.21447031  0.10584612  0.08182696 -0.04932725]\n",
      " [ 0.03446378  0.19312335  0.39677415 -0.41676255 -0.02183045  0.02015784\n",
      "  -0.13059847 -0.45772847  0.07996513 -0.11330328]\n",
      " [ 0.37894026 -0.53817909  0.2404931   0.38475105 -0.59807215 -0.1360385\n",
      "   0.49640689  0.2467508  -0.15252283  0.10901709]\n",
      " [-0.0280219  -0.15771169 -0.11402566 -0.12456765 -0.36676686  0.19061019\n",
      "   0.23968577 -0.02539399  0.3752536   0.62480358]\n",
      " [ 0.56845675  0.80510463  0.41186723  0.1408326   0.94093073 -0.07514382\n",
      "  -0.08021204 -0.4317495  -0.00116484  0.10815841]\n",
      " [-0.22412663  0.38067772  0.12512193  0.18208462 -0.53513598 -0.66945414\n",
      "  -0.1623287   0.38130332  0.28374003 -0.08489598]\n",
      " [-0.36691832 -0.19518283 -0.14716688 -0.31657854 -0.71852729  0.03345003\n",
      "  -0.08101423  0.23761151 -0.05485076 -0.11211973]\n",
      " [-0.38371965 -0.31151207 -0.06600896  0.12904002 -0.67597826  0.44588314\n",
      "   0.2470284   0.22905935  0.11484716  0.10773823]\n",
      " [-0.1008024  -0.0763287   0.62334702 -0.02334793 -0.16615794 -0.21271471\n",
      "  -0.54014797  0.12035678  0.07019469 -0.49931083]]\n",
      "now loss =  0.017235060968372466  accuracy =  0.991\n",
      "gradient of first layer [[-0.15136196 -0.2496948   0.04712498  0.31085723 -0.24091055 -0.32616431\n",
      "  -0.32672378 -0.06548305  0.22642528  0.20238812]\n",
      " [ 0.0551346  -0.64707892  0.5169954   0.08162604  0.91330259  0.11970122\n",
      "   0.21489404  0.10497714  0.08169886 -0.04928244]\n",
      " [ 0.03500862  0.193054    0.39692263 -0.41690742 -0.02071361  0.0207959\n",
      "  -0.13025247 -0.45842473  0.07988095 -0.11326235]\n",
      " [ 0.37883528 -0.5381634   0.24046073  0.38477861 -0.59828885 -0.13616254\n",
      "   0.49633793  0.2468854  -0.15251073  0.10900557]\n",
      " [-0.02905785 -0.1575109  -0.11439421 -0.12429142 -0.3689169   0.18934373\n",
      "   0.23895711 -0.02404866  0.37530481  0.62461697]\n",
      " [ 0.56889835  0.8050419   0.4119834   0.14070868  0.94183061 -0.0746124\n",
      "  -0.07992398 -0.43231942 -0.00123044  0.10820066]\n",
      " [-0.22621096  0.38100487  0.12443136  0.18261774 -0.53946815 -0.67190527\n",
      "  -0.16371014  0.38396962  0.28394361 -0.08514397]\n",
      " [-0.36701295 -0.19516869 -0.14719604 -0.3165537  -0.71872261  0.03333823\n",
      "  -0.08107638  0.23773284 -0.05483984 -0.11213009]\n",
      " [-0.38464306 -0.31134707 -0.06631975  0.12928587 -0.67789045  0.44476509\n",
      "   0.24639348  0.23025364  0.11491619  0.10759547]\n",
      " [-0.10095518 -0.07630587  0.62329994 -0.02330783 -0.16647329 -0.21289524\n",
      "  -0.54024833  0.12055269  0.0702123  -0.49932759]]\n",
      "now loss =  0.024065869277149585  accuracy =  0.99\n",
      "gradient of first layer [[-0.15261605 -0.24949825  0.04670187  0.31117871 -0.24352208 -0.32763706\n",
      "  -0.32755646 -0.06388137  0.2265443   0.20223705]\n",
      " [ 0.05575453 -0.64711801  0.51703623  0.08142106  0.91448602  0.12046759\n",
      "   0.21526053  0.10416356  0.08151236 -0.04927509]\n",
      " [ 0.03515259  0.1930457   0.39693609 -0.4169518  -0.02043471  0.0209678\n",
      "  -0.1301695  -0.4586113   0.07983846 -0.1132636 ]\n",
      " [ 0.37873687 -0.53814869  0.2404297   0.38480451 -0.59849244 -0.1362787\n",
      "   0.49627312  0.24701136 -0.15249971  0.10899459]\n",
      " [-0.02997511 -0.15733548 -0.11476458 -0.12406505 -0.37085951  0.18825855\n",
      "   0.23831792 -0.02287662  0.37533531  0.62446233]\n",
      " [ 0.5705866   0.80480679  0.41245291  0.14024521  0.94528337 -0.07259994\n",
      "  -0.07882538 -0.43449045 -0.00146573  0.10836658]\n",
      " [-0.22670877  0.38112668  0.12413181  0.18270876 -0.54058873 -0.67245772\n",
      "  -0.16407466  0.38458761  0.2838916  -0.08525537]\n",
      " [-0.36706483 -0.1951608  -0.14721293 -0.31654022 -0.71883026  0.03327718\n",
      "  -0.08111065  0.23779915 -0.05483442 -0.11213608]\n",
      " [-0.38475352 -0.31130664 -0.06640649  0.12930366 -0.67814879  0.44463612\n",
      "   0.24630006  0.23039294  0.11488324  0.10755228]\n",
      " [-0.10105817 -0.07629037  0.62326701 -0.02328087 -0.16668662 -0.21301663\n",
      "  -0.54031624  0.12068443  0.07022351 -0.49933924]]\n",
      "now loss =  0.025548846204297836  accuracy =  0.991\n",
      "gradient of first layer [[-0.15536704 -0.24909236  0.04584524  0.31190838 -0.24920727 -0.3308932\n",
      "  -0.32936823 -0.06036024  0.2268636   0.20193326]\n",
      " [ 0.05553941 -0.6470673   0.51692884  0.08146976  0.91401901  0.12021077\n",
      "   0.21510066  0.10443779  0.08150106 -0.04932531]\n",
      " [ 0.03570121  0.19297239  0.39709543 -0.41709789 -0.01930545  0.02161194\n",
      "  -0.12981627 -0.45931174  0.07976186 -0.11321522]\n",
      " [ 0.37862675 -0.5381326   0.24039564  0.38483378 -0.5987199  -0.13640895\n",
      "   0.49620074  0.24715227 -0.15248666  0.10898266]\n",
      " [-0.03043317 -0.15724446 -0.11493751 -0.12394184 -0.37181391  0.1876938\n",
      "   0.23798941 -0.02228205  0.37534936  0.62437009]\n",
      " [ 0.57059438  0.80481122  0.41243718  0.14023768  0.94529    -0.07258761\n",
      "  -0.07882583 -0.43450189 -0.00148242  0.1083572 ]\n",
      " [-0.22996151  0.38161779  0.12309235  0.18356469 -0.54732653 -0.67630606\n",
      "  -0.16622696  0.38874913  0.28424652 -0.08562924]\n",
      " [-0.36716745 -0.19514578 -0.14724472 -0.31651297 -0.71904227  0.03315579\n",
      "  -0.08117812  0.23793047 -0.05482231 -0.11214723]\n",
      " [-0.3852689  -0.31121643 -0.06658581  0.12944068 -0.6792192   0.44401284\n",
      "   0.24594443  0.23105726  0.11491925  0.10747039]\n",
      " [-0.1012037  -0.0762691   0.62322196 -0.02324221 -0.16698726 -0.21318877\n",
      "  -0.54041189  0.12087064  0.07024072 -0.49935503]]\n",
      "now loss =  0.04650738447184914  accuracy =  0.99\n",
      "gradient of first layer [[-0.15634072 -0.24891218  0.04546077  0.312153   -0.25126292 -0.3320487\n",
      "  -0.33004523 -0.05911645  0.22690401  0.20177235]\n",
      " [ 0.05793052 -0.64728113  0.51737393  0.08077455  0.91880585  0.12303211\n",
      "   0.21654415  0.10137389  0.08094552 -0.04926579]\n",
      " [ 0.0364164   0.19291083  0.39723202 -0.41730265 -0.01786898  0.02244759\n",
      "  -0.12938839 -0.46022468  0.07959441 -0.11320293]\n",
      " [ 0.37851474 -0.53811561  0.24035932  0.3848635  -0.59895229 -0.13654155\n",
      "   0.49612637  0.24729542 -0.15247465  0.10896963]\n",
      " [-0.02998804 -0.15716325 -0.11508271 -0.12410322 -0.37103031  0.18817595\n",
      "   0.238135   -0.02284269  0.37501921  0.624195  ]\n",
      " [ 0.57168011  0.80469206  0.41268062  0.13992735  0.94748567 -0.07129989\n",
      "  -0.07814877 -0.43589361 -0.00169615  0.10841411]\n",
      " [-0.22853915  0.38153816  0.12324791  0.18312897 -0.54453685 -0.67462692\n",
      "  -0.16541339  0.386923    0.28381783 -0.08566408]\n",
      " [-0.36720837 -0.19513895 -0.14725946 -0.3165024  -0.71912794  0.03310736\n",
      "  -0.08120589  0.23798272 -0.05481924 -0.11215293]\n",
      " [-0.38448922 -0.3112437  -0.06651159  0.12920567 -0.67768857  0.44491082\n",
      "   0.2463713   0.23006511  0.11466221  0.10742286]\n",
      " [-0.10132    -0.07625117  0.62318351 -0.02321151 -0.16722894 -0.21332641\n",
      "  -0.54048939  0.12101923  0.07025255 -0.49936899]]\n",
      "now loss =  0.09219223801932253  accuracy =  0.982\n",
      "gradient of first layer [[-0.15928771 -0.24842909  0.04424874  0.31285424 -0.25752564 -0.33543445\n",
      "  -0.33204098 -0.05539918  0.22703489  0.20134099]\n",
      " [ 0.05427534 -0.64653869  0.5151888   0.08144248  0.9106532   0.11905607\n",
      "   0.21392813  0.10587734  0.08058542 -0.05007912]\n",
      " [ 0.0363903   0.19297107  0.39699847 -0.41735853 -0.0180488   0.02247687\n",
      "  -0.12946032 -0.46022176  0.07941933 -0.11330907]\n",
      " [ 0.37850858 -0.53811292  0.24034947  0.38486269 -0.59896945 -0.13654625\n",
      "   0.4961207   0.24730214 -0.15248015  0.10896551]\n",
      " [-0.03059674 -0.15688891 -0.11596325 -0.12412834 -0.37267305  0.18762104\n",
      "   0.23755578 -0.02215089  0.37453758  0.62379518]\n",
      " [ 0.56846505  0.80521642  0.41131104  0.14067571  0.94062562 -0.07496305\n",
      "  -0.08032753 -0.43185212 -0.00158225  0.10793958]\n",
      " [-0.23132169  0.38215993  0.12130491  0.18354937 -0.55089866 -0.67755586\n",
      "  -0.16745803  0.39030805  0.28332644 -0.08640042]\n",
      " [-0.36707221 -0.19515666 -0.14722298 -0.31654069 -0.71884933  0.03326967\n",
      "  -0.08111801  0.23780831 -0.05484086 -0.11214189]\n",
      " [-0.38360494 -0.31125719 -0.06665266  0.12885324 -0.67608987  0.44605682\n",
      "   0.24684587  0.22888469  0.11421925  0.10731135]\n",
      " [-0.10131245 -0.07624974  0.62317454 -0.02321704 -0.16721963 -0.21331377\n",
      "  -0.54048673  0.12100794  0.07024273 -0.49937315]]\n",
      "now loss =  0.01343917223134628  accuracy =  0.992\n",
      "gradient of first layer [[-0.16095345 -0.24818905  0.04372252  0.3133025  -0.26097785 -0.33740414\n",
      "  -0.33314007 -0.05327342  0.2272337   0.20116829]\n",
      " [ 0.05577027 -0.64673598  0.51557256  0.08101734  0.91369542  0.12085634\n",
      "   0.21489873  0.10395284  0.08034589 -0.04995077]\n",
      " [ 0.03695927  0.19289806  0.39715943 -0.41751434 -0.01688142  0.02314924\n",
      "  -0.12909292 -0.46094885  0.07933605 -0.11326032]\n",
      " [ 0.37834942 -0.53809031  0.24030058  0.38490598 -0.59929843 -0.13673498\n",
      "   0.49601592  0.24750547 -0.15246013  0.10894946]\n",
      " [-0.03251712 -0.15658698 -0.11657447 -0.12360443 -0.37665562  0.18531859\n",
      "   0.23626652 -0.01968932  0.37475221  0.62357369]\n",
      " [ 0.57053491  0.80491922  0.41191484  0.14010199  0.94488462 -0.07248501\n",
      "  -0.0789631  -0.43450685 -0.0018609   0.10814534]\n",
      " [-0.23148123  0.38219506  0.12118103  0.18357298 -0.55127537 -0.67771432\n",
      "  -0.16757448  0.39049661  0.28329569 -0.08643739]\n",
      " [-0.36713126 -0.19514832 -0.1472411  -0.31652462 -0.71897139  0.03319968\n",
      "  -0.08115686  0.23788373 -0.05483339 -0.1121478 ]\n",
      " [-0.38563085 -0.31095181 -0.06728402  0.12940503 -0.68028318  0.44363794\n",
      "   0.24549781  0.23147891  0.11446001  0.10709132]\n",
      " [-0.10147486 -0.0762267   0.62312438 -0.02317299 -0.16755551 -0.21350611\n",
      "  -0.54059361  0.12121534  0.07026301 -0.49938952]]\n",
      "now loss =  0.031047616032171942  accuracy =  0.992\n",
      "gradient of first layer [[-0.16165116 -0.24808199  0.04345113  0.31347438 -0.26245196 -0.33820943\n",
      "  -0.33360832 -0.05239261  0.22727954  0.20107631]\n",
      " [ 0.0573156  -0.64691102  0.51585096  0.08053582  0.91677374  0.12276062\n",
      "   0.2158804   0.10194594  0.08000605 -0.04986438]\n",
      " [ 0.03758978  0.19281928  0.39731613 -0.41769623 -0.01559938  0.02390695\n",
      "  -0.1286868  -0.46175902  0.07922782 -0.11321238]\n",
      " [ 0.37821897 -0.53807194  0.24025878  0.38494099 -0.59956899 -0.13688878\n",
      "   0.49592996  0.24767171 -0.15244488  0.10893571]\n",
      " [-0.03165905 -0.15667738 -0.11641155 -0.12386305 -0.37493727  0.18635534\n",
      "   0.23680213 -0.02079592  0.37456239  0.62361023]\n",
      " [ 0.5728497   0.80461249  0.41254697  0.13944593  0.94962248 -0.06971305\n",
      "  -0.07745543 -0.43747645 -0.00221218  0.10835008]\n",
      " [-0.23106256  0.38217474  0.12110534  0.18339449 -0.55052907 -0.67713911\n",
      "  -0.16733268  0.38992567  0.28309303 -0.08646702]\n",
      " [-0.3671718  -0.19514254 -0.14725471 -0.31651395 -0.71905579  0.03315214\n",
      "  -0.08118366  0.23793526 -0.0548291  -0.11215231]\n",
      " [-0.38520618 -0.31099268 -0.06721317  0.12927418 -0.67943857  0.44415256\n",
      "   0.24575973  0.23093014  0.11435817  0.10710479]\n",
      " [-0.10159558 -0.07620955  0.62308492 -0.02314083 -0.1678063  -0.21364815\n",
      "  -0.5406733   0.12136902  0.07027654 -0.49940255]]\n",
      "now loss =  0.044231504916210154  accuracy =  0.991\n",
      "gradient of first layer [[-0.16394135 -0.24774813  0.04267501  0.31407932 -0.26723003 -0.34090149\n",
      "  -0.33512875 -0.04948145  0.22751861  0.20081937]\n",
      " [ 0.05733823 -0.64683981  0.5156085   0.08046212  0.91667825  0.12284377\n",
      "   0.21582827  0.10188765  0.07980496 -0.04997737]\n",
      " [ 0.03859938  0.19270279  0.39757198 -0.41798461 -0.01354048  0.0251055\n",
      "  -0.12804475 -0.46304926  0.0790498  -0.11314571]\n",
      " [ 0.37809789 -0.53805454  0.24021917  0.38497347 -0.59982071 -0.1370317\n",
      "   0.49584977  0.24782589 -0.15243123  0.10892257]\n",
      " [-0.03087361 -0.15671474 -0.1163354  -0.12411322 -0.37339824  0.18728698\n",
      "   0.23724999 -0.02180277  0.37431166  0.62358004]\n",
      " [ 0.57323342  0.80458201  0.41257671  0.13931307  0.95036705 -0.06923181\n",
      "  -0.07722297 -0.43797751 -0.00233163  0.10834955]\n",
      " [-0.23274088  0.38247802  0.12033576  0.18378195 -0.55414563 -0.67906293\n",
      "  -0.1684987   0.39203443  0.28310966 -0.08674534]\n",
      " [-0.36722162 -0.19513499 -0.14727216 -0.31650094 -0.71916     0.03309359\n",
      "  -0.08121697  0.23799859 -0.05482445 -0.11215829]\n",
      " [-0.38542648 -0.31092331 -0.06738022  0.12931156 -0.67994729  0.44389782\n",
      "   0.24557793  0.23120566  0.11429983  0.10702324]\n",
      " [-0.10173473 -0.07618957  0.62303935 -0.02310352 -0.16809561 -0.21381237\n",
      "  -0.54076545  0.12154621  0.07029219 -0.49941767]]\n",
      "now loss =  0.1972098438593924  accuracy =  0.956\n",
      "gradient of first layer [[-0.16326561 -0.24768457  0.04243901  0.31378404 -0.26608361 -0.34003651\n",
      "  -0.3348281  -0.05038519  0.22706353  0.20065096]\n",
      " [ 0.06105966 -0.64678603  0.51516922  0.07905738  0.92347966  0.12747095\n",
      "   0.21775264  0.09699454  0.07801025 -0.05045179]\n",
      " [ 0.03920036  0.19275182  0.39740709 -0.41822847 -0.01249386  0.02585338\n",
      "  -0.12777309 -0.4638438   0.07867847 -0.11327855]\n",
      " [ 0.3781023  -0.53805086  0.24020821  0.38496922 -0.59981838 -0.13702468\n",
      "   0.49584869  0.2478191  -0.15244212  0.10891626]\n",
      " [-0.03195038 -0.15617061 -0.11771733 -0.12405108 -0.37620835  0.18610218\n",
      "   0.23616203 -0.02050695  0.37356017  0.62288343]\n",
      " [ 0.57421979  0.80461965  0.41236448  0.13890924  0.95211851 -0.06798191\n",
      "  -0.07673754 -0.43928666 -0.00288655  0.1081747 ]\n",
      " [-0.22548173  0.38206474  0.12093626  0.18138259 -0.54006324 -0.67021737\n",
      "  -0.16425256  0.38261872  0.28082428 -0.0868841 ]\n",
      " [-0.36712397 -0.19514603 -0.14724793 -0.31652936 -0.71896142  0.03321019\n",
      "  -0.08115465  0.23787352 -0.05484193 -0.11215178]\n",
      " [-0.38475682 -0.31077691 -0.06778835  0.12898924 -0.67890728  0.4447437\n",
      "   0.24579517  0.23030724  0.11369345  0.10674635]\n",
      " [-0.10168069 -0.07619121  0.62303958 -0.0231225  -0.16799304 -0.21374579\n",
      "  -0.54073514  0.12147569  0.07027156 -0.49942114]]\n",
      "now loss =  0.03482225252814662  accuracy =  0.975\n",
      "gradient of first layer [[-0.16657664 -0.2471207   0.04098856  0.31458145 -0.27316516 -0.34386042\n",
      "  -0.33712305 -0.04621017  0.22714217  0.20012611]\n",
      " [ 0.05325285 -0.6453012   0.51127195  0.08079773  0.90647424  0.11858062\n",
      "   0.21223044  0.10676874  0.07784466 -0.05187352]\n",
      " [ 0.03960852  0.1928124   0.39719218 -0.41842893 -0.01185329  0.02640264\n",
      "  -0.12760264 -0.46440212  0.0783574  -0.11339552]\n",
      " [ 0.37805938 -0.53804179  0.24018262  0.3849775  -0.59991443 -0.13707203\n",
      "   0.49581758  0.24787209 -0.15244602  0.1089071 ]\n",
      " [-0.03207164 -0.15581251 -0.11876155 -0.12426335 -0.37703025  0.18612092\n",
      "   0.2357673  -0.02044637  0.37275307  0.62236793]\n",
      " [ 0.57246797  0.80499374  0.41134242  0.13925668  0.94821661 -0.06993805\n",
      "  -0.07801287 -0.43711289 -0.00303868  0.10779192]\n",
      " [-0.22920569  0.38302863  0.11824607  0.18198193 -0.5486748  -0.67425898\n",
      "  -0.16710076  0.38717481  0.2801041  -0.08791804]\n",
      " [-0.36711851 -0.19514643 -0.14724717 -0.31653107 -0.71895069  0.03321681\n",
      "  -0.08115135  0.23786646 -0.05484336 -0.11215167]\n",
      " [-0.38604133 -0.31040646 -0.0687835   0.12919716 -0.68189531  0.44331808\n",
      "   0.24476132  0.23189151  0.11336381  0.1063154 ]\n",
      " [-0.1016972  -0.07618491  0.62301991 -0.0231221  -0.16803591 -0.21376136\n",
      "  -0.54074946  0.1214947   0.07026261 -0.49942862]]\n",
      "now loss =  0.012960439993794337  accuracy =  0.99\n",
      "gradient of first layer [[-0.16781747 -0.24696063  0.04058012  0.31491727 -0.27573594 -0.34531731\n",
      "  -0.33794299 -0.04463133  0.22727883  0.19999259]\n",
      " [ 0.05419567 -0.64539402  0.51145862  0.08050081  0.90835114  0.11973191\n",
      "   0.21283149  0.10554938  0.07765047 -0.05181375]\n",
      " [ 0.0400129   0.19276747  0.39729461 -0.41854746 -0.01103351  0.0268861\n",
      "  -0.12734214 -0.46492104  0.07828974 -0.11336462]\n",
      " [ 0.37793162 -0.53802596  0.24014307  0.38501296 -0.60017765 -0.13722281\n",
      "   0.49573368  0.24803498 -0.15243     0.10889437]\n",
      " [-0.03216627 -0.15578541 -0.1188525  -0.12425228 -0.3772587   0.18602025\n",
      "   0.23568789 -0.02033289  0.37271596  0.62232659]\n",
      " [ 0.57437065  0.80476884  0.4118805   0.13871249  0.95210796 -0.0676767\n",
      "  -0.07677369 -0.43954601 -0.0033164   0.10795997]\n",
      " [-0.22972883  0.3831157   0.11798367  0.18209211 -0.54981432 -0.67483801\n",
      "  -0.16746085  0.38782537  0.28009627 -0.08800313]\n",
      " [-0.36713749 -0.19514409 -0.14725297 -0.31652578 -0.71898974  0.03319437\n",
      "  -0.0811638   0.23789068 -0.05484092 -0.11215354]\n",
      " [-0.38612494 -0.31038509 -0.06884891  0.12920948 -0.68208977  0.44322676\n",
      "   0.24469538  0.23199375  0.11334274  0.10628738]\n",
      " [-0.10182839 -0.07616863  0.62297911 -0.02308574 -0.1683063  -0.21391612\n",
      "  -0.54083565  0.12166192  0.07027893 -0.49944177]]\n",
      "now loss =  0.03327557284841959  accuracy =  0.992\n",
      "gradient of first layer [[-0.16990152 -0.24668033  0.03989028  0.31549295 -0.28005044 -0.34779231\n",
      "  -0.33933884 -0.0419727   0.22749731  0.19974556]\n",
      " [ 0.05577383 -0.64546772  0.5117262   0.08002889  0.91150055  0.12154976\n",
      "   0.21374747  0.10354911  0.07723181 -0.05183759]\n",
      " [ 0.04073283  0.19271699  0.3974501  -0.41875667 -0.00958044  0.0277189\n",
      "  -0.12690814 -0.46583406  0.07813051 -0.11335111]\n",
      " [ 0.37776765 -0.53800516  0.24009107  0.38505856 -0.60051606 -0.13741692\n",
      "   0.49562518  0.24824403 -0.15241054  0.10887687]\n",
      " [-0.03230968 -0.15565198 -0.11910348 -0.12423652 -0.37764561  0.18579016\n",
      "   0.23547028 -0.02013556  0.37252329  0.62212832]\n",
      " [ 0.57510818  0.80471009  0.41203297  0.13849267  0.953592   -0.0668087\n",
      "  -0.07632261 -0.44048705 -0.00348025  0.10797954]\n",
      " [-0.23022623  0.38328222  0.11763185  0.18220094 -0.55093137 -0.67546388\n",
      "  -0.16789388  0.38846788  0.27996412 -0.08821255]\n",
      " [-0.36718604 -0.19513776 -0.14726866 -0.31651232 -0.71909007  0.03313682\n",
      "  -0.0811961   0.2379526  -0.05483547 -0.11215898]\n",
      " [-0.38655535 -0.31026616 -0.06909902  0.12931629 -0.68302826  0.4426818\n",
      "   0.24434149  0.2325509   0.11327755  0.10613917]\n",
      " [-0.10198533 -0.07614855  0.62292896 -0.02304217 -0.16863038 -0.21410194\n",
      "  -0.54093967  0.12186201  0.0702972  -0.49945879]]\n",
      "now loss =  0.01929450564600975  accuracy =  0.993\n",
      "gradient of first layer [[-0.17076279 -0.24657771  0.03960482  0.31572697 -0.28183695 -0.34879625\n",
      "  -0.33990331 -0.04088126  0.2275961   0.19965787]\n",
      " [ 0.05625897 -0.64551662  0.51180713  0.07986912  0.91245826  0.12215949\n",
      "   0.21406042  0.10291604  0.07712314 -0.05180486]\n",
      " [ 0.04096545  0.19269263  0.39750911 -0.41882449 -0.00910807  0.02799604\n",
      "  -0.12675919 -0.46613171  0.07809045 -0.1133345 ]\n",
      " [ 0.37757439 -0.53798255  0.24003133  0.38511252 -0.6009144  -0.13764432\n",
      "   0.49549888  0.24848982 -0.15238541  0.10885841]\n",
      " [-0.03287573 -0.15557751 -0.11930768 -0.12408238 -0.37882553  0.1851235\n",
      "   0.23508866 -0.01941686  0.372571    0.62205328]\n",
      " [ 0.57629674  0.80457112  0.41236125  0.13814623  0.95601896 -0.06538542\n",
      "  -0.07554581 -0.44200873 -0.00365997  0.10808543]\n",
      " [-0.2313839   0.38342446  0.11720435  0.18249934 -0.55336009 -0.67678608\n",
      "  -0.16865383  0.38992389  0.2800689  -0.08833677]\n",
      " [-0.36731339 -0.19512287 -0.14730805 -0.31647676 -0.71935258  0.03298699\n",
      "  -0.08127932  0.23811456 -0.05481892 -0.11217115]\n",
      " [-0.38660603 -0.31025519 -0.06913361  0.12932694 -0.68314089  0.44262252\n",
      "   0.24430123  0.23261461  0.11326699  0.10612071]\n",
      " [-0.10216901 -0.07612706  0.62287206 -0.02299092 -0.16900905 -0.21431798\n",
      "  -0.5410597   0.12209558  0.07032102 -0.49947635]]\n",
      "now loss =  0.021400130783169678  accuracy =  0.991\n",
      "gradient of first layer [[-0.17219764 -0.24640552  0.03913832  0.31612074 -0.28481216 -0.35047583\n",
      "  -0.34084407 -0.03906242  0.22777025  0.1995175 ]\n",
      " [ 0.05724421 -0.64561773  0.51201232  0.07955798  0.91443209  0.12336755\n",
      "   0.21469352  0.10164258  0.076924   -0.05173872]\n",
      " [ 0.04198938  0.19257507  0.39780414 -0.41911929 -0.00700638  0.02921196\n",
      "  -0.12609241 -0.46743727  0.07793891 -0.11324613]\n",
      " [ 0.3774644  -0.5379695   0.23999627  0.38514296 -0.601142   -0.13777334\n",
      "   0.49542687  0.24862939 -0.15237158  0.10884784]\n",
      " [-0.03104279 -0.15578453 -0.11879736 -0.12461647 -0.37507118  0.18730433\n",
      "   0.23627785 -0.02175597  0.37228427  0.62220082]\n",
      " [ 0.57757936  0.8044259   0.41271254  0.13776794  0.95863952 -0.06385086\n",
      "  -0.07471084 -0.44364881 -0.00386146  0.10819387]\n",
      " [-0.23266419  0.3835888   0.1167114   0.18282376 -0.55606052 -0.6782475\n",
      "  -0.16950145  0.39153025  0.28017142 -0.08848181]\n",
      " [-0.3673646  -0.19511672 -0.1473252  -0.31646296 -0.71945898  0.03292738\n",
      "  -0.0813129   0.23817936 -0.05481309 -0.11217631]\n",
      " [-0.38589254 -0.31033248 -0.06894572  0.12911573 -0.68168593  0.44347367\n",
      "   0.2447612   0.23170263  0.11314709  0.10617339]\n",
      " [-0.10227155 -0.07611488  0.62283911 -0.02296267 -0.16922137 -0.21443811\n",
      "  -0.54112684  0.12222563  0.07033371 -0.49948627]]\n",
      "now loss =  0.022634053551713138  accuracy =  0.994\n",
      "gradient of first layer [[-0.17371032 -0.24622269  0.0386543   0.31654424 -0.28794014 -0.35226189\n",
      "  -0.3418415  -0.03713962  0.22795537  0.19936135]\n",
      " [ 0.05963037 -0.6458613   0.51267914  0.07886901  0.91932663  0.12617367\n",
      "   0.21621897  0.09861389  0.07653259 -0.05157264]\n",
      " [ 0.04238945  0.19253866  0.39791222 -0.41923307 -0.00618641  0.02967671\n",
      "  -0.12584191 -0.46794324  0.07786774 -0.11322509]\n",
      " [ 0.37733281 -0.53795401  0.23995507  0.38518006 -0.60141375 -0.13792861\n",
      "   0.49534055  0.24879659 -0.15235451  0.10883506]\n",
      " [-0.03312212 -0.15551231 -0.1194819  -0.12402719 -0.37937424  0.18482309\n",
      "   0.23488169 -0.01910456  0.3725107   0.62195273]\n",
      " [ 0.57868571  0.80430993  0.41302642  0.13744908  0.96091173 -0.06254926\n",
      "  -0.0740011  -0.44505327 -0.00403849  0.10827393]\n",
      " [-0.23232318  0.38357914  0.11674948  0.18271169 -0.55538476 -0.67785027\n",
      "  -0.16931001  0.39109939  0.28005732 -0.08850375]\n",
      " [-0.3674555  -0.19510593 -0.14735385 -0.31643738 -0.71964676  0.03282009\n",
      "  -0.08137263  0.23829488 -0.05480152 -0.11218533]\n",
      " [-0.38727881 -0.31015336 -0.06940067  0.12950744 -0.68455525  0.44182283\n",
      "   0.24383328  0.23346882  0.11330126  0.10601212]\n",
      " [-0.10240075 -0.07609962  0.62279854 -0.02292627 -0.16948823 -0.21459057\n",
      "  -0.54121166  0.12238981  0.07035034 -0.49949895]]\n",
      "now loss =  0.05168788823907292  accuracy =  0.98\n",
      "gradient of first layer [[-0.17563422 -0.24591853  0.03772065  0.31698615 -0.2921138  -0.35442451\n",
      "  -0.3431691  -0.03474866  0.22796319  0.19905477]\n",
      " [ 0.05702169 -0.64529036  0.51077398  0.07927241  0.91328397  0.12344487\n",
      "   0.21428956  0.10175758  0.07606985 -0.05222611]\n",
      " [ 0.04269883  0.19256035  0.39778672 -0.41938617 -0.00567708  0.03010575\n",
      "  -0.12568846 -0.46836725  0.07765809 -0.11328583]\n",
      " [ 0.37739633 -0.53795817  0.23996267  0.38515819 -0.60129009 -0.13784959\n",
      "   0.49537988  0.24871404 -0.15237188  0.10883663]\n",
      " [-0.03182165 -0.15548999 -0.11972552 -0.124581   -0.37705773  0.18652648\n",
      "   0.23557767 -0.02083985  0.37183405  0.62179301]\n",
      " [ 0.57815416  0.80446224  0.4124693   0.13747737  0.95958029 -0.06304365\n",
      "  -0.07442349 -0.44444099 -0.00425631  0.10808146]\n",
      " [-0.23274692  0.38391057  0.11547556  0.18247759 -0.55694788 -0.67797879\n",
      "  -0.16981274  0.39145999  0.27927011 -0.0889651 ]\n",
      " [-0.36726614 -0.19512535 -0.14730366 -0.31649379 -0.71926113  0.03304641\n",
      "  -0.08125024  0.23805316 -0.05483317 -0.11217067]\n",
      " [-0.38599154 -0.31019119 -0.06940985  0.12902605 -0.68212883  0.44344428\n",
      "   0.24457538  0.23178288  0.11280943  0.10595117]\n",
      " [-0.10231114 -0.07610634  0.62281232 -0.02295615 -0.16931183 -0.21448007\n",
      "  -0.54115558  0.12227382  0.07032806 -0.49949561]]\n",
      "now loss =  0.016792512029655527  accuracy =  0.996\n",
      "gradient of first layer [[-0.17742225 -0.24571914  0.03715924  0.31749128 -0.29580826 -0.35652616\n",
      "  -0.34433834 -0.03248237  0.22820074  0.19889115]\n",
      " [ 0.05629604 -0.64520673  0.51050324  0.07946023  0.91175751  0.12261865\n",
      "   0.21381464  0.10266603  0.07613931 -0.05229935]\n",
      " [ 0.04304501  0.1925268   0.39788646 -0.41948513 -0.00496542  0.03051029\n",
      "  -0.12546735 -0.46880518  0.07760265 -0.11326219]\n",
      " [ 0.37717638 -0.53793364  0.23989504  0.38522095 -0.60174376 -0.13810897\n",
      "   0.49523607  0.24899316 -0.15234166  0.10881692]\n",
      " [-0.03291839 -0.15534896 -0.12007189 -0.12426143 -0.3793171   0.18520484\n",
      "   0.23483873 -0.01943712  0.3719632   0.62166487]\n",
      " [ 0.57829633  0.80444525  0.4124914   0.13742611  0.95986168 -0.06285876\n",
      "  -0.07432868 -0.44462784 -0.00429005  0.10809105]\n",
      " [-0.23573912  0.38424374  0.11450938  0.18331095 -0.56314925 -0.68147434\n",
      "  -0.17176659  0.39524337  0.27965371 -0.08923818]\n",
      " [-0.36736884 -0.1951139  -0.14733517 -0.31646445 -0.71947294  0.03292524\n",
      "  -0.0813174   0.2381835  -0.054819   -0.11217985]\n",
      " [-0.38681378 -0.31008941 -0.06966959  0.12926352 -0.68382394  0.44245946\n",
      "   0.24402551  0.23283213  0.11290896  0.10585956]\n",
      " [-0.10253734 -0.07608112  0.62274267 -0.02289167 -0.16977847 -0.21474675\n",
      "  -0.54130347  0.12256084  0.07035907 -0.4995159 ]]\n",
      "now loss =  0.03536770063993389  accuracy =  0.99\n",
      "gradient of first layer [[-0.17808371 -0.24563217  0.03688616  0.3176565  -0.29721385 -0.35728186\n",
      "  -0.34478282 -0.03165435  0.22824078  0.19880667]\n",
      " [ 0.0595586  -0.64552272  0.51131225  0.0784602   0.91837794  0.12653346\n",
      "   0.21590977  0.09849737  0.07553941 -0.05208379]\n",
      " [ 0.04433176  0.19239577  0.39825053 -0.41986331 -0.00232622  0.03203286\n",
      "  -0.12463654 -0.47043971  0.07739638 -0.11316592]\n",
      " [ 0.37703772 -0.53791814  0.23985009  0.3852598  -0.6020312  -0.13827146\n",
      "   0.49514523  0.2491686  -0.15232406  0.10880386]\n",
      " [-0.03146774 -0.15547413 -0.119719   -0.1247039  -0.37636917  0.18692159\n",
      "   0.23575239 -0.02128069  0.37167702  0.62173365]\n",
      " [ 0.57947695  0.80432865  0.41276643  0.13705449  0.96224789 -0.0614288\n",
      "  -0.07356898 -0.44614143 -0.00451893  0.1081655 ]\n",
      " [-0.23399281  0.38409478  0.11483193  0.18273664 -0.55967053 -0.67933572\n",
      "  -0.17066216  0.39299287  0.27925197 -0.08916057]\n",
      " [-0.36743386 -0.19510654 -0.14735702 -0.31644657 -0.71960818  0.03284943\n",
      "  -0.08136005  0.23826561 -0.05481128 -0.11218619]\n",
      " [-0.38616961 -0.31013669 -0.06953507  0.12906119 -0.68252811  0.4432239\n",
      "   0.24442378  0.23201185  0.11276396  0.10587889]\n",
      " [-0.1026703  -0.07606619  0.6226991  -0.0228546  -0.17005435 -0.21490236\n",
      "  -0.54139063  0.12272899  0.07037561 -0.49952858]]\n",
      "now loss =  0.07693884125787706  accuracy =  0.987\n",
      "gradient of first layer [[-0.17928122 -0.24543303  0.03631765  0.31795007 -0.29980049 -0.35866272\n",
      "  -0.34562529 -0.03015437  0.22824819  0.19860358]\n",
      " [ 0.05951562 -0.64527906  0.51063529  0.07830653  0.91790524  0.12657559\n",
      "   0.21567316  0.09849425  0.07501147 -0.05241505]\n",
      " [ 0.04463631  0.19242145  0.39818898 -0.41998865 -0.00178659  0.03240989\n",
      "  -0.12448883 -0.47083762  0.07722682 -0.11321936]\n",
      " [ 0.37704119 -0.53791578  0.23984368  0.38525691 -0.60202845 -0.13826627\n",
      "   0.4951452   0.24916349 -0.15233047  0.10880057]\n",
      " [-0.03092575 -0.15535281 -0.12002284 -0.12497397 -0.37551869  0.18761173\n",
      "   0.23594965 -0.02200192  0.37121407  0.62153338]\n",
      " [ 0.57978411  0.80436441  0.41265383  0.13691201  0.96276333 -0.06103123\n",
      "  -0.07342935 -0.44655087 -0.00472764  0.10809222]\n",
      " [-0.2330184   0.38422416  0.11447434  0.18228943 -0.55803799 -0.67809051\n",
      "  -0.17022963  0.39170008  0.27858526 -0.08940076]\n",
      " [-0.36732481 -0.19511627 -0.14732897 -0.31647941 -0.71938626  0.03297883\n",
      "  -0.08129053  0.23812688 -0.05483106 -0.11217945]\n",
      " [-0.38508205 -0.31013992 -0.06949774  0.12867554 -0.68045248  0.44453752\n",
      "   0.24503432  0.23061231  0.11236585  0.10581615]\n",
      " [-0.102655   -0.07606502  0.62269602 -0.02286099 -0.17002733 -0.21488316\n",
      "  -0.54138302  0.12270886  0.07036726 -0.49953103]]\n",
      "now loss =  0.028462914139701194  accuracy =  0.993\n",
      "gradient of first layer [[-0.18070927 -0.24526374  0.03577842  0.31832835 -0.30280699 -0.36030289\n",
      "  -0.34657067 -0.02836391  0.22837987  0.19845152]\n",
      " [ 0.05834633 -0.64509419  0.50996351  0.07854175  0.91530641  0.12532367\n",
      "   0.21486425  0.09991937  0.07495529 -0.05261335]\n",
      " [ 0.04425168  0.19247884  0.39801259 -0.41989359 -0.00261198  0.03196946\n",
      "  -0.12475516 -0.47035669  0.07723492 -0.11327981]\n",
      " [ 0.37692428 -0.53790272  0.2398031   0.38528906 -0.60227257 -0.13840184\n",
      "   0.49506842  0.24931062 -0.152317    0.10878947]\n",
      " [-0.0340507  -0.15497249 -0.12113966 -0.12411271 -0.38204799  0.18395219\n",
      "   0.23386168 -0.01805545  0.37152591  0.62118138]\n",
      " [ 0.58048761  0.80430934  0.41274841  0.13666517  0.96414319 -0.06014499\n",
      "  -0.07298285 -0.44746637 -0.00491243  0.10811736]\n",
      " [-0.23433881  0.38442396  0.11373905  0.18255986 -0.56096024 -0.67950526\n",
      "  -0.17113441  0.39331069  0.2785416  -0.08961092]\n",
      " [-0.36733873 -0.19511431 -0.14733664 -0.31647663 -0.71941707  0.03296412\n",
      "  -0.0812999   0.23814378 -0.05483135 -0.11218145]\n",
      " [-0.38677774 -0.3099311  -0.07011741  0.12913767 -0.68400379  0.44255815\n",
      "   0.24389997  0.232751    0.11252568  0.10562145]\n",
      " [-0.10275945 -0.07605327  0.6226592  -0.02283247 -0.17024578 -0.21500399\n",
      "  -0.54145166  0.12284019  0.07037892 -0.49954108]]\n",
      "now loss =  0.031907842994707454  accuracy =  0.991\n",
      "gradient of first layer [[-1.81542760e-01 -2.45151204e-01  3.53967139e-02  3.18528025e-01\n",
      "  -3.04598897e-01 -3.61240072e-01 -3.47135811e-01 -2.73277006e-02\n",
      "   2.28405499e-01  1.98334494e-01]\n",
      " [ 5.81774917e-02 -6.44998859e-01  5.09540866e-01  7.84679489e-02\n",
      "   9.14735691e-01  1.25263357e-01  2.14695162e-01  1.00072327e-01\n",
      "   7.47124906e-02 -5.27517009e-02]\n",
      " [ 4.52030607e-02  1.92399144e-01  3.98232296e-01 -4.20191376e-01\n",
      "  -6.91817511e-04  3.31128553e-02 -1.24149225e-01 -4.71572118e-01\n",
      "   7.70452725e-02 -1.13226963e-01]\n",
      " [ 3.76724458e-01 -5.37881071e-01  2.39736495e-01  3.85345269e-01\n",
      "  -6.02688155e-01 -1.38635141e-01  4.94937386e-01  2.49562717e-01\n",
      "  -1.52292011e-01  1.08771136e-01]\n",
      " [-3.21057834e-02 -1.55114256e-01 -1.20750227e-01 -1.24736699e-01\n",
      "  -3.78152732e-01  1.86296298e-01  2.35080578e-01 -2.05447248e-02\n",
      "   3.71087440e-01  6.21254907e-01]\n",
      " [ 5.80790539e-01  8.04305902e-01  4.12698668e-01  1.36529045e-01\n",
      "   9.64685179e-01 -5.97319850e-02 -7.28064962e-02 -4.47874168e-01\n",
      "  -5.05931891e-03  1.08094293e-01]\n",
      " [-2.35499738e-01  3.84622347e-01  1.12997436e-01  1.82767545e-01\n",
      "  -5.63584177e-01 -6.80727193e-01 -1.71951769e-01  3.94717533e-01\n",
      "   2.78429136e-01 -8.98389523e-02]\n",
      " [-3.67409791e-01 -1.95106056e-01 -1.47362899e-01 -3.16457464e-01\n",
      "  -7.19566311e-01  3.28820724e-02 -8.13469481e-02  2.38233016e-01\n",
      "  -5.48243730e-02 -1.12188934e-01]\n",
      " [-3.85550590e-01 -3.10020189e-01 -6.98741851e-02  1.28742471e-01\n",
      "  -6.81545683e-01  4.44036782e-01  2.44667730e-01  2.31180899e-01\n",
      "   1.12245240e-01  1.05664131e-01]\n",
      " [-1.02952663e-01 -7.60322819e-02  6.22594549e-01 -2.27782056e-02\n",
      "  -1.70647749e-01 -2.15229482e-01 -5.41578405e-01  1.23083907e-01\n",
      "   7.04028900e-02 -4.99558903e-01]]\n",
      "now loss =  0.020089314919418295  accuracy =  0.994\n",
      "gradient of first layer [[-0.18373308 -0.24492067  0.03469785  0.31915897 -0.30913317 -0.36381772\n",
      "  -0.34857198 -0.02455678  0.22869957  0.19813712]\n",
      " [ 0.06087229 -0.64523301  0.51030295  0.07766868  0.92025623  0.12843107\n",
      "   0.21642001  0.09666083  0.0742665  -0.05256548]\n",
      " [ 0.04640847  0.19229187  0.39859313 -0.42054211  0.00179208  0.034518\n",
      "  -0.12337704 -0.47309244  0.07685675 -0.11314265]\n",
      " [ 0.37661476 -0.53786975  0.2397016   0.38537689 -0.60291525 -0.138764\n",
      "   0.49486566  0.24970136 -0.15227702  0.10876155]\n",
      " [-0.03270601 -0.15500494 -0.12097009 -0.12456079 -0.37940141  0.18553512\n",
      "   0.23464096 -0.01976474  0.37111841  0.62114036]\n",
      " [ 0.58138723  0.80425234  0.4128555   0.1363485   0.96589898 -0.0590205\n",
      "  -0.0724228  -0.44863439 -0.00516285  0.10813702]\n",
      " [-0.23526599  0.3846302   0.11299546  0.18268065 -0.56314712 -0.6804454\n",
      "  -0.17182548  0.39441606  0.27833617 -0.08985482]\n",
      " [-0.36748669 -0.19509806 -0.14738746 -0.31643532 -0.71972557  0.03279173\n",
      "  -0.08139728  0.2383302  -0.05481395 -0.11219571]\n",
      " [-0.38672746 -0.30987001 -0.07026169  0.12908452 -0.68398229  0.44261719\n",
      "   0.24386942  0.23268303  0.11237681  0.10552304]\n",
      " [-0.10306427 -0.07602076  0.62255905 -0.02274604 -0.17087878 -0.21536059\n",
      "  -0.54165138  0.12322496  0.07041814 -0.49956866]]\n",
      "now loss =  0.04498005834065401  accuracy =  0.965\n",
      "gradient of first layer [[-1.83358281e-01 -2.44781343e-01  3.42545487e-02  3.18897586e-01\n",
      "  -3.08680947e-01 -3.63266333e-01 -3.48483401e-01 -2.50917166e-02\n",
      "   2.28205647e-01  1.97911125e-01]\n",
      " [ 5.69495540e-02 -6.44309325e-01  5.07289356e-01  7.83138402e-02\n",
      "   9.11115432e-01  1.24222169e-01  2.13391358e-01  1.01408601e-01\n",
      "   7.34327673e-02 -5.36768007e-02]\n",
      " [ 4.57936420e-02  1.92487324e-01  3.98001110e-01 -4.20466490e-01\n",
      "   2.95200249e-04  3.38586581e-02 -1.23900190e-01 -4.72352744e-01\n",
      "   7.66220235e-02 -1.13389615e-01]\n",
      " [ 3.76698539e-01 -5.37871571e-01  2.39704477e-01  3.85345778e-01\n",
      "  -6.02756290e-01 -1.38658948e-01  4.94915151e-01  2.49592024e-01\n",
      "  -1.52306001e-01  1.08759778e-01]\n",
      " [-3.24768924e-02 -1.54596124e-01 -1.22117830e-01 -1.24932007e-01\n",
      "  -3.79604995e-01  1.85951611e-01  2.34388694e-01 -2.01522069e-02\n",
      "   3.70082875e-01  6.20523841e-01]\n",
      " [ 5.82770016e-01  8.04280906e-01  4.12672239e-01  1.35768137e-01\n",
      "   9.68390951e-01 -5.72224364e-02 -7.16591212e-02 -4.50470102e-01\n",
      "  -5.81786707e-03  1.08012614e-01]\n",
      " [-2.32013968e-01  3.84853625e-01  1.12148866e-01  1.81212272e-01\n",
      "  -5.57520576e-01 -6.76174367e-01 -1.70171094e-01  3.90069993e-01\n",
      "   2.76451093e-01 -9.03670676e-02]\n",
      " [-3.67184222e-01 -1.95124170e-01 -1.47307849e-01 -3.16527687e-01\n",
      "  -7.19109758e-01  3.31519097e-02 -8.12029450e-02  2.37945403e-01\n",
      "  -5.48673925e-02 -1.12175541e-01]\n",
      " [-3.85690165e-01 -3.09692019e-01 -7.07175958e-02  1.28588343e-01\n",
      "  -6.82271459e-01  4.43922893e-01  2.44284584e-01  2.31311860e-01\n",
      "   1.11585315e-01  1.05197550e-01]\n",
      " [-1.02944011e-01 -7.60261322e-02  6.22573135e-01 -2.27878038e-02\n",
      "  -1.70644606e-01 -2.15212593e-01 -5.41578117e-01  1.23069465e-01\n",
      "   7.03838989e-02 -4.99567406e-01]]\n",
      "now loss =  0.009951079687739093  accuracy =  0.992\n",
      "gradient of first layer [[-0.18619358 -0.24450299  0.03330105  0.31969869 -0.31456996 -0.36656287\n",
      "  -0.35034035 -0.02152023  0.22855473  0.19765259]\n",
      " [ 0.05773534 -0.64431869  0.50727951  0.07801019  0.91258479  0.12522167\n",
      "   0.21385086  0.10037894  0.07313303 -0.05370825]\n",
      " [ 0.04636157  0.19245419  0.39812352 -0.42064487  0.00143753  0.03452868\n",
      "  -0.12354945 -0.47307353  0.07649554 -0.11337399]\n",
      " [ 0.3765231  -0.5378548   0.23964765  0.38539619 -0.6031193  -0.13886392\n",
      "   0.49480047  0.24981341 -0.1522829   0.10874437]\n",
      " [-0.03366686 -0.15443034 -0.12263223 -0.12462291 -0.38213146  0.18456057\n",
      "   0.23355953 -0.01865286  0.37012296  0.62033325]\n",
      " [ 0.58262339  0.80431106  0.41253712  0.13577867  0.96803529 -0.0573558\n",
      "  -0.07176559 -0.45030056 -0.00586122  0.10797211]\n",
      " [-0.23317942  0.38501996  0.11153306  0.18147192 -0.56007403 -0.67745277\n",
      "  -0.17097529  0.39150393  0.27642994 -0.09055432]\n",
      " [-0.36726392 -0.19511653 -0.14733374 -0.31650482 -0.71927472  0.03305884\n",
      "  -0.08125504  0.23804595 -0.05485694 -0.11218255]\n",
      " [-0.38704794 -0.30953383 -0.07122993  0.1289574  -0.68511805  0.44233919\n",
      "   0.24337045  0.23302315  0.11169953  0.10503199]\n",
      " [-0.10312024 -0.07600929  0.62251604 -0.02273717 -0.17100925 -0.21541847\n",
      "  -0.54169331  0.12329184  0.07040711 -0.49958288]]\n",
      "now loss =  0.019356917946075417  accuracy =  0.994\n",
      "gradient of first layer [[-0.1878638  -0.24435022  0.03278176  0.32018866 -0.31801472 -0.36852332\n",
      "  -0.35142894 -0.01941053  0.22879339  0.19751563]\n",
      " [ 0.05955149 -0.64447813  0.50782411  0.07747071  0.9163196   0.12735799\n",
      "   0.21502903  0.09808328  0.07285655 -0.05356899]\n",
      " [ 0.04726816  0.19237662  0.39839954 -0.42091079  0.00330557  0.03558768\n",
      "  -0.12296424 -0.47421679  0.07635837 -0.11330759]\n",
      " [ 0.37634443 -0.53783867  0.23959237  0.38544868 -0.6034877  -0.13907351\n",
      "   0.49468421  0.25003903 -0.15225704  0.10873001]\n",
      " [-0.03329224 -0.15444488 -0.1225263  -0.12472762 -0.38135872  0.18497305\n",
      "   0.23378214 -0.01911625  0.3700485   0.62033594]\n",
      " [ 0.58292689  0.8042838   0.41262433  0.13568682  0.96865651 -0.05699524\n",
      "  -0.07156796 -0.45068578 -0.00590903  0.10799598]\n",
      " [-0.23409062  0.38510406  0.11123757  0.18173323 -0.56196063 -0.67851336\n",
      "  -0.17156883  0.39265192  0.27655169 -0.09063077]\n",
      " [-0.36740833 -0.19510349 -0.1473784  -0.31646238 -0.71957248  0.03288942\n",
      "  -0.081349    0.23822832 -0.05483601 -0.11219414]\n",
      " [-0.38685761 -0.30953832 -0.07117717  0.12890495 -0.68472539  0.44254465\n",
      "   0.24348055  0.23278917  0.11165916  0.10502971]\n",
      " [-0.10330115 -0.07599295  0.62246007 -0.02268403 -0.17138227 -0.21563068\n",
      "  -0.54181102  0.12352029  0.07043329 -0.49959741]]\n",
      "now loss =  0.012538400951583126  accuracy =  0.995\n",
      "gradient of first layer [[-0.18934529 -0.24421346  0.03230648  0.32062107 -0.32108216 -0.3702563\n",
      "  -0.3523957  -0.01754355  0.22899876  0.19739296]\n",
      " [ 0.06102473 -0.64459857  0.50822687  0.07701897  0.91932998  0.12910703\n",
      "   0.21597817  0.09621526  0.07260101 -0.05347114]\n",
      " [ 0.04768124  0.1923454   0.3985167  -0.42103385  0.0041535   0.03607004\n",
      "  -0.12270166 -0.47473749  0.07628703 -0.11328373]\n",
      " [ 0.37617441 -0.53782325  0.23953885  0.38549854 -0.60383907 -0.13927266\n",
      "   0.49457349  0.25025342 -0.15223274  0.1087163 ]\n",
      " [-0.03462593 -0.15430429 -0.12296844 -0.12433377 -0.3841216   0.18339074\n",
      "   0.23289127 -0.01742779  0.37021062  0.62019841]\n",
      " [ 0.58347579  0.80423627  0.41277019  0.13551656  0.96977455 -0.0563374\n",
      "  -0.07121176 -0.45138439 -0.00600467  0.1080348 ]\n",
      " [-0.23463891  0.38516339  0.11101233  0.18187592 -0.56312436 -0.67913195\n",
      "  -0.17193248  0.39333338  0.27659261 -0.09069035]\n",
      " [-0.36754144 -0.19509141 -0.1474204  -0.31642338 -0.71984763  0.03273357\n",
      "  -0.08143569  0.23839614 -0.05481707 -0.1122049 ]\n",
      " [-0.38772513 -0.30944765 -0.07146298  0.12916124 -0.68652168  0.44151537\n",
      "   0.2429019   0.23388759  0.11176617  0.10494125]\n",
      " [-0.1034706  -0.07597758  0.6224067  -0.02263434 -0.17173251 -0.21582915\n",
      "  -0.54192137  0.12373396  0.07045749 -0.49961109]]\n",
      "now loss =  0.0368704776736507  accuracy =  0.99\n",
      "gradient of first layer [[-0.19015093 -0.24407371  0.03179919  0.32078342 -0.32289732 -0.37113074\n",
      "  -0.35297813 -0.01656124  0.22892328  0.19722827]\n",
      " [ 0.06108012 -0.64439245  0.50744496  0.07676599  0.91897277  0.12939404\n",
      "   0.21583468  0.09603877  0.07199198 -0.05378021]\n",
      " [ 0.04805897  0.19236365  0.39847091 -0.42118946  0.00484235  0.03653965\n",
      "  -0.12250459 -0.47522868  0.07610003 -0.11333411]\n",
      " [ 0.37617923 -0.53782141  0.23953111  0.38549424 -0.60383449 -0.13926421\n",
      "   0.49457471  0.25024605 -0.1522404   0.10871306]\n",
      " [-0.03411432 -0.15418653 -0.12327103 -0.12460165 -0.3833123   0.18402965\n",
      "   0.23306307 -0.01810092  0.36974482  0.6199795 ]\n",
      " [ 0.58422618  0.80423655  0.41272392  0.13520769  0.97115994 -0.05536882\n",
      "  -0.07077928 -0.45237124 -0.00632299  0.10798811]\n",
      " [-0.23360107  0.38528127  0.11050399  0.18131775 -0.56147227 -0.67766753\n",
      "  -0.17143351  0.39190854  0.27581979 -0.09092626]\n",
      " [-0.36741153 -0.19510154 -0.14738662 -0.31646409 -0.71958321  0.03288839\n",
      "  -0.08135224  0.23823127 -0.05484105 -0.11219718]\n",
      " [-0.38686288 -0.309415   -0.07150294  0.12882765 -0.68490642  0.44255334\n",
      "   0.24335589  0.23278155  0.11137863  0.10483576]\n",
      " [-0.10345978 -0.0759763   0.62240075 -0.02264047 -0.17171559 -0.21581357\n",
      "  -0.54191621  0.12371899  0.07044893 -0.49961382]]\n",
      "now loss =  0.014536115452332719  accuracy =  0.995\n",
      "gradient of first layer [[-0.19158754 -0.24394657  0.03134477  0.32120728 -0.32586866 -0.3728135\n",
      "  -0.35391461 -0.01475133  0.22913006  0.19711535]\n",
      " [ 0.0610165  -0.64437574  0.50739984  0.07677666  0.91882591  0.12932335\n",
      "   0.21578464  0.09611667  0.07198052 -0.05379617]\n",
      " [ 0.04834318  0.1923426   0.39855235 -0.42127516  0.00542599  0.03687172\n",
      "  -0.12232346 -0.47558664  0.07605109 -0.11331764]\n",
      " [ 0.37598454 -0.53780433  0.23947042  0.38555206 -0.60423677 -0.13949267\n",
      "   0.49444792  0.25049145 -0.15221161  0.10869815]\n",
      " [-0.03537584 -0.15406505 -0.12368096 -0.12422926 -0.38592477  0.18254047\n",
      "   0.23222904 -0.01650759  0.36991174  0.61986437]\n",
      " [ 0.58476248  0.80419148  0.41287687  0.13504274  0.97226019 -0.05473156\n",
      "  -0.07043123 -0.45305035 -0.0064126   0.10802559]\n",
      " [-0.23544581  0.38545224  0.10990442  0.18185691 -0.56529881 -0.67982551\n",
      "  -0.17264143  0.39423068  0.27607302 -0.09107668]\n",
      " [-0.36755484 -0.19508897 -0.14743124 -0.3164215  -0.71987931  0.0327202\n",
      "  -0.08144556  0.23841191 -0.05481981 -0.11220813]\n",
      " [-0.38776034 -0.30932988 -0.07179182  0.12909268 -0.68676329  0.44149425\n",
      "   0.24276397  0.23391523  0.11149959  0.10475536]\n",
      " [-0.10365141 -0.07595949  0.62234101 -0.02258356 -0.17211153 -0.21603843\n",
      "  -0.542041    0.12396052  0.07047726 -0.4996285 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 500x500 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from layers import Affine, Relu, Sigmoid\n",
    "from model import Model\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "X, y = make_moons(n_samples=1000, noise=0.1)\n",
    "\n",
    "# visualize in 2D\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n",
    "\n",
    "nn = Model(layers=[Affine(10), Relu(), Affine(10), Affine(1)])\n",
    "\n",
    "targets = np.reshape(y, (y.shape[0], 1))\n",
    "nn.fit(X, targets, n_epochs=100, lr=1e-3)\n",
    "\n",
    "\n",
    "# Create a mesh grid for plotting decision boundary\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# Predict class for each point in the grid\n",
    "Z = nn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Predict on the original data\n",
    "y_pred = nn.predict(X)\n",
    "# Threshold predictions to get binary labels\n",
    "y_pred_labels = (y_pred > 0.5).astype(int).flatten()\n",
    "\n",
    "# Find misclassified points\n",
    "misclassified = y_pred_labels != y\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "# Plot correctly classified points\n",
    "plt.scatter(X[~misclassified, 0], X[~misclassified, 1], c=y[~misclassified], s=40, cmap=plt.cm.Spectral)\n",
    "# Highlight misclassified points\n",
    "plt.scatter(X[misclassified, 0], X[misclassified, 1], facecolors='none', edgecolors='r', s=80, marker='o')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Decision Boundary and Misclassified Points')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
